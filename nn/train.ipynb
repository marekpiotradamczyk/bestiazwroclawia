{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T17:25:19.453256Z",
     "start_time": "2024-05-04T17:25:19.449983Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import io\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy \n",
    "from tabulate import tabulate\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict as dd\n",
    "import plotly.express as px\n",
    "import chess\n",
    "import chess.pgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe9d0fda-f33e-4313-850b-f3b640d2c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic structure\n",
    "\n",
    "DATA_DIR = \"data/\"\n",
    "STOCKFISH_DIR = 'stockfish/'\n",
    "ARCHIVE_DIR = DATA_DIR + 'archives/'\n",
    "\n",
    "###\n",
    "##\n",
    "###\n",
    "\n",
    "### Stockfish\n",
    "\n",
    "STOCKFISH_AVX512_TAR = \"stockfish-ubuntu-x86-64-avx512.tar\"\n",
    "STOCKFISH_AVX512 = \"stockfish-ubuntu-x86-64-avx512\"\n",
    "STOCKFISH_AVX512_EXE = STOCKFISH_DIR + STOCKFISH_AVX512\n",
    "\n",
    "\n",
    "###\n",
    "##\n",
    "###\n",
    "\n",
    "### URLs\n",
    "\n",
    "ELITE_DATABASE_URL  = \"https://database.nikonoel.fr/lichess_elite_2021-11.zip\"\n",
    "STOCKFISH_DOWNSTREAM = \"https://github.com/official-stockfish/Stockfish/releases/latest/download/\"\n",
    "STOCKFISH_AVX512_URL = STOCKFISH_DOWNSTREAM + STOCKFISH_AVX512_TAR\n",
    "\n",
    "### \n",
    "##\n",
    "###\n",
    "\n",
    "### Datasets \n",
    "\n",
    "ELITE_DATASET_ARCHIVE = \"lichess_elite_2021-11.zip\"\n",
    "ELITE_DATASET_FILENAME = \"lichess_elite_2021-11.pgn\"\n",
    "\n",
    "\n",
    "LICHESS_EVAL_ARCHIVE = ARCHIVE_DIR + \"lichess_db_eval.jsonl.zst\"\n",
    "LICHESS_EVAL_FILENAME = DATA_DIR + \"lichess_db_eval.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b13750a6be5e8a69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:57:01.510501Z",
     "start_time": "2024-05-03T14:57:01.178672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BITBOARD_DIR = DATA_DIR + 'bitboards/'\n",
    "ELITE_DATA_BASE_URL  = \"https://database.nikonoel.fr/\"\n",
    "STOCKFISH_DOWNSTREAM = \"https://github.com/official-stockfish/Stockfish/releases/latest/download/\"\n",
    "\n",
    "SAMPLE_ZIP = \"lichess_elite_2021-11.zip\"\n",
    "SAMPLE_PGN = \"lichess_elite_2021-11.pgn\"\n",
    "SAMPLE_BITBOARD = \"elite_bitboard.csv\"\n",
    "BITBOARD_1M = \"1M.csv\"\n",
    "BITBOARD_10M = \"10M.csv\"\n",
    "ELITE_DATA_SAMPLE_URL = ELITE_DATA_BASE_URL + SAMPLE_ZIP\n",
    "SAMPLE_ZIP_FILE = ARCHIVE_DIR + SAMPLE_ZIP\n",
    "SAMPLE_PGN_FILE  = DATA_DIR + SAMPLE_PGN\n",
    "\n",
    "SAMPLE_BITBOARD_FILE = BITBOARD_DIR + SAMPLE_BITBOARD\n",
    "BITBOARD_10M_FILE = BITBOARD_DIR + BITBOARD_10M\n",
    "BITBOARD_1M_FILE = BITBOARD_DIR + BITBOARD_1M\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edce589a-5035-4fc0-96c1-3375d53daa5f",
   "metadata": {},
   "source": [
    "# Dataset and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91dc98ff-9857-4ec6-9c98-9550feba6472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0\n",
      "Reserved: 0\n"
     ]
    }
   ],
   "source": [
    "# def sizeof_fmt(num, suffix=\"B\"):\n",
    "#     for unit in (\"\", \"Ki\", \"Mi\", \"Gi\"):\n",
    "#         if abs(num) < 1024.0:\n",
    "#             return f\"{num:3.1f}{unit}{suffix}\"\n",
    "#         num /= 1024.0\n",
    "#     return f\"{num:.1f}Yi{suffix}\"\n",
    "    \n",
    "def sizeof_fmt(num):\n",
    "    for unit in (\"\", \"K\", \"M\", \"G\"):\n",
    "        if abs(num) < 1000.0:\n",
    "            return f\"{num:.0f}{unit}\"\n",
    "        num /= 1000.0\n",
    "    return f\"{num:.0f}\"\n",
    "\n",
    "\n",
    "print(f\"Allocated: {sizeof_fmt(torch.cuda.memory_allocated())}\") \n",
    "print(f\"Reserved: {sizeof_fmt(torch.cuda.memory_reserved())}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "608b0930c754cabd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:57:06.739621Z",
     "start_time": "2024-05-03T14:57:06.735393Z"
    }
   },
   "outputs": [],
   "source": [
    "class BitboardDrawDatasetSimple(Dataset):\n",
    "    def __init__(self, bitboard_file):\n",
    "        bitboards_df = pd.read_csv(bitboard_file, dtype=\"uint64\", usecols=range(12))\n",
    "        metadata_df = pd.read_csv(bitboard_file)\n",
    "\n",
    "        self.bitboards = self.bitboards_to_layers(bitboards_df)\n",
    "        # self.meta_features = self.binary_features_to_layers(metadata_df[[\"white\", \"cK\", \"cQ\", \"ck\", \"cq\"]])\n",
    "        self.is_draw = metadata_df['draw'].to_numpy(dtype=np.single)\n",
    "        self.length = self.is_draw.size\n",
    "\n",
    "        # self.bitboards = np.hstack((self.bitboards, self.meta_features))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.bitboards[idx], self.is_draw[idx]\n",
    "\n",
    "    def bitboards_to_layers(self, bitboards):\n",
    "        cont = np.ascontiguousarray(np.expand_dims(bitboards.to_numpy(), 2)).view(np.uint8)\n",
    "        return np.unpackbits(np.flip(cont, axis=2), axis=2).astype(np.single).reshape(-1, 768)\n",
    "\n",
    "    def binary_features_to_layers(self, features):\n",
    "        i = (features.to_numpy(dtype=np.uint64) - 1) ^ 0xffffffffffffffff\n",
    "        cont = np.ascontiguousarray(np.expand_dims(i, 2)).view(np.uint8)\n",
    "        return np.unpackbits(np.flip(cont, axis=2), axis=2).astype(np.single).reshape(-1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e65a3c3f-e7a9-4c4f-96a6-9f5a53a8f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitboardDrawDataset(Dataset):\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset_info(cls, dataset_info):\n",
    "        return cls(dataset_info.source_file, dataset_info.chunk_size, dataset_info.shuffle, dataset_info.in_memory)\n",
    "    \n",
    "    def __init__(self, bitboard_file, chunk_size, shuffle = True, in_memory = False):\n",
    "        self.curr_batch = 0\n",
    "        self.shuffle = shuffle\n",
    "        self.bitboard_file = bitboard_file\n",
    "        self.chunk_size = chunk_size\n",
    "        self.datasamples = self.calculate_dataset_size()\n",
    "        \n",
    "        self.batches = ceil(self.datasamples / chunk_size)\n",
    "        self.splits = np.arange(chunk_size, self.datasamples, chunk_size)\n",
    "        self.indices = np.arange(1, self.datasamples+1)\n",
    "        self.splits = np.append(self.splits, [0])\n",
    "\n",
    "        self.in_memory = in_memory or self.chunk_size == self.datasamples\n",
    "        \n",
    "        self.load_data()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.datasamples\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        batch = idx // self.chunk_size\n",
    "        # Laod new data \n",
    "        if self.curr_batch != batch and not self.in_memory:\n",
    "            self.curr_batch = batch\n",
    "            self.load_data()            \n",
    "\n",
    "        idx -= self.chunk_size * self.curr_batch\n",
    "        \n",
    "        return self.bitboards[idx], self.is_draw[idx]\n",
    "\n",
    "    def bitboards_to_layers(self, bitboards):\n",
    "        cont = np.ascontiguousarray(np.expand_dims(bitboards.to_numpy(), 2)).view(np.uint8)\n",
    "        return np.unpackbits(np.flip(cont, axis=2), axis=2).astype(np.single).reshape(-1, 768)\n",
    "\n",
    "    def binary_features_to_layers(self, features):\n",
    "        i = (features.to_numpy(dtype=np.uint64) - 1) ^ 0xffffffffffffffff\n",
    "        cont = np.ascontiguousarray(np.expand_dims(i, 2)).view(np.uint8)\n",
    "        return np.unpackbits(np.flip(cont, axis=2), axis=2).astype(np.single).reshape(-1, 768)\n",
    "        \n",
    "    def load_data(self):\n",
    "        if self.curr_batch == 0 and self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "        ignore = set(self.indices)\n",
    "\n",
    "        if self.curr_batch == self.batches-1:\n",
    "            ignore.difference_update(self.indices[self.splits[self.curr_batch-1]:])\n",
    "        else:\n",
    "            ignore.difference_update(self.indices[self.splits[self.curr_batch-1]:self.splits[self.curr_batch]])\n",
    "\n",
    "        df = pd.read_csv(self.bitboard_file, dtype=\"uint64\", usecols=range(18), skiprows=ignore)\n",
    "        self.bitboards = self.bitboards_to_layers(df.iloc[:, range(12)])\n",
    "        # self.meta_features = self.binary_features_to_layers(df[[\"white\", \"cK\", \"cQ\", \"ck\", \"cq\"]])\n",
    "        self.is_draw = df['draw'].to_numpy(dtype=np.single)\n",
    "\n",
    "        # self.bitboards = np.hstack((self.bitboards, self.meta_features))\n",
    "    \n",
    "    def calculate_dataset_size(self):\n",
    "        with open(self.bitboard_file) as f:\n",
    "            return sum(1 for line in f) - 1\n",
    "\n",
    "    def dataloader(self, batch_size):\n",
    "        return DataLoader(self, batch_size=batch_size, shuffle=self.in_memory)\n",
    "\n",
    "DatasetInfo = namedtuple(\"DatasetInfo\", [\"source_file\", \"chunk_size\", \"shuffle\", \"in_memory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d423b5aec85db4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T16:43:42.159881Z",
     "start_time": "2024-05-04T16:43:42.156480Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, sizes, normalization=False):\n",
    "        super().__init__()\n",
    "        self.sizes = sizes\n",
    "        self.layers = layers\n",
    "        ll = []\n",
    "\n",
    "        assert self.layers == len(sizes) - 2, \"Wrong layers to sizes number.\"\n",
    "        \n",
    "        for i in range(layers+1):\n",
    "            ll.append(nn.Linear(self.sizes[i], self.sizes[i+1]))   \n",
    "            if i < layers:\n",
    "                if layers > 1 and normalization:\n",
    "                    ll.append(nn.BatchNorm1d(self.sizes[i+1]))\n",
    "                ll.append(nn.ReLU())\n",
    "                \n",
    "        self.model = nn.Sequential(*ll)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Dense_{'_'.join(map(str, self.sizes))}\"\n",
    "\n",
    "    def get_hidden_layer_count(self):\n",
    "        return self.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28ac6b0b-0e42-4d5f-8242-4245266048e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_layers, out_layers, ksize=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_layers, out_layers, kernel_size=ksize, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_layers),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd36d802-e721-4497-b508-ffb40a1a8942",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_layers, out_layers, ksize=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_layers, out_layers, kernel_size=ksize, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_layers),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_layers, out_layers, kernel_size=ksize, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_layers),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.relu(self.conv(x) + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4677c52-9176-442b-97dc-b533b2a3edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, neurons=256):\n",
    "        super().__init__()\n",
    "        self.neurons = neurons\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Unflatten(1, (12, 8, 8)),\n",
    "            ConvLayer(12, 32),\n",
    "            # nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "\n",
    "            ConvLayer(32, 64),\n",
    "            # nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "\n",
    "            ConvLayer(64, 128),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            ConvLayer(128, 256),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Flatten(1),\n",
    "\n",
    "            DenseNetwork(3, [256 * 2 * 2, 128, 64, 32, 1], normalization=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "\n",
    "    def get_hidden_layer_count(self):\n",
    "        return 3\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"ConvNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05723af0-d688-4600-8c41-94c64d428626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniAlphaZeroNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, res_layers, layers):\n",
    "        super().__init__()\n",
    "        ll = []\n",
    "\n",
    "        for i in range(res_layers):\n",
    "            ll.append(ResLayer(layers, layers))\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Unflatten(1, (12, 8, 8)),\n",
    "            ConvLayer(12, layers),\n",
    "            *ll,\n",
    "            nn.Flatten(1),\n",
    "            DenseNetwork(3, [layers*8*8, 1024, 256, 128, 1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "\n",
    "    def get_hidden_layer_count(self):\n",
    "        return 3\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"MiniAlphaZero\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd4926-831f-4d5e-bc3c-dd550104e677",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87ecc2ce56ba7086",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:15:49.831344Z",
     "start_time": "2024-05-04T19:15:49.822055Z"
    }
   },
   "outputs": [],
   "source": [
    "def timeit(f):\n",
    "\n",
    "    def timed(*args, **kw):\n",
    "\n",
    "        ts = time.time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time.time()\n",
    "\n",
    "        print(f\"Took: {te-ts:.2f}s\")\n",
    "        return result\n",
    "    return timed\n",
    "\n",
    "class Train:\n",
    "    \n",
    "    def __init__(self, train_dataset, validate_dataset, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.validate_dataset = validate_dataset\n",
    "        \n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.validate_dataloader = DataLoader(self.validate_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.total_batches = len(self.train_dataloader)\n",
    "        self.print_every = 100\n",
    "        self.epoch_print_interval = 1\n",
    "        \n",
    "    def train_one_epoch(self, model, optimizer, p=False) -> int:\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "    \n",
    "        for i, data in enumerate(self.train_dataloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.unsqueeze(1).to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = self.loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "            if i % self.print_every == self.print_every - 1:\n",
    "                last_loss = running_loss / self.print_every\n",
    "                if p: print(f\"  batch {i+1} loss: {last_loss}\")\n",
    "                running_loss = 0.\n",
    "            elif i == self.total_batches - 1:\n",
    "                last_loss = running_loss / (i % self.print_every + 1)\n",
    "                if p: print(f\"  batch {i+1} loss: {last_loss}\")\n",
    "            \n",
    "        return last_loss\n",
    "    \n",
    "    @timeit\n",
    "    def train(self, model, optimizer, epochs, p=True):\n",
    "        best_result = dd(None)\n",
    "        try:\n",
    "            best_result['vloss'] = np.inf\n",
    "        \n",
    "            for epoch in range(1, epochs + 1):\n",
    "                if p and epoch % self.epoch_print_interval == 0 or epoch == 1: print(f'EPOCH {epoch}')\n",
    "            \n",
    "                # Make sure gradient tracking is on, and do a pass over the data\n",
    "                model.train(True)\n",
    "                avg_loss = self.train_one_epoch(model, optimizer)\n",
    "            \n",
    "                model.eval()\n",
    "    \n",
    "                with torch.no_grad():\n",
    "                    train_acc, train_loss, train_prec, train_recall = self.test(model, self.train_dataloader)\n",
    "                    # train_acc, train_loss, train_prec, train_recall = 0, 0, 0, 0\n",
    "                    validate_acc, validate_loss, validate_prec, validate_recall = self.test(model, self.validate_dataloader)\n",
    "    \n",
    "                    if p and epoch % self.epoch_print_interval == 0 or epoch == 1: \n",
    "                        print(tabulate([[\"Loss\", train_loss, validate_loss], \n",
    "                                        [\"Precision\", train_prec, validate_prec],\n",
    "                                        [\"Recall\", train_recall, validate_recall],\n",
    "                                        [\"Accuracy\", f\"{train_acc:.2f}%\", f\"{validate_acc:.2f}%\"]],\n",
    "                                       headers=[\"\", \"Train\", \"Test\"]))\n",
    "            \n",
    "                if validate_loss < best_result['vloss']:\n",
    "                    best_result['vloss'] = validate_loss\n",
    "                    best_result['model'] = deepcopy(model.state_dict())\n",
    "                    best_result['epoch'] = epoch\n",
    "                    best_result['acc']   = validate_acc\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            self.save_model(model, best_result['model'], best_result['epoch'], optimizer.param_groups[0]['lr'], optimizer.param_groups[0]['momentum'], best_result['acc'])\n",
    "\n",
    "        return best_result\n",
    "\n",
    "    def test(self, model, dataloader):\n",
    "        acc, loss = 0, 0\n",
    "        conf_mat = torch.zeros(2, 2) \n",
    "        for i, (vinputs, vlabels) in enumerate(dataloader):\n",
    "            vinputs, vlabels = vinputs.to(device), vlabels.unsqueeze(1).to(device)\n",
    "            voutputs = model(vinputs)\n",
    "            pred = nn.functional.sigmoid(voutputs).round()\n",
    "            \n",
    "            acc += (pred == vlabels).sum() / self.batch_size\n",
    "            loss += self.loss_fn(voutputs, vlabels) \n",
    "            # conf_mat += confusion_matrix(vlabels.to('cpu'), pred.to('cpu'))\n",
    "\n",
    "        acc = acc / (i+1) * 100\n",
    "        loss /= (i+1)\n",
    "\n",
    "        prec = conf_mat[1, 1] / (conf_mat[1, 1] + conf_mat[0, 1])\n",
    "        recall = conf_mat[1, 1] / (conf_mat[1, 1] + conf_mat[1, 0])\n",
    "        \n",
    "        return acc, loss, prec, recall\n",
    "    \n",
    "    def find_best(self, model, epochs, lr=1e-3, momentum=0.9):\n",
    "        print(str(model))\n",
    "        model.to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=0.001)\n",
    "        # optimizer = optim.Adam(model.parameters())\n",
    "        best_results = self.train(model, optimizer, epochs)\n",
    "        self.save_model(model, best_results['model'], best_results['epoch'], lr, momentum, best_results['acc'])\n",
    "        model.to('cpu')\n",
    "        self.cleanup()\n",
    "        \n",
    "        return best_results\n",
    "\n",
    "    def save_model(self, model, state_dict, epoch, lr, momentum, acc):\n",
    "        base_path = f\"models/{sizeof_fmt(len(self.train_dataset))}/{str(model.get_hidden_layer_count())}l/\"\n",
    "        if(not os.path.isdir(base_path)):\n",
    "            !mkdir -p {base_path}\n",
    "        \n",
    "        model_path = base_path + f\"{str(model)}_b{self.batch_size}_e{epoch}_lr{lr}_m{momentum}_acc{acc:.2f}\"\n",
    "        torch.save(state_dict, model_path)\n",
    "            \n",
    "    def cleanup(self):  \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "668744f1-9212-467e-95d5-bc2c1b9ef433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(dataset_info, k, new_model, epochs=50, lr=1e-2):\n",
    "    def result_avg(key):\n",
    "        return sum([r[key] for r in results]) / k\n",
    "    \n",
    "    dataset = BitboardDrawDataset.from_dataset_info(dataset_info)\n",
    "    folds = random_split(dataset, [ 1/k ] * k)\n",
    "    results = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        print(f\"RUNNING ITERATION {i+1}.\")\n",
    "        train = ConcatDataset([x for j,x in enumerate(folds) if j != k])\n",
    "        test  = folds[i]\n",
    "        model = new_model()\n",
    "\n",
    "        trainer = Train(train, test, 512)\n",
    "\n",
    "        results.append(trainer.find_best(model, epochs, lr=lr))\n",
    "\n",
    "    print(\"DONE\")\n",
    "    avg_acc = result_avg('acc')\n",
    "    print(f\"AVG ACC: {avg_acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdd566f9-6064-47c6-baf1-64b39a52cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_training(train_dataset_info, test_dataset_info, model, epochs=50, lr=1e-2):\n",
    "    train = BitboardDrawDataset.from_dataset_info(train_dataset_info)\n",
    "    test  = BitboardDrawDataset.from_dataset_info(test_dataset_info)\n",
    "    trainer = Train(train, test, 512)\n",
    "\n",
    "    return trainer.find_best(model, epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c98374d5-ed51-4fe6-887d-b1aa7bcf3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_info = DatasetInfo(\"data/eval_dataset/bitboards/6000000_0.csv\", 6000000, False, True)\n",
    "validate_dataset_info = DatasetInfo(\"data/eval_dataset/bitboards/1000000_30000000.csv\", 1000000, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bc21e23-32c0-4374-88b5-9227febd9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DatasetInfo(\"data/eval_dataset/bitboards/100000_1509579_19_24.csv\", 100000, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ccf7ea8-e1ca-4e5b-8680-923f21d84351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING ITERATION 1.\n",
      "Dense_768_768_1\n",
      "EPOCH 1\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5995900630950928  0.5993191003799438\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.95%              66.98%\n",
      "EPOCH 2\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5815632343292236  0.5812456011772156\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   68.59%              68.60%\n",
      "EPOCH 3\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5676711797714233  0.5673450231552124\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   69.83%              69.84%\n",
      "EPOCH 4\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5551223158836365  0.5547881722450256\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   70.87%              70.89%\n",
      "EPOCH 5\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5431786179542542  0.5429592132568359\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   71.88%              71.89%\n",
      "EPOCH 6\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5335446000099182  0.5332822203636169\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   72.66%              72.66%\n",
      "EPOCH 7\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.525742769241333  0.5254504084587097\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   73.18%             73.18%\n",
      "EPOCH 8\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5169444680213928  0.5166186094284058\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   73.89%              73.85%\n",
      "EPOCH 9\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5100762844085693  0.5097302794456482\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.37%              74.35%\n",
      "EPOCH 10\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5023343563079834  0.5021371245384216\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.92%              74.88%\n",
      "EPOCH 11\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4935680329799652  0.49326205253601074\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   75.56%              75.53%\n",
      "EPOCH 12\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4919840395450592  0.4917687177658081\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   75.72%              75.70%\n",
      "EPOCH 13\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.48345282673835754  0.48307397961616516\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   76.30%               76.31%\n",
      "EPOCH 14\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.48094800114631653  0.4808965027332306\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   76.39%               76.36%\n",
      "EPOCH 15\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.47440052032470703  0.4741915464401245\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   76.85%               76.83%\n",
      "EPOCH 16\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.46532824635505676  0.4651091694831848\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.49%               77.46%\n",
      "EPOCH 17\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.46384748816490173  0.46337759494781494\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.60%               77.61%\n",
      "EPOCH 18\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.45776137709617615  0.4576372504234314\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.99%               77.97%\n",
      "EPOCH 19\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.45800694823265076  0.4577847421169281\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.93%               77.93%\n",
      "EPOCH 20\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4627646803855896  0.4624086916446686\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   77.65%              77.64%\n",
      "EPOCH 21\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.45784157514572144  0.45753976702690125\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.83%               77.81%\n",
      "EPOCH 22\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.44706663489341736  0.4464592933654785\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.61%               78.61%\n",
      "EPOCH 23\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.44599753618240356  0.44552668929100037\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.76%               78.79%\n",
      "EPOCH 24\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.44019269943237305  0.43962836265563965\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.06%               79.07%\n",
      "EPOCH 25\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4369129240512848  0.43649938702583313\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.30%              79.28%\n",
      "EPOCH 26\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4354192316532135  0.4349888265132904\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.41%              79.42%\n",
      "EPOCH 27\n",
      "           Train               Test\n",
      "---------  ------------------  -----------------\n",
      "Loss       0.4330322742462158  0.432279109954834\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.54%              79.60%\n",
      "EPOCH 28\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.43477970361709595  0.43452203273773193\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.38%               79.36%\n",
      "EPOCH 29\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4289141595363617  0.42829999327659607\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.76%              79.75%\n",
      "EPOCH 30\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.42593711614608765  0.42548075318336487\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.92%               79.94%\n",
      "EPOCH 31\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.42464759945869446  0.42434191703796387\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.04%               80.06%\n",
      "EPOCH 32\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.431519091129303  0.4312719702720642\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   79.53%             79.51%\n",
      "EPOCH 33\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4240916967391968  0.42362871766090393\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.11%              80.11%\n",
      "EPOCH 34\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.41936296224594116  0.4191795885562897\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.42%               80.40%\n",
      "EPOCH 35\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.41430678963661194  0.4140189290046692\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.71%               80.68%\n",
      "EPOCH 36\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4157892167568207  0.41551175713539124\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.64%              80.64%\n",
      "EPOCH 37\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4172395169734955  0.41667601466178894\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.52%              80.53%\n",
      "EPOCH 38\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4125024080276489  0.41182956099510193\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.81%              80.84%\n",
      "EPOCH 39\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4255010485649109  0.4252706468105316\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.01%              79.99%\n",
      "EPOCH 40\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.41437816619873047  0.41390109062194824\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.71%               80.74%\n",
      "EPOCH 41\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4098726511001587  0.40937289595603943\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.92%              80.93%\n",
      "EPOCH 42\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.41996362805366516  0.4194229245185852\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.22%               80.24%\n",
      "EPOCH 43\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4059901833534241  0.4054671823978424\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.13%              81.15%\n",
      "EPOCH 44\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4117610454559326  0.4114716053009033\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.82%              80.82%\n",
      "EPOCH 45\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4080387055873871  0.4075736403465271\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.99%              80.98%\n",
      "EPOCH 46\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4030950367450714  0.4025742709636688\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.30%              81.29%\n",
      "EPOCH 47\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4100727140903473  0.4095797538757324\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.82%              80.81%\n",
      "EPOCH 48\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.40269720554351807  0.4021538496017456\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.33%               81.33%\n",
      "EPOCH 49\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4129815697669983  0.4124622941017151\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.81%              80.80%\n",
      "EPOCH 50\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.39852574467658997  0.39816489815711975\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.58%               81.56%\n",
      "Took: 3093.47s\n",
      "RUNNING ITERATION 2.\n",
      "Dense_768_768_1\n",
      "EPOCH 1\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5995181798934937  0.5995272994041443\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.00%              67.00%\n",
      "EPOCH 2\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5823249816894531  0.5822798609733582\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   68.51%              68.53%\n",
      "EPOCH 3\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5674335360527039  0.5674408078193665\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   69.84%              69.87%\n",
      "EPOCH 4\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5550280213356018  0.5550727248191833\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   70.87%              70.92%\n",
      "EPOCH 5\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5438638925552368  0.5439187288284302\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   71.81%              71.85%\n",
      "EPOCH 6\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5330072045326233  0.5331030488014221\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   72.68%              72.70%\n",
      "EPOCH 7\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5243023037910461  0.5244238376617432\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   73.30%              73.35%\n",
      "EPOCH 8\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.515546441078186  0.5159955024719238\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   74.00%             73.96%\n",
      "EPOCH 9\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5093779563903809  0.5097224116325378\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.32%              74.33%\n",
      "EPOCH 10\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5002259016036987  0.5007132291793823\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   75.12%              75.10%\n",
      "EPOCH 11\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4929693639278412  0.49350717663764954\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   75.62%              75.60%\n",
      "EPOCH 12\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.48644939064979553  0.48696163296699524\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   76.09%               76.08%\n",
      "EPOCH 13\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4806399345397949  0.48103126883506775\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   76.47%              76.43%\n",
      "EPOCH 14\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.47540420293807983  0.47585439682006836\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   76.84%               76.82%\n",
      "EPOCH 15\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4689834713935852  0.4694124162197113\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   77.28%              77.27%\n",
      "EPOCH 16\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.46857184171676636  0.46933111548423767\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.23%               77.18%\n",
      "EPOCH 17\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.46142610907554626  0.4620146155357361\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.78%               77.77%\n",
      "EPOCH 18\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.46769511699676514  0.46835842728614807\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.25%               77.20%\n",
      "EPOCH 19\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4571916162967682  0.4577891230583191\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   78.02%              77.98%\n",
      "EPOCH 20\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.44995996356010437  0.4505096971988678\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.44%               78.40%\n",
      "EPOCH 21\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.44751065969467163  0.44821539521217346\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.61%               78.56%\n",
      "EPOCH 22\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.44324788451194763  0.4437260925769806\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.93%               78.92%\n",
      "EPOCH 23\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4460984170436859  0.4469957947731018\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   78.76%              78.70%\n",
      "EPOCH 24\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4385795295238495  0.43934646248817444\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.18%              79.15%\n",
      "EPOCH 25\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4396021366119385  0.44020771980285645\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.07%              78.99%\n",
      "EPOCH 26\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.43148529529571533  0.43181851506233215\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.68%               79.66%\n",
      "EPOCH 27\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.43583106994628906  0.4364090859889984\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.42%               79.38%\n",
      "EPOCH 28\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4334721863269806  0.43426162004470825\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.42%              79.38%\n",
      "EPOCH 29\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4282187819480896  0.4289027154445648\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.87%              79.85%\n",
      "EPOCH 30\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4232836067676544  0.4240875244140625\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.14%              80.10%\n",
      "EPOCH 31\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4339294135570526  0.43425166606903076\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.38%              79.33%\n",
      "EPOCH 32\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42055752873420715  0.4209878444671631\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.28%               80.30%\n",
      "EPOCH 33\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.41723141074180603  0.41789862513542175\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.54%               80.50%\n",
      "EPOCH 34\n",
      "           Train               Test\n",
      "---------  ------------------  -----------------\n",
      "Loss       0.4171147048473358  0.417744904756546\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.48%              80.47%\n",
      "EPOCH 35\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.41966697573661804  0.4204517900943756\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.33%               80.26%\n",
      "EPOCH 36\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.41349175572395325  0.4141402542591095\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.73%               80.70%\n",
      "EPOCH 37\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42159852385520935  0.4221501052379608\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.30%               80.26%\n",
      "EPOCH 38\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.41375744342803955  0.41436272859573364\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.74%               80.70%\n",
      "EPOCH 39\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4119715094566345  0.4124026894569397\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.82%              80.77%\n",
      "EPOCH 40\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4101252257823944  0.4106873869895935\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.94%              80.89%\n",
      "EPOCH 41\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4075238108634949  0.4082711637020111\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.06%              81.00%\n",
      "EPOCH 42\n",
      "           Train              Test\n",
      "---------  -----------------  -------------------\n",
      "Loss       0.406448096036911  0.40715575218200684\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   81.14%             81.07%\n",
      "EPOCH 43\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.40507054328918457  0.4057878851890564\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.21%               81.13%\n",
      "EPOCH 44\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4073624312877655  0.40791818499565125\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.01%              80.97%\n",
      "EPOCH 45\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4036315977573395  0.40435487031936646\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.30%              81.24%\n",
      "EPOCH 46\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.40173259377479553  0.40229371190071106\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.44%               81.40%\n",
      "EPOCH 47\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4044296145439148  0.4052285850048065\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.30%              81.24%\n",
      "EPOCH 48\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.40063512325286865  0.4010479152202606\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.47%               81.42%\n",
      "EPOCH 49\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3975803554058075  0.3979257047176361\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.68%              81.65%\n",
      "EPOCH 50\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.40002256631851196  0.4006437361240387\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.54%               81.50%\n",
      "Took: 3542.70s\n",
      "RUNNING ITERATION 3.\n",
      "Dense_768_768_1\n",
      "EPOCH 1\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6000549793243408  0.5998106598854065\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.86%              66.93%\n",
      "EPOCH 2\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.583864152431488  0.5834893584251404\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   68.36%             68.40%\n",
      "EPOCH 3\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5726961493492126  0.5722769498825073\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   69.27%              69.33%\n",
      "EPOCH 4\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5573039650917053  0.5569726824760437\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   70.70%              70.73%\n",
      "EPOCH 5\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5455027222633362  0.5451344847679138\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   71.64%              71.65%\n",
      "EPOCH 6\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5333647131919861  0.5330139398574829\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   72.66%              72.68%\n",
      "EPOCH 7\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5250077247619629  0.5245575904846191\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   73.30%              73.29%\n",
      "EPOCH 8\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5149893760681152  0.5145606994628906\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.08%              74.10%\n",
      "EPOCH 9\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5074357986450195  0.5071670413017273\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.58%              74.57%\n",
      "EPOCH 10\n",
      "           Train              Test\n",
      "---------  -----------------  -------------------\n",
      "Loss       0.499944806098938  0.49939417839050293\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   75.15%             75.16%\n",
      "EPOCH 11\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.49546924233436584  0.49497660994529724\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   75.37%               75.40%\n",
      "EPOCH 12\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4904214143753052  0.4899943470954895\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   75.71%              75.72%\n",
      "EPOCH 13\n",
      "           Train             Test\n",
      "---------  ----------------  -------------------\n",
      "Loss       0.48149573802948  0.48091956973075867\n",
      "Precision  nan               nan\n",
      "Recall     nan               nan\n",
      "Accuracy   76.42%            76.47%\n",
      "EPOCH 14\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4757441580295563  0.47517091035842896\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   76.80%              76.83%\n",
      "EPOCH 15\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.470191091299057  0.4697096347808838\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   77.18%             77.19%\n",
      "EPOCH 16\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.46565133333206177  0.46527785062789917\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.47%               77.48%\n",
      "EPOCH 17\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4624972641468048  0.4618794023990631\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   77.64%              77.66%\n",
      "EPOCH 18\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.45992499589920044  0.45948919653892517\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.77%               77.80%\n",
      "EPOCH 19\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.45386654138565063  0.4531724154949188\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.23%               78.25%\n",
      "EPOCH 20\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4503311216831207  0.44964224100112915\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   78.44%              78.48%\n",
      "EPOCH 21\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.451091468334198  0.4507395029067993\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   78.41%             78.42%\n",
      "EPOCH 22\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4435969293117523  0.44308799505233765\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   78.95%              78.95%\n",
      "EPOCH 23\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.44043365120887756  0.4399116635322571\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.08%               79.12%\n",
      "EPOCH 24\n",
      "           Train              Test\n",
      "---------  -----------------  -------------------\n",
      "Loss       0.444502055644989  0.44399991631507874\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   78.86%             78.84%\n",
      "EPOCH 25\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.44591495394706726  0.44525378942489624\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.62%               78.65%\n",
      "EPOCH 26\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.43380945920944214  0.4331419765949249\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.49%               79.52%\n",
      "EPOCH 27\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.43242940306663513  0.4317933917045593\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.60%               79.63%\n",
      "EPOCH 28\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42978397011756897  0.4288907051086426\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.71%               79.75%\n",
      "EPOCH 29\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42648056149482727  0.4257562756538391\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.94%               79.98%\n",
      "EPOCH 30\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42478716373443604  0.4242497980594635\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.05%               80.06%\n",
      "EPOCH 31\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4242948591709137  0.42366114258766174\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.10%              80.11%\n",
      "EPOCH 32\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42227932810783386  0.4217698276042938\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.18%               80.21%\n",
      "EPOCH 33\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42047223448753357  0.4199802279472351\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.31%               80.35%\n",
      "EPOCH 34\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4204370379447937  0.4197416305541992\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.33%              80.34%\n",
      "EPOCH 35\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4205731451511383  0.41978931427001953\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.25%              80.29%\n",
      "EPOCH 36\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42001092433929443  0.4192589521408081\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.40%               80.44%\n",
      "EPOCH 37\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.41672810912132263  0.4156937003135681\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.49%               80.56%\n",
      "EPOCH 38\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4203231632709503  0.41976413130760193\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.23%              80.27%\n",
      "EPOCH 39\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.41660961508750916  0.41578957438468933\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.55%               80.56%\n",
      "EPOCH 40\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.41460874676704407  0.4136330783367157\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.62%               80.69%\n",
      "EPOCH 41\n",
      "           Train                Test\n",
      "---------  -------------------  -----------------\n",
      "Loss       0.41120588779449463  0.410553902387619\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.83%               80.86%\n",
      "EPOCH 42\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4066152572631836  0.40586423873901367\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.18%              81.21%\n",
      "EPOCH 43\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.40814104676246643  0.40735524892807007\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.07%               81.11%\n",
      "EPOCH 44\n",
      "           Train                Test\n",
      "---------  -------------------  -----------------\n",
      "Loss       0.40473127365112305  0.403903067111969\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.23%               81.27%\n",
      "EPOCH 45\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.40268412232398987  0.4017675220966339\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.34%               81.39%\n",
      "EPOCH 46\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.40284058451652527  0.4020928144454956\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.38%               81.40%\n",
      "EPOCH 47\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4009576141834259  0.40013980865478516\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.51%              81.55%\n",
      "EPOCH 48\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.39919033646583557  0.3984432816505432\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.59%               81.63%\n",
      "EPOCH 49\n",
      "           Train               Test\n",
      "---------  ------------------  -----------------\n",
      "Loss       0.4002864360809326  0.399629145860672\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.52%              81.58%\n",
      "EPOCH 50\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.39854225516319275  0.39753082394599915\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.62%               81.70%\n",
      "Took: 3149.79s\n",
      "RUNNING ITERATION 4.\n",
      "Dense_768_768_1\n",
      "EPOCH 1\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5999709963798523  0.6001409888267517\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.88%              66.80%\n",
      "EPOCH 2\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5821031928062439  0.5824176073074341\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   68.58%              68.52%\n",
      "EPOCH 3\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5716801285743713  0.5720140337944031\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   69.41%              69.35%\n",
      "EPOCH 4\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5549551844596863  0.5553107857704163\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   70.92%              70.86%\n",
      "EPOCH 5\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5446220636367798  0.5451605319976807\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   71.76%              71.70%\n",
      "EPOCH 6\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5337560176849365  0.5341096520423889\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   72.65%              72.57%\n",
      "EPOCH 7\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5254643559455872  0.5259944200515747\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   73.25%              73.17%\n",
      "EPOCH 8\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5169879198074341  0.5175095200538635\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   73.91%              73.84%\n",
      "EPOCH 9\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5111486911773682  0.5116276144981384\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.27%              74.22%\n",
      "EPOCH 10\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5024725198745728  0.5025632977485657\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.95%              74.92%\n",
      "EPOCH 11\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.49783456325531006  0.4982982277870178\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   75.14%               75.08%\n",
      "EPOCH 12\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4907936155796051  0.4910629689693451\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   75.66%              75.61%\n",
      "EPOCH 13\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4811665415763855  0.4814535677433014\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   76.45%              76.40%\n",
      "EPOCH 14\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4748063087463379  0.4753461480140686\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   76.87%              76.82%\n",
      "EPOCH 15\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4718347191810608  0.4723423719406128\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   77.02%              76.97%\n",
      "EPOCH 16\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.46628114581108093  0.46651533246040344\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.41%               77.36%\n",
      "EPOCH 17\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4633912742137909  0.46344441175460815\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   77.52%              77.49%\n",
      "EPOCH 18\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4583703875541687  0.4584554135799408\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   77.96%              77.92%\n",
      "EPOCH 19\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.45577120780944824  0.45598727464675903\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.06%               78.05%\n",
      "EPOCH 20\n",
      "           Train             Test\n",
      "---------  ----------------  -------------------\n",
      "Loss       0.45170858502388  0.45187973976135254\n",
      "Precision  nan               nan\n",
      "Recall     nan               nan\n",
      "Accuracy   78.33%            78.29%\n",
      "EPOCH 21\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4496404826641083  0.4497511386871338\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   78.42%              78.42%\n",
      "EPOCH 22\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4487634599208832  0.4487122893333435\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   78.56%              78.56%\n",
      "EPOCH 23\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.450015127658844  0.4500466287136078\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   78.44%             78.44%\n",
      "EPOCH 24\n",
      "           Train              Test\n",
      "---------  -----------------  -----------------\n",
      "Loss       0.437113881111145  0.437212735414505\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   79.27%             79.26%\n",
      "EPOCH 25\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4397886097431183  0.44003263115882874\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.05%              79.03%\n",
      "EPOCH 26\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.43341583013534546  0.4336555600166321\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.53%               79.54%\n",
      "EPOCH 27\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4295503795146942  0.42955440282821655\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.75%              79.75%\n",
      "EPOCH 28\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4283592402935028  0.4285339117050171\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.79%              79.79%\n",
      "EPOCH 29\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4280388057231903  0.4281393885612488\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.79%              79.79%\n",
      "EPOCH 30\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.42721298336982727  0.42738568782806396\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.83%               79.84%\n",
      "EPOCH 31\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42708632349967957  0.4272599220275879\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.83%               79.80%\n",
      "EPOCH 32\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42307502031326294  0.4232281446456909\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.07%               80.10%\n",
      "EPOCH 33\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.41694650053977966  0.41717997193336487\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.52%               80.51%\n",
      "EPOCH 34\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42229411005973816  0.4224516749382019\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.24%               80.22%\n",
      "EPOCH 35\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.41500231623649597  0.4151281714439392\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.63%               80.64%\n",
      "EPOCH 36\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42037108540534973  0.4208207428455353\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.20%               80.18%\n",
      "EPOCH 37\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4139598309993744  0.4142798185348511\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.64%              80.68%\n",
      "EPOCH 38\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.41336730122566223  0.41334038972854614\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.68%               80.68%\n",
      "EPOCH 39\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4094454050064087  0.40956512093544006\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.96%              80.98%\n",
      "EPOCH 40\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.41017812490463257  0.41032376885414124\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.87%               80.86%\n",
      "EPOCH 41\n",
      "           Train              Test\n",
      "---------  -----------------  -------------------\n",
      "Loss       0.409634530544281  0.40978527069091797\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   80.90%             80.90%\n",
      "EPOCH 42\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.410538911819458  0.4107001721858978\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   80.86%             80.88%\n",
      "EPOCH 43\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4069685637950897  0.4071398973464966\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.05%              81.05%\n",
      "EPOCH 44\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4072648286819458  0.40744179487228394\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.08%              81.07%\n",
      "EPOCH 45\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4053812623023987  0.40539461374282837\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.15%              81.17%\n",
      "EPOCH 46\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.40076640248298645  0.4009874761104584\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.43%               81.44%\n",
      "EPOCH 47\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4075985252857208  0.4081730246543884\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.01%              80.99%\n",
      "EPOCH 48\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4028407037258148  0.40294378995895386\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.34%              81.33%\n",
      "EPOCH 49\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4078003764152527  0.4076055586338043\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.14%              81.15%\n",
      "EPOCH 50\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4028919041156769  0.40292757749557495\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.26%              81.25%\n",
      "Took: 3140.81s\n",
      "RUNNING ITERATION 5.\n",
      "Dense_768_768_1\n",
      "EPOCH 1\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5999701023101807  0.6002622842788696\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.95%              66.90%\n",
      "EPOCH 2\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.582810640335083  0.5830201506614685\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   68.44%             68.41%\n",
      "EPOCH 3\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5681394934654236  0.5683183670043945\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   69.89%              69.86%\n",
      "EPOCH 4\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5571024417877197  0.5573256611824036\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   70.65%              70.59%\n",
      "EPOCH 5\n",
      "           Train               Test\n",
      "---------  ------------------  -----------------\n",
      "Loss       0.5429707765579224  0.543238639831543\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   71.93%              71.89%\n",
      "EPOCH 6\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.533340334892273  0.5336621403694153\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   72.61%             72.59%\n",
      "EPOCH 7\n",
      "           Train               Test\n",
      "---------  ------------------  ----------------\n",
      "Loss       0.5252900123596191  0.52543044090271\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   73.24%              73.22%\n",
      "EPOCH 8\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5169462561607361  0.5172792077064514\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   73.91%              73.85%\n",
      "EPOCH 9\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5067546963691711  0.5071527361869812\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.67%              74.62%\n",
      "EPOCH 10\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.49831587076187134  0.49853816628456116\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   75.24%               75.21%\n",
      "EPOCH 11\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.49650144577026367  0.4970385432243347\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   75.34%               75.24%\n",
      "EPOCH 12\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.48761388659477234  0.48795151710510254\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   76.01%               75.95%\n",
      "EPOCH 13\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4822132885456085  0.4824533760547638\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   76.23%              76.17%\n",
      "EPOCH 14\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4750090539455414  0.47538959980010986\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   76.85%              76.76%\n",
      "EPOCH 15\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4746682047843933  0.4749452471733093\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   76.85%              76.82%\n",
      "EPOCH 16\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.46434667706489563  0.4648016095161438\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.54%               77.48%\n",
      "EPOCH 17\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.46151962876319885  0.46208298206329346\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.73%               77.66%\n",
      "EPOCH 18\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4589933454990387  0.4591522514820099\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   77.90%              77.84%\n",
      "EPOCH 19\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.45387348532676697  0.4544944763183594\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.15%               78.08%\n",
      "EPOCH 20\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.45035284757614136  0.4507409632205963\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.45%               78.40%\n",
      "EPOCH 21\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4470685124397278  0.4472573399543762\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   78.62%              78.55%\n",
      "EPOCH 22\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.44304925203323364  0.44321131706237793\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.85%               78.81%\n",
      "EPOCH 23\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4399504065513611  0.44046831130981445\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.09%              79.00%\n",
      "EPOCH 24\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4367830753326416  0.4373498558998108\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.30%              79.25%\n",
      "EPOCH 25\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4389798641204834  0.4394397735595703\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.15%              79.09%\n",
      "EPOCH 26\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.43493300676345825  0.4355267584323883\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.34%               79.27%\n",
      "EPOCH 27\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42959779500961304  0.4298095405101776\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.77%               79.74%\n",
      "EPOCH 28\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4326954483985901  0.43318527936935425\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.56%              79.54%\n",
      "EPOCH 29\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42650824785232544  0.4268760085105896\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.97%               79.92%\n",
      "EPOCH 30\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4260570704936981  0.4266175329685211\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.92%              79.87%\n",
      "EPOCH 31\n",
      "           Train               Test\n",
      "---------  ------------------  -----------------\n",
      "Loss       0.4205714464187622  0.420847088098526\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.32%              80.27%\n",
      "EPOCH 32\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.41969430446624756  0.42001357674598694\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.36%               80.33%\n",
      "EPOCH 33\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.42095574736595154  0.42122694849967957\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.20%               80.17%\n",
      "EPOCH 34\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.41901078820228577  0.4194422662258148\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.35%               80.36%\n",
      "EPOCH 35\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42371514439582825  0.4241984784603119\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.99%               79.98%\n",
      "EPOCH 36\n",
      "           Train              Test\n",
      "---------  -----------------  -------------------\n",
      "Loss       0.415118932723999  0.41564327478408813\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   80.69%             80.64%\n",
      "EPOCH 37\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4132370948791504  0.41358131170272827\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.70%              80.67%\n",
      "EPOCH 38\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4121028482913971  0.41222497820854187\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.81%              80.83%\n",
      "EPOCH 39\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4151349365711212  0.41550037264823914\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.57%              80.54%\n",
      "EPOCH 40\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.408468633890152  0.4090205132961273\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   80.98%             80.95%\n",
      "EPOCH 41\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.40852802991867065  0.40889185667037964\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.02%               80.99%\n",
      "EPOCH 42\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.40656912326812744  0.40681707859039307\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.11%               81.07%\n",
      "EPOCH 43\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4062820076942444  0.40673673152923584\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.14%              81.09%\n",
      "EPOCH 44\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4058699309825897  0.4061585068702698\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.12%              81.09%\n",
      "EPOCH 45\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4039040505886078  0.40427765250205994\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.28%              81.27%\n",
      "EPOCH 46\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.40411055088043213  0.40445008873939514\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.23%               81.22%\n",
      "EPOCH 47\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4051101505756378  0.40554046630859375\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.27%              81.24%\n",
      "EPOCH 48\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4001803994178772  0.40033453702926636\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.46%              81.43%\n",
      "EPOCH 49\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.3972761631011963  0.39751988649368286\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.69%              81.65%\n",
      "EPOCH 50\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4054489731788635  0.40592139959335327\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.12%              81.07%\n",
      "Took: 3161.87s\n",
      "DONE\n",
      "AVG ACC: 81.41544342041016\n"
     ]
    }
   ],
   "source": [
    "k_fold_cross_validation(eval_dataset_info, 5, lambda: DenseNetwork(1, [768, 768, 1]), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "312630a8-dcbf-4dba-a0b6-e1a33f3e929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DenseNetwork(3, [768, 10_000, 256, 128, 1])\n",
    "# model = DenseNetwork(3, [768, 768, 256, 128, 1])\n",
    "# model = DenseNetwork(2, [768, 256, 128, 1])\n",
    "# model = DenseNetwork(1, [768, 768, 1])\n",
    "# model = DenseNetwork(0, [768, 1])\n",
    "# model = ConvolutionNetwork()\n",
    "model = MiniAlphaZeroNetwork(6, 32)\n",
    "# model.load_state_dict(torch.load(\"models/1M/3l/Dense_768_768_256_128_1_b512_e50_lr0.01_m0.9_acc92.27\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "175c06b0-c2cf-4562-9fda-1c8639a384fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniAlphaZero\n",
      "EPOCH 1\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5921809077262878  0.6279385685920715\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.07%              64.80%\n",
      "EPOCH 2\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5368253588676453  0.5750608444213867\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   71.81%              69.01%\n",
      "EPOCH 3\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5397118926048279  0.5827426910400391\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   71.94%              68.48%\n",
      "EPOCH 4\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5417366027832031  0.5880905985832214\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   71.43%              68.05%\n",
      "EPOCH 5\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5095325708389282  0.5696843266487122\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.03%              69.37%\n",
      "EPOCH 6\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4960879385471344  0.5596574544906616\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.98%              70.24%\n",
      "EPOCH 7\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4890677332878113  0.5612123608589172\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   75.48%              70.26%\n",
      "EPOCH 8\n",
      "           Train             Test\n",
      "---------  ----------------  ------------------\n",
      "Loss       0.54814213514328  0.6229577660560608\n",
      "Precision  nan               nan\n",
      "Recall     nan               nan\n",
      "Accuracy   70.92%            66.21%\n",
      "EPOCH 9\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4983505308628082  0.5856897830963135\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.92%              68.92%\n",
      "EPOCH 10\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4716269075870514  0.5692930817604065\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   77.02%              69.63%\n",
      "Took: 11316.31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'vloss': tensor(0.5597, device='cuda:0'),\n",
       "             'model': OrderedDict([('model.1.model.0.weight',\n",
       "                           tensor([[[[-2.6960e-02, -3.7865e-03, -2.8808e-02],\n",
       "                                     [-2.3475e-03,  4.6759e-02, -2.3275e-02],\n",
       "                                     [-2.0079e-03, -1.3963e-02, -2.7925e-02]],\n",
       "                           \n",
       "                                    [[ 1.0891e-01,  6.1010e-02,  1.3627e-02],\n",
       "                                     [-1.4610e-02,  2.2349e-01, -2.0620e-02],\n",
       "                                     [-1.9177e-02,  5.6077e-02, -2.4855e-03]],\n",
       "                           \n",
       "                                    [[ 6.5802e-02,  5.6134e-02,  1.0835e-01],\n",
       "                                     [ 7.0924e-03,  1.1770e-01, -4.1149e-02],\n",
       "                                     [-2.4105e-02,  4.1086e-04, -5.3843e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-2.2674e-02,  1.9464e-02, -8.9619e-03],\n",
       "                                     [-1.8323e-02, -1.4846e-02, -2.0701e-02],\n",
       "                                     [ 1.0065e-03, -1.0037e-02, -2.2374e-02]],\n",
       "                           \n",
       "                                    [[ 4.2278e-03, -2.5015e-02, -2.3747e-02],\n",
       "                                     [-1.8210e-02, -2.9513e-03,  9.2932e-04],\n",
       "                                     [-1.5645e-02, -4.5659e-03,  2.6845e-03]],\n",
       "                           \n",
       "                                    [[-6.7510e-02, -5.9338e-02,  1.1995e-02],\n",
       "                                     [-3.8068e-03, -2.3585e-02, -2.2547e-02],\n",
       "                                     [ 4.5118e-03,  8.6962e-03, -3.9974e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 3.6560e-02,  1.8403e-02,  1.0978e-02],\n",
       "                                     [-3.5554e-02,  1.8264e-01,  1.4572e-02],\n",
       "                                     [ 3.8408e-02,  4.0584e-01,  2.6842e-02]],\n",
       "                           \n",
       "                                    [[-8.3910e-03, -6.3396e-02, -3.2118e-03],\n",
       "                                     [-1.8451e-02, -5.4624e-02, -4.3285e-02],\n",
       "                                     [-1.4299e-03, -6.5030e-03,  1.1578e-03]],\n",
       "                           \n",
       "                                    [[ 6.0897e-03,  2.8894e-03, -4.8712e-03],\n",
       "                                     [ 3.4355e-02, -2.2644e-02, -2.9559e-03],\n",
       "                                     [ 3.1756e-02,  3.1775e-03, -3.5193e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 1.3470e-02,  8.2071e-02, -1.2812e-02],\n",
       "                                     [-4.4276e-02,  3.1772e-02, -3.1838e-02],\n",
       "                                     [-1.5092e-02, -2.6527e-02,  1.8265e-03]],\n",
       "                           \n",
       "                                    [[-1.1814e-02, -1.7312e-02, -1.4539e-02],\n",
       "                                     [-2.7822e-02, -2.0153e-02,  7.4593e-03],\n",
       "                                     [-8.5253e-03,  3.5268e-03, -3.4051e-02]],\n",
       "                           \n",
       "                                    [[-1.1207e-02, -2.7494e-02,  9.2174e-03],\n",
       "                                     [-3.8230e-02, -3.6367e-02, -3.9350e-02],\n",
       "                                     [-5.5366e-02, -3.3628e-02, -4.0105e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-3.6149e-02,  4.9040e-02, -6.2192e-03],\n",
       "                                     [-5.4260e-02, -7.3262e-05, -5.1334e-02],\n",
       "                                     [-2.7978e-02,  4.5386e-03, -1.7037e-02]],\n",
       "                           \n",
       "                                    [[-4.5700e-02, -2.3623e-02, -1.8961e-02],\n",
       "                                     [ 1.3645e-02, -3.4922e-02, -3.9531e-02],\n",
       "                                     [-3.4627e-02, -6.9609e-02, -1.7870e-02]],\n",
       "                           \n",
       "                                    [[-1.1172e-02, -2.1713e-02, -2.3610e-02],\n",
       "                                     [-5.2964e-02, -6.0995e-02, -7.0491e-02],\n",
       "                                     [-9.2728e-04, -3.6469e-02,  2.5650e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-2.7425e-02, -4.1497e-02, -5.9007e-03],\n",
       "                                     [-7.0027e-02, -5.1325e-02, -4.2135e-02],\n",
       "                                     [ 3.4856e-03, -2.7616e-02, -4.5687e-02]],\n",
       "                           \n",
       "                                    [[-3.1235e-02, -4.7398e-02, -1.6769e-02],\n",
       "                                     [-2.2794e-02, -3.7955e-02, -1.5165e-02],\n",
       "                                     [-6.4670e-03,  4.7825e-03,  4.1433e-03]],\n",
       "                           \n",
       "                                    [[ 4.2110e-02, -1.4580e-02,  3.5362e-02],\n",
       "                                     [ 1.5412e-02,  9.9891e-03,  5.5009e-03],\n",
       "                                     [ 2.8334e-02, -1.1190e-02,  3.6283e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   ...,\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 3.6681e-03,  4.0056e-02,  2.3890e-03],\n",
       "                                     [-5.3893e-02,  4.5688e-02,  3.2687e-02],\n",
       "                                     [ 4.5593e-02,  2.1109e-02,  2.3989e-02]],\n",
       "                           \n",
       "                                    [[-2.5853e-02,  1.4831e-01, -8.5383e-03],\n",
       "                                     [ 5.7660e-02, -3.3786e-02, -3.1366e-02],\n",
       "                                     [-6.9995e-03, -1.6933e-02,  6.6126e-02]],\n",
       "                           \n",
       "                                    [[ 3.2819e-02, -5.0678e-03,  2.1799e-01],\n",
       "                                     [-9.5473e-03,  5.5266e-02, -1.7802e-02],\n",
       "                                     [ 2.4816e-02, -3.9055e-03,  1.7201e-01]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 3.4138e-03,  5.5293e-02,  1.8457e-02],\n",
       "                                     [ 8.9607e-03,  4.4865e-02, -1.4324e-02],\n",
       "                                     [-2.1134e-02, -2.2697e-02, -1.3196e-02]],\n",
       "                           \n",
       "                                    [[-1.9529e-02,  1.3567e-01,  5.6189e-03],\n",
       "                                     [ 8.5688e-03,  3.1809e-02, -8.3648e-03],\n",
       "                                     [ 1.8447e-02,  2.3712e-02,  4.6476e-02]],\n",
       "                           \n",
       "                                    [[-7.7379e-03, -6.5278e-03, -4.7363e-02],\n",
       "                                     [-1.4686e-02, -2.2097e-02, -1.5475e-02],\n",
       "                                     [-7.7902e-03, -2.9131e-02, -4.8066e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 1.4569e-01,  3.5529e-03,  8.1254e-03],\n",
       "                                     [ 2.0872e-02,  5.6771e-02,  6.8484e-02],\n",
       "                                     [-8.5238e-02,  1.4762e-02, -1.1611e-01]],\n",
       "                           \n",
       "                                    [[-3.6257e-02,  7.2053e-02, -1.7663e-02],\n",
       "                                     [ 7.3548e-02,  4.5206e-02, -1.9869e-02],\n",
       "                                     [ 6.7045e-02, -5.0513e-02,  4.5096e-02]],\n",
       "                           \n",
       "                                    [[ 2.1091e-03,  1.2610e-02, -1.8900e-02],\n",
       "                                     [ 2.4420e-02, -7.0809e-03, -7.2868e-03],\n",
       "                                     [ 4.6715e-02, -1.2380e-03,  6.7827e-04]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-4.8924e-02, -4.6279e-02, -4.9931e-02],\n",
       "                                     [ 2.1994e-02, -4.8710e-02,  1.0673e-03],\n",
       "                                     [-2.2338e-03, -2.6181e-03, -2.6646e-03]],\n",
       "                           \n",
       "                                    [[ 7.4866e-04,  1.0575e-02,  2.3810e-02],\n",
       "                                     [ 7.3377e-03, -1.9577e-02,  1.1385e-02],\n",
       "                                     [ 2.1691e-02,  6.1593e-03,  1.5383e-03]],\n",
       "                           \n",
       "                                    [[-1.4507e-02,  3.1747e-02, -5.7960e-03],\n",
       "                                     [-2.1728e-03,  1.8770e-02, -1.3630e-02],\n",
       "                                     [ 1.8217e-02, -2.0671e-02,  2.8211e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 1.0403e-02,  7.0144e-03, -9.6447e-03],\n",
       "                                     [ 1.8818e-03, -6.5313e-02, -3.7445e-03],\n",
       "                                     [-1.9041e-02, -1.7524e-02, -5.7622e-02]],\n",
       "                           \n",
       "                                    [[ 2.7284e-02, -3.1953e-02, -1.9257e-03],\n",
       "                                     [ 6.1404e-02, -2.1827e-02,  6.1964e-02],\n",
       "                                     [ 8.8211e-02, -2.4618e-02,  1.2823e-02]],\n",
       "                           \n",
       "                                    [[-3.9643e-02,  1.9743e-02, -2.7172e-02],\n",
       "                                     [ 9.6107e-02, -3.1254e-02, -2.8527e-02],\n",
       "                                     [ 6.8869e-03, -8.4519e-03,  3.6187e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 5.7214e-03, -4.6520e-03, -2.1643e-02],\n",
       "                                     [ 1.7871e-02, -1.0078e-02, -1.2020e-02],\n",
       "                                     [ 3.2067e-03,  3.3407e-04, -1.7796e-02]],\n",
       "                           \n",
       "                                    [[-2.2683e-02,  4.9650e-03, -4.6257e-02],\n",
       "                                     [-4.0516e-02, -4.4664e-02, -4.0960e-03],\n",
       "                                     [-2.6211e-02, -4.4605e-02, -2.8921e-02]],\n",
       "                           \n",
       "                                    [[ 3.8824e-02,  1.8078e-02, -3.1740e-02],\n",
       "                                     [-3.6132e-02, -2.0113e-02, -6.5713e-03],\n",
       "                                     [ 1.2067e-02, -9.7717e-03, -5.5472e-03]]]], device='cuda:0')),\n",
       "                          ('model.1.model.0.bias',\n",
       "                           tensor([-7.8734e-05, -5.5493e-05, -4.1789e-05, -3.1897e-05, -4.9169e-05,\n",
       "                                    1.5968e-06, -1.2926e-05,  6.3185e-05, -4.4085e-06, -1.1860e-05,\n",
       "                                    5.2699e-06, -1.7358e-05, -5.1527e-05, -5.0192e-05,  6.2909e-05,\n",
       "                                   -6.3986e-05, -7.6552e-05, -7.5252e-05, -1.6205e-05, -7.0586e-05,\n",
       "                                   -1.3260e-05, -8.3728e-05, -7.9535e-05, -6.6733e-07, -5.3473e-05,\n",
       "                                    7.7722e-05, -8.0453e-05,  5.8806e-05,  4.1079e-05, -5.8020e-05,\n",
       "                                    6.5961e-05, -7.6414e-05], device='cuda:0')),\n",
       "                          ('model.1.model.1.weight',\n",
       "                           tensor([0.2790, 0.3472, 0.2472, 0.2473, 0.2893, 0.2392, 0.2474, 0.2360, 0.2311,\n",
       "                                   0.2653, 0.3417, 0.2650, 0.2362, 0.1990, 0.2444, 0.3127, 0.2827, 0.2793,\n",
       "                                   0.2724, 0.2015, 0.2628, 0.2752, 0.2529, 0.2610, 0.2469, 0.2601, 0.2539,\n",
       "                                   0.2484, 0.2801, 0.2535, 0.2738, 0.2367], device='cuda:0')),\n",
       "                          ('model.1.model.1.bias',\n",
       "                           tensor([ 0.0011,  0.0887,  0.0989,  0.0565, -0.0093,  0.0366,  0.0798,  0.1048,\n",
       "                                    0.1050,  0.0788,  0.0101,  0.1244,  0.0524,  0.0632,  0.1117,  0.1780,\n",
       "                                    0.0511,  0.0298,  0.1630,  0.1441,  0.0578,  0.0846,  0.0897, -0.0058,\n",
       "                                    0.0688,  0.0377,  0.2441,  0.1327,  0.0084,  0.0596, -0.0113,  0.0898],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.1.model.1.running_mean',\n",
       "                           tensor([ 0.0088,  0.0304, -0.0186,  0.0331,  0.0345, -0.0142, -0.0149, -0.0144,\n",
       "                                   -0.0050, -0.0077,  0.0054, -0.0163,  0.0190, -0.0106, -0.0176, -0.0217,\n",
       "                                    0.0316, -0.0095, -0.0460,  0.0040, -0.0097, -0.0215, -0.0033,  0.0104,\n",
       "                                    0.0028,  0.0215, -0.0079, -0.0184,  0.0017,  0.0117,  0.0431, -0.0237],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.1.model.1.running_var',\n",
       "                           tensor([0.0031, 0.0173, 0.0042, 0.0065, 0.0039, 0.0035, 0.0035, 0.0032, 0.0042,\n",
       "                                   0.0036, 0.0039, 0.0052, 0.0031, 0.0029, 0.0045, 0.0127, 0.0099, 0.0040,\n",
       "                                   0.0076, 0.0032, 0.0029, 0.0046, 0.0035, 0.0022, 0.0031, 0.0034, 0.0054,\n",
       "                                   0.0052, 0.0032, 0.0040, 0.0060, 0.0033], device='cuda:0')),\n",
       "                          ('model.1.model.1.num_batches_tracked',\n",
       "                           tensor(70314, device='cuda:0')),\n",
       "                          ('model.2.conv.0.weight',\n",
       "                           tensor([[[[-0.0036,  0.0245,  0.0285],\n",
       "                                     [-0.0107,  0.0056, -0.0031],\n",
       "                                     [-0.0193, -0.0195, -0.0026]],\n",
       "                           \n",
       "                                    [[-0.0198, -0.0333, -0.0430],\n",
       "                                     [ 0.0030, -0.0398,  0.0215],\n",
       "                                     [ 0.0493, -0.0294,  0.0185]],\n",
       "                           \n",
       "                                    [[-0.0258, -0.0209,  0.0030],\n",
       "                                     [ 0.0060,  0.0084, -0.0362],\n",
       "                                     [ 0.0222,  0.0142,  0.0076]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-0.0010,  0.0096, -0.0245],\n",
       "                                     [ 0.0090, -0.0299,  0.0546],\n",
       "                                     [-0.0082, -0.0011, -0.0399]],\n",
       "                           \n",
       "                                    [[ 0.0143,  0.0307, -0.0189],\n",
       "                                     [ 0.0149, -0.0191,  0.0008],\n",
       "                                     [-0.0106, -0.0112, -0.0477]],\n",
       "                           \n",
       "                                    [[ 0.0222,  0.0083,  0.0219],\n",
       "                                     [ 0.0096,  0.0263, -0.0134],\n",
       "                                     [ 0.0042,  0.0420,  0.0066]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-0.0064,  0.0057, -0.0251],\n",
       "                                     [-0.0104,  0.0305, -0.0351],\n",
       "                                     [-0.0059,  0.0014, -0.0148]],\n",
       "                           \n",
       "                                    [[-0.0009,  0.0158,  0.0049],\n",
       "                                     [-0.0056, -0.0034,  0.0173],\n",
       "                                     [-0.0182,  0.0017,  0.0196]],\n",
       "                           \n",
       "                                    [[-0.0037, -0.0106, -0.0065],\n",
       "                                     [ 0.0019,  0.0068, -0.0248],\n",
       "                                     [ 0.0007,  0.0016,  0.0067]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-0.0111, -0.0022,  0.0137],\n",
       "                                     [ 0.0023,  0.0086, -0.0049],\n",
       "                                     [-0.0283,  0.0302,  0.0692]],\n",
       "                           \n",
       "                                    [[-0.0215,  0.0250, -0.0017],\n",
       "                                     [-0.0097, -0.0198, -0.0038],\n",
       "                                     [-0.0147,  0.0006,  0.0062]],\n",
       "                           \n",
       "                                    [[-0.0153, -0.0049, -0.0267],\n",
       "                                     [-0.0100, -0.0047, -0.0006],\n",
       "                                     [ 0.0046, -0.0259,  0.0025]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 0.0268,  0.0059,  0.0298],\n",
       "                                     [-0.0078,  0.0153, -0.0196],\n",
       "                                     [-0.0303,  0.0069, -0.0389]],\n",
       "                           \n",
       "                                    [[ 0.0043, -0.0202, -0.0262],\n",
       "                                     [ 0.0214, -0.0350, -0.0056],\n",
       "                                     [ 0.0385,  0.0146,  0.0118]],\n",
       "                           \n",
       "                                    [[-0.0361, -0.0065, -0.0335],\n",
       "                                     [-0.0054,  0.0260,  0.0050],\n",
       "                                     [-0.0217,  0.0049, -0.0102]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-0.0032, -0.0022, -0.0044],\n",
       "                                     [ 0.0069, -0.0172,  0.0234],\n",
       "                                     [ 0.0368, -0.0011,  0.0418]],\n",
       "                           \n",
       "                                    [[-0.0077, -0.0302, -0.0281],\n",
       "                                     [ 0.0226, -0.0113,  0.0072],\n",
       "                                     [ 0.0255, -0.0075,  0.0168]],\n",
       "                           \n",
       "                                    [[ 0.0240,  0.0174,  0.0286],\n",
       "                                     [-0.0034,  0.0048,  0.0229],\n",
       "                                     [-0.0132,  0.0240, -0.0033]]],\n",
       "                           \n",
       "                           \n",
       "                                   ...,\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 0.0697,  0.0339, -0.0113],\n",
       "                                     [ 0.0710, -0.0117, -0.0140],\n",
       "                                     [ 0.0593, -0.0135,  0.0021]],\n",
       "                           \n",
       "                                    [[-0.0168, -0.0166, -0.0054],\n",
       "                                     [ 0.0118, -0.0138, -0.0426],\n",
       "                                     [ 0.0202,  0.0260,  0.0044]],\n",
       "                           \n",
       "                                    [[ 0.0198,  0.0035,  0.0086],\n",
       "                                     [-0.0139, -0.0070, -0.0010],\n",
       "                                     [ 0.0017,  0.0044,  0.0208]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-0.0204,  0.0073,  0.0044],\n",
       "                                     [-0.0250,  0.0016, -0.0053],\n",
       "                                     [ 0.0051,  0.0036,  0.0172]],\n",
       "                           \n",
       "                                    [[ 0.0117,  0.0134, -0.0052],\n",
       "                                     [-0.0167, -0.0105, -0.0213],\n",
       "                                     [ 0.0198, -0.0170, -0.0115]],\n",
       "                           \n",
       "                                    [[ 0.0197, -0.0112,  0.0204],\n",
       "                                     [ 0.0650, -0.0074, -0.0064],\n",
       "                                     [ 0.0189, -0.0131,  0.0041]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-0.0334, -0.0335, -0.0342],\n",
       "                                     [ 0.0079, -0.0291, -0.0196],\n",
       "                                     [-0.0240, -0.0082, -0.0260]],\n",
       "                           \n",
       "                                    [[-0.0116,  0.0418,  0.0076],\n",
       "                                     [-0.0393,  0.0060, -0.0459],\n",
       "                                     [-0.0237,  0.0141, -0.0186]],\n",
       "                           \n",
       "                                    [[ 0.0128, -0.0204, -0.0025],\n",
       "                                     [-0.0105,  0.0069, -0.0065],\n",
       "                                     [ 0.0196,  0.0277,  0.0103]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-0.0074,  0.0195, -0.0022],\n",
       "                                     [ 0.0006,  0.0084,  0.0282],\n",
       "                                     [-0.0300,  0.0182, -0.0142]],\n",
       "                           \n",
       "                                    [[-0.0199,  0.0130, -0.0249],\n",
       "                                     [ 0.0085, -0.0226,  0.0002],\n",
       "                                     [-0.0161, -0.0131, -0.0326]],\n",
       "                           \n",
       "                                    [[-0.0231, -0.0234, -0.0053],\n",
       "                                     [-0.0351,  0.0412, -0.0033],\n",
       "                                     [ 0.0036, -0.0386, -0.0081]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-0.0186,  0.0351,  0.0001],\n",
       "                                     [-0.0144, -0.0203, -0.0173],\n",
       "                                     [-0.0333, -0.0307, -0.0045]],\n",
       "                           \n",
       "                                    [[ 0.0085, -0.0313,  0.0189],\n",
       "                                     [-0.0037, -0.0075, -0.0253],\n",
       "                                     [-0.0126, -0.0184, -0.0073]],\n",
       "                           \n",
       "                                    [[-0.0080, -0.0174, -0.0127],\n",
       "                                     [-0.0096,  0.0011, -0.0299],\n",
       "                                     [-0.0141, -0.0162, -0.0014]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 0.0008, -0.0154,  0.0099],\n",
       "                                     [-0.0135,  0.0364, -0.0185],\n",
       "                                     [ 0.0004, -0.0150,  0.0633]],\n",
       "                           \n",
       "                                    [[-0.0135,  0.0054, -0.0186],\n",
       "                                     [-0.0127, -0.0143,  0.0224],\n",
       "                                     [ 0.0217, -0.0098, -0.0201]],\n",
       "                           \n",
       "                                    [[-0.0045, -0.0071, -0.0021],\n",
       "                                     [ 0.0048,  0.0268, -0.0304],\n",
       "                                     [ 0.0092,  0.0063, -0.0155]]]], device='cuda:0')),\n",
       "                          ('model.2.conv.0.bias',\n",
       "                           tensor([ 1.0724e-05,  9.0350e-06, -3.8198e-05,  3.1278e-05, -4.5785e-05,\n",
       "                                   -5.1288e-05,  4.5290e-05,  2.7332e-05, -4.2568e-05, -6.9386e-06,\n",
       "                                    4.9781e-05,  4.4035e-05,  1.8694e-05, -4.5091e-05,  1.2248e-05,\n",
       "                                   -2.6254e-05,  2.4341e-05,  4.3137e-05, -1.7821e-05, -4.8321e-05,\n",
       "                                    3.3650e-05,  2.5161e-05, -4.0283e-05, -4.7803e-05, -4.4196e-05,\n",
       "                                   -3.9493e-05,  1.9747e-05,  2.3621e-05, -2.1572e-05,  4.9651e-05,\n",
       "                                   -1.9760e-05,  3.7040e-05], device='cuda:0')),\n",
       "                          ('model.2.conv.1.weight',\n",
       "                           tensor([0.1026, 0.1055, 0.1112, 0.1211, 0.1008, 0.1020, 0.1258, 0.1256, 0.1613,\n",
       "                                   0.1005, 0.1353, 0.1649, 0.1647, 0.1303, 0.1091, 0.1491, 0.1239, 0.0983,\n",
       "                                   0.1136, 0.1464, 0.1582, 0.1245, 0.1268, 0.1336, 0.1417, 0.1735, 0.1730,\n",
       "                                   0.1468, 0.1227, 0.1105, 0.1616, 0.1293], device='cuda:0')),\n",
       "                          ('model.2.conv.1.bias',\n",
       "                           tensor([ 0.0071, -0.0327, -0.0354,  0.0156,  0.0159, -0.0159, -0.0096, -0.0272,\n",
       "                                   -0.0649, -0.0239, -0.0164, -0.0348,  0.0222, -0.0028, -0.0321, -0.0331,\n",
       "                                   -0.0190, -0.0020, -0.0045, -0.0675, -0.0295, -0.0571, -0.0102,  0.0027,\n",
       "                                   -0.0370, -0.0874, -0.0546, -0.0379, -0.0391, -0.0643, -0.0075, -0.0209],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.2.conv.1.running_mean',\n",
       "                           tensor([-0.0055, -0.0903, -0.0457, -0.0489,  0.0223, -0.0334, -0.0708, -0.0751,\n",
       "                                   -0.1833,  0.0126, -0.2367, -0.2870, -0.2681, -0.0930,  0.1039, -0.0646,\n",
       "                                   -0.1032, -0.0099, -0.1270, -0.1047, -0.2752,  0.0633, -0.0266, -0.0848,\n",
       "                                    0.0223,  0.0599, -0.4114, -0.1360, -0.0712, -0.0297, -0.1959, -0.0738],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.2.conv.1.running_var',\n",
       "                           tensor([0.0092, 0.0048, 0.0103, 0.0114, 0.0100, 0.0088, 0.0148, 0.0126, 0.0107,\n",
       "                                   0.0099, 0.0181, 0.0191, 0.0321, 0.0255, 0.0126, 0.0161, 0.0130, 0.0071,\n",
       "                                   0.0114, 0.0092, 0.0185, 0.0093, 0.0114, 0.0105, 0.0108, 0.0279, 0.0203,\n",
       "                                   0.0126, 0.0160, 0.0072, 0.0179, 0.0089], device='cuda:0')),\n",
       "                          ('model.2.conv.1.num_batches_tracked',\n",
       "                           tensor(70314, device='cuda:0')),\n",
       "                          ('model.2.conv.3.weight',\n",
       "                           tensor([[[[-1.7200e-02, -2.3185e-02,  3.4156e-03],\n",
       "                                     [ 1.5447e-02,  2.0163e-02, -3.4553e-04],\n",
       "                                     [-2.6156e-02, -1.2583e-02, -1.8325e-03]],\n",
       "                           \n",
       "                                    [[-2.5958e-02,  2.3195e-02,  4.8516e-03],\n",
       "                                     [ 1.2434e-02,  4.4971e-03, -4.4904e-03],\n",
       "                                     [ 1.3041e-02,  1.0335e-02,  1.0382e-03]],\n",
       "                           \n",
       "                                    [[ 6.9060e-03,  1.2268e-02,  1.8484e-02],\n",
       "                                     [-5.7589e-03,  1.7088e-02, -2.7564e-02],\n",
       "                                     [-2.2667e-02, -2.7336e-02,  1.0802e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 4.4714e-02,  2.2777e-02, -2.0706e-02],\n",
       "                                     [ 4.3057e-02,  4.9234e-03, -3.2131e-02],\n",
       "                                     [ 2.4488e-02,  4.6776e-04, -3.5072e-02]],\n",
       "                           \n",
       "                                    [[-2.4486e-02, -3.0229e-02, -1.4201e-02],\n",
       "                                     [ 6.1370e-03, -4.3917e-02, -5.7711e-03],\n",
       "                                     [ 1.2597e-02,  9.0072e-03, -1.3180e-02]],\n",
       "                           \n",
       "                                    [[-4.6153e-02, -2.9999e-02,  1.6367e-02],\n",
       "                                     [-7.1627e-03,  3.8500e-04,  1.7003e-02],\n",
       "                                     [-1.1443e-02,  6.1499e-03, -9.1154e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-1.0382e-02,  1.3163e-02, -7.7858e-03],\n",
       "                                     [-7.5147e-04, -1.0908e-02, -3.1430e-02],\n",
       "                                     [-2.3517e-02, -2.5767e-02, -2.2513e-02]],\n",
       "                           \n",
       "                                    [[-1.0244e-02,  1.0635e-02,  1.6516e-02],\n",
       "                                     [-1.1226e-03,  6.6200e-03,  2.4400e-02],\n",
       "                                     [ 2.4851e-02,  3.4987e-02, -7.1243e-03]],\n",
       "                           \n",
       "                                    [[ 1.6703e-02,  5.4735e-03, -1.1309e-02],\n",
       "                                     [ 1.6588e-03, -3.7809e-03,  2.1284e-02],\n",
       "                                     [ 8.9800e-04,  5.5017e-03,  4.3713e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 4.9310e-04,  2.2948e-03,  9.9884e-03],\n",
       "                                     [ 1.8558e-03,  2.1106e-03,  1.7187e-02],\n",
       "                                     [ 2.9299e-03, -1.0835e-02, -2.1700e-02]],\n",
       "                           \n",
       "                                    [[ 7.0578e-03,  3.7912e-02,  9.7261e-03],\n",
       "                                     [-5.7260e-03,  4.4718e-03,  1.3379e-02],\n",
       "                                     [ 3.8623e-03,  1.5879e-02, -4.0046e-03]],\n",
       "                           \n",
       "                                    [[ 1.0719e-03,  3.4169e-04,  1.0033e-02],\n",
       "                                     [ 1.0460e-02, -1.2896e-02, -6.4430e-03],\n",
       "                                     [ 1.1660e-02, -5.6995e-04, -2.4096e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-1.8352e-02,  2.1638e-02, -8.6836e-03],\n",
       "                                     [-1.4965e-03, -2.9574e-03, -1.1886e-02],\n",
       "                                     [-7.5112e-03,  2.0118e-02, -1.9489e-02]],\n",
       "                           \n",
       "                                    [[ 3.9164e-03,  1.3287e-02,  2.2766e-02],\n",
       "                                     [ 5.2035e-04,  1.8926e-02, -3.9890e-03],\n",
       "                                     [ 2.0255e-02, -2.5212e-02,  1.9099e-02]],\n",
       "                           \n",
       "                                    [[-6.6406e-03, -2.3932e-02,  2.9391e-02],\n",
       "                                     [-2.6397e-02,  1.2865e-02,  2.5837e-02],\n",
       "                                     [-3.4634e-02, -1.5033e-02,  3.1669e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 1.7575e-02,  9.7910e-03,  1.3621e-02],\n",
       "                                     [-1.2393e-02,  2.2039e-03, -2.0202e-02],\n",
       "                                     [ 9.2331e-03,  4.3793e-03, -1.6745e-03]],\n",
       "                           \n",
       "                                    [[-2.2828e-02, -1.6929e-03, -3.9647e-02],\n",
       "                                     [ 1.1322e-02, -3.1167e-02, -3.0455e-02],\n",
       "                                     [ 5.3712e-05, -3.6395e-03, -3.5682e-02]],\n",
       "                           \n",
       "                                    [[-3.6721e-02, -2.2964e-02,  2.4209e-02],\n",
       "                                     [-1.9813e-02,  3.9503e-03,  9.9501e-03],\n",
       "                                     [ 1.4887e-02,  2.2977e-02,  4.4927e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   ...,\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 2.1223e-02,  2.9200e-02, -3.9855e-02],\n",
       "                                     [-3.0197e-02, -9.8879e-03,  1.4752e-02],\n",
       "                                     [ 1.0581e-02,  7.5580e-03,  3.3146e-02]],\n",
       "                           \n",
       "                                    [[-5.3474e-02,  3.7765e-02,  2.8241e-02],\n",
       "                                     [-2.8909e-03,  2.3313e-02,  1.6650e-02],\n",
       "                                     [ 1.5547e-02,  3.4298e-02, -8.1441e-03]],\n",
       "                           \n",
       "                                    [[ 2.2822e-02,  1.1904e-02,  9.5365e-03],\n",
       "                                     [ 1.1700e-02,  7.0035e-03, -1.2784e-03],\n",
       "                                     [ 3.3460e-02,  3.2340e-04,  1.6952e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 6.1540e-03,  8.5645e-03, -3.0964e-02],\n",
       "                                     [-1.5046e-02,  1.1528e-02,  4.1814e-03],\n",
       "                                     [ 1.0557e-02,  5.8605e-04, -1.2150e-02]],\n",
       "                           \n",
       "                                    [[ 4.2302e-03,  2.2841e-02, -1.7962e-02],\n",
       "                                     [-9.0194e-03, -3.7360e-03,  9.9752e-03],\n",
       "                                     [-1.8806e-03, -4.3570e-03, -3.8895e-02]],\n",
       "                           \n",
       "                                    [[-2.3506e-03, -4.6934e-03,  2.6747e-03],\n",
       "                                     [ 9.6430e-03,  1.8284e-02, -4.0827e-02],\n",
       "                                     [-3.3590e-02,  3.8251e-02,  4.5653e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-5.8672e-02,  3.4053e-03, -1.1906e-02],\n",
       "                                     [ 2.0550e-02, -3.2406e-03, -4.6364e-03],\n",
       "                                     [-1.2615e-02,  2.0405e-02,  4.1845e-02]],\n",
       "                           \n",
       "                                    [[ 1.2595e-02,  2.8174e-02, -1.8927e-03],\n",
       "                                     [ 6.0393e-03, -6.1063e-03,  1.5903e-02],\n",
       "                                     [-2.0442e-02, -4.3603e-03,  1.4886e-02]],\n",
       "                           \n",
       "                                    [[-1.9114e-02,  1.8736e-03,  1.5360e-02],\n",
       "                                     [ 4.3560e-03, -3.2740e-03,  1.9500e-02],\n",
       "                                     [-1.3177e-03,  2.5236e-02,  2.1680e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-7.7671e-03,  2.4224e-03, -2.5494e-02],\n",
       "                                     [-2.2912e-02, -1.9693e-02, -3.1665e-02],\n",
       "                                     [-1.2876e-02,  3.1492e-02, -1.1344e-02]],\n",
       "                           \n",
       "                                    [[ 1.8061e-02,  1.7740e-02, -1.6656e-02],\n",
       "                                     [ 4.5279e-02, -2.6897e-02, -9.2844e-03],\n",
       "                                     [ 2.4006e-02,  4.8248e-02, -3.7863e-02]],\n",
       "                           \n",
       "                                    [[ 1.3993e-02, -3.5006e-02,  2.1552e-02],\n",
       "                                     [ 2.1282e-03,  2.4903e-02,  3.0904e-02],\n",
       "                                     [ 1.4571e-02,  1.2034e-02,  4.1437e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-1.6487e-02, -2.5899e-02, -1.0377e-02],\n",
       "                                     [-5.4253e-03,  2.2679e-02,  1.1654e-02],\n",
       "                                     [-9.6797e-03, -5.8127e-03, -1.9904e-02]],\n",
       "                           \n",
       "                                    [[-7.1783e-03,  5.9824e-03,  1.0803e-02],\n",
       "                                     [-3.8689e-03, -1.5500e-02, -1.7188e-02],\n",
       "                                     [ 7.4208e-04, -2.6021e-02, -3.9550e-03]],\n",
       "                           \n",
       "                                    [[ 2.4178e-03, -8.4781e-03,  3.5948e-02],\n",
       "                                     [ 5.4200e-03, -3.8046e-02,  1.4470e-02],\n",
       "                                     [-1.7760e-02,  3.4527e-02, -1.4887e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 1.1816e-02, -3.2681e-03, -4.2800e-02],\n",
       "                                     [ 3.0919e-02, -2.9177e-03,  2.0655e-03],\n",
       "                                     [ 2.4661e-02,  2.4092e-03,  1.1090e-02]],\n",
       "                           \n",
       "                                    [[-4.3537e-03,  1.9898e-02,  5.1087e-02],\n",
       "                                     [-3.2561e-02,  7.8682e-03, -1.8363e-02],\n",
       "                                     [-4.8944e-02, -3.6707e-02,  2.7206e-03]],\n",
       "                           \n",
       "                                    [[-2.8935e-02, -1.9161e-02,  8.0012e-03],\n",
       "                                     [-4.2855e-02,  1.0300e-02, -4.0674e-03],\n",
       "                                     [-2.7270e-02,  3.4981e-03,  1.9962e-03]]]], device='cuda:0')),\n",
       "                          ('model.2.conv.3.bias',\n",
       "                           tensor([ 2.6363e-05, -2.6186e-06,  3.5301e-05,  4.0933e-05,  4.3126e-05,\n",
       "                                   -2.5040e-06, -2.7640e-05, -2.9164e-05,  3.9537e-05, -9.6725e-06,\n",
       "                                   -3.5774e-05, -4.7711e-05, -6.2551e-06,  2.2116e-05, -1.7548e-07,\n",
       "                                   -2.2153e-05, -2.1792e-05, -4.9869e-05,  3.5818e-05, -4.8750e-05,\n",
       "                                   -4.2754e-05,  3.5733e-06, -9.8069e-06,  8.3491e-06,  4.3255e-05,\n",
       "                                    8.1291e-06,  3.5405e-05, -1.2599e-05, -2.7465e-05, -2.0337e-05,\n",
       "                                    9.7450e-06, -2.1836e-05], device='cuda:0')),\n",
       "                          ('model.2.conv.4.weight',\n",
       "                           tensor([0.1825, 0.1031, 0.1483, 0.1339, 0.1405, 0.1441, 0.1352, 0.1101, 0.1125,\n",
       "                                   0.1229, 0.1258, 0.1468, 0.0771, 0.1882, 0.1068, 0.1492, 0.0626, 0.1553,\n",
       "                                   0.1382, 0.1366, 0.1723, 0.1397, 0.1092, 0.1656, 0.1200, 0.1648, 0.1457,\n",
       "                                   0.1134, 0.1319, 0.1193, 0.1503, 0.1399], device='cuda:0')),\n",
       "                          ('model.2.conv.4.bias',\n",
       "                           tensor([-0.0895,  0.0035, -0.0131,  0.0684, -0.0430, -0.0066, -0.0074, -0.0189,\n",
       "                                   -0.0316, -0.0072, -0.0458, -0.0229, -0.0206, -0.0681, -0.0005, -0.0351,\n",
       "                                    0.0195, -0.0371, -0.0609, -0.0311, -0.0706, -0.0504, -0.0270, -0.1059,\n",
       "                                   -0.0211,  0.0040,  0.0184, -0.0358, -0.0277, -0.0896,  0.0355, -0.0139],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.2.conv.4.running_mean',\n",
       "                           tensor([-0.0124, -0.0516, -0.0002, -0.0044, -0.0026, -0.0029,  0.0038, -0.0193,\n",
       "                                   -0.0082,  0.0079, -0.0372,  0.0063, -0.0130, -0.0127,  0.0061, -0.0323,\n",
       "                                   -0.0228, -0.0467, -0.0011, -0.0041, -0.0044, -0.0022, -0.0208, -0.0412,\n",
       "                                   -0.0146, -0.0214,  0.0127, -0.0164, -0.0208,  0.0120,  0.0016,  0.0048],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.2.conv.4.running_var',\n",
       "                           tensor([0.0011, 0.0015, 0.0008, 0.0019, 0.0010, 0.0008, 0.0009, 0.0006, 0.0008,\n",
       "                                   0.0008, 0.0011, 0.0011, 0.0006, 0.0011, 0.0006, 0.0017, 0.0007, 0.0011,\n",
       "                                   0.0013, 0.0007, 0.0010, 0.0009, 0.0006, 0.0011, 0.0007, 0.0011, 0.0017,\n",
       "                                   0.0007, 0.0008, 0.0006, 0.0008, 0.0007], device='cuda:0')),\n",
       "                          ('model.2.conv.4.num_batches_tracked',\n",
       "                           tensor(70314, device='cuda:0')),\n",
       "                          ('model.3.conv.0.weight',\n",
       "                           tensor([[[[ 6.4721e-04,  7.4110e-03,  2.9422e-02],\n",
       "                                     [-4.4068e-03,  7.7433e-03, -6.0281e-03],\n",
       "                                     [-1.0800e-02,  7.8116e-03, -1.8198e-02]],\n",
       "                           \n",
       "                                    [[-3.3271e-03, -7.3130e-03, -4.7986e-02],\n",
       "                                     [-1.4632e-02, -1.7309e-02, -3.1662e-02],\n",
       "                                     [-4.8623e-02, -1.8211e-02, -3.7802e-02]],\n",
       "                           \n",
       "                                    [[-2.1544e-02,  4.8695e-03,  4.3177e-03],\n",
       "                                     [-3.1345e-02,  2.2836e-02,  1.3811e-02],\n",
       "                                     [-3.1189e-02,  1.4256e-02,  8.9154e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-2.9459e-02, -3.0535e-02,  7.0921e-03],\n",
       "                                     [-2.0349e-02, -3.2102e-02, -2.3632e-03],\n",
       "                                     [-1.6038e-02,  1.0550e-02, -1.5413e-02]],\n",
       "                           \n",
       "                                    [[ 7.3065e-03,  2.8218e-02, -8.2770e-03],\n",
       "                                     [-5.0572e-03,  8.9091e-03,  2.7923e-02],\n",
       "                                     [-2.9016e-02,  1.2735e-02, -5.8205e-04]],\n",
       "                           \n",
       "                                    [[-1.4987e-02,  1.2394e-04,  1.6598e-02],\n",
       "                                     [ 2.7382e-02,  1.4735e-02,  1.9529e-02],\n",
       "                                     [ 2.6505e-02, -1.6515e-02, -3.6901e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-1.1198e-02,  8.6177e-03,  2.0955e-04],\n",
       "                                     [-5.2898e-03, -1.1840e-02,  4.0556e-03],\n",
       "                                     [-6.7109e-03, -3.1960e-02, -7.9181e-03]],\n",
       "                           \n",
       "                                    [[ 4.2082e-03, -2.3374e-02, -1.4198e-02],\n",
       "                                     [-9.2948e-03,  1.4936e-02,  3.1318e-02],\n",
       "                                     [-1.4395e-02,  3.1948e-02,  1.6451e-02]],\n",
       "                           \n",
       "                                    [[ 1.3221e-02, -4.1123e-03, -1.4053e-03],\n",
       "                                     [-4.3304e-03, -1.7151e-02, -2.2467e-02],\n",
       "                                     [ 2.9587e-03, -6.6816e-03, -1.9024e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 1.2646e-02, -6.7107e-03, -1.6422e-02],\n",
       "                                     [ 1.6384e-02,  1.4111e-02,  1.6773e-02],\n",
       "                                     [-5.9587e-03,  5.1620e-03,  5.6874e-03]],\n",
       "                           \n",
       "                                    [[ 1.5531e-02, -1.2843e-02, -1.7875e-02],\n",
       "                                     [ 8.7163e-04, -1.5244e-02,  1.0988e-02],\n",
       "                                     [-1.6753e-02,  1.6836e-02, -2.4753e-03]],\n",
       "                           \n",
       "                                    [[-2.8851e-03,  3.0342e-03, -1.8585e-03],\n",
       "                                     [-2.1533e-02,  9.1634e-03,  2.3269e-03],\n",
       "                                     [-2.5645e-02, -9.2368e-03, -1.3935e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-2.8957e-02, -3.9606e-02, -4.2441e-02],\n",
       "                                     [-4.1470e-02, -3.2096e-02, -2.8146e-02],\n",
       "                                     [-2.0027e-02, -3.2497e-02, -4.9090e-03]],\n",
       "                           \n",
       "                                    [[-1.5201e-03,  2.0002e-02,  9.2799e-03],\n",
       "                                     [-3.6187e-02, -3.5237e-02,  2.0889e-02],\n",
       "                                     [-1.9445e-02, -7.4321e-03, -2.6542e-03]],\n",
       "                           \n",
       "                                    [[ 2.7610e-02,  3.1230e-02, -2.6385e-02],\n",
       "                                     [ 6.3653e-03,  1.1711e-02, -4.0613e-02],\n",
       "                                     [ 4.8027e-03,  1.1027e-03, -1.5861e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-1.4409e-02, -2.6649e-02,  3.7552e-03],\n",
       "                                     [-8.7702e-03,  1.3096e-02,  1.6090e-03],\n",
       "                                     [ 1.1402e-02, -2.4483e-02, -1.7835e-02]],\n",
       "                           \n",
       "                                    [[ 3.8400e-03, -2.3421e-02, -2.4529e-02],\n",
       "                                     [ 1.1409e-02,  2.6195e-03,  3.3022e-03],\n",
       "                                     [ 1.3493e-02,  3.0023e-03, -2.9551e-02]],\n",
       "                           \n",
       "                                    [[-2.4205e-02, -1.1230e-02,  2.8924e-03],\n",
       "                                     [-2.2844e-02, -7.9834e-03, -1.7507e-02],\n",
       "                                     [-2.9973e-02, -2.4452e-02, -1.6602e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   ...,\n",
       "                           \n",
       "                           \n",
       "                                   [[[-4.7229e-04,  4.2070e-03,  6.1337e-02],\n",
       "                                     [-6.2016e-03, -1.5979e-03,  5.3660e-02],\n",
       "                                     [ 4.8735e-02,  7.2251e-03,  2.8351e-02]],\n",
       "                           \n",
       "                                    [[-3.6055e-02, -2.3249e-02, -5.4470e-02],\n",
       "                                     [ 1.0574e-02,  8.8165e-03, -6.4776e-02],\n",
       "                                     [ 9.4150e-03,  4.0721e-03, -2.0285e-03]],\n",
       "                           \n",
       "                                    [[-1.1558e-03,  4.0669e-02,  2.4528e-02],\n",
       "                                     [-3.8028e-03,  1.4190e-02,  7.5513e-02],\n",
       "                                     [ 3.2291e-03,  3.4666e-02, -1.2853e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 5.0296e-03,  4.7247e-02, -8.7110e-03],\n",
       "                                     [-4.0562e-02, -3.0492e-02,  5.8587e-02],\n",
       "                                     [-7.3036e-03, -9.1548e-03, -1.8541e-02]],\n",
       "                           \n",
       "                                    [[-3.3823e-02,  7.8913e-03,  6.3456e-02],\n",
       "                                     [-1.1696e-02,  6.1191e-03,  3.2881e-02],\n",
       "                                     [-4.8400e-03, -1.3358e-02,  7.7647e-03]],\n",
       "                           \n",
       "                                    [[-5.0779e-03, -1.7803e-02, -2.8712e-02],\n",
       "                                     [ 1.8261e-04, -4.3664e-03,  1.7193e-02],\n",
       "                                     [ 1.1075e-02,  1.2737e-02, -3.6294e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-3.2558e-02, -2.4719e-02, -1.4261e-02],\n",
       "                                     [-3.5281e-02, -2.9109e-02, -1.2651e-02],\n",
       "                                     [-1.7527e-02, -1.3180e-02, -1.1907e-03]],\n",
       "                           \n",
       "                                    [[-5.9507e-03, -3.0774e-02, -7.2758e-03],\n",
       "                                     [ 1.4011e-02, -4.3292e-02, -2.8204e-02],\n",
       "                                     [-2.3033e-02, -4.4223e-03, -2.8877e-02]],\n",
       "                           \n",
       "                                    [[ 5.5442e-03, -9.4763e-03,  3.2418e-03],\n",
       "                                     [ 5.9249e-03,  9.2253e-03,  1.2784e-02],\n",
       "                                     [ 8.2190e-03,  4.5043e-05,  1.0231e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 3.7693e-03,  1.3702e-02, -9.7832e-04],\n",
       "                                     [ 4.0140e-03, -1.2957e-02,  8.1026e-03],\n",
       "                                     [ 1.2960e-02,  2.4396e-03, -2.2120e-02]],\n",
       "                           \n",
       "                                    [[-2.5762e-02, -2.9386e-02, -2.5603e-02],\n",
       "                                     [-9.0239e-04,  7.0706e-03, -1.0048e-02],\n",
       "                                     [ 2.1313e-02,  2.5469e-02, -1.5128e-02]],\n",
       "                           \n",
       "                                    [[-1.4762e-02,  2.0966e-02,  3.9394e-03],\n",
       "                                     [ 7.1771e-03, -5.5968e-03,  1.3643e-02],\n",
       "                                     [-3.1616e-02,  8.1391e-04, -5.9150e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 1.3575e-02, -9.8089e-04,  2.9972e-03],\n",
       "                                     [ 4.7217e-02, -5.5540e-03,  4.1806e-03],\n",
       "                                     [ 2.6006e-02,  4.5582e-04,  5.4629e-03]],\n",
       "                           \n",
       "                                    [[ 4.9540e-03, -2.5429e-02, -7.3072e-03],\n",
       "                                     [ 7.0034e-03, -2.2867e-02, -3.3644e-02],\n",
       "                                     [-4.7101e-03, -2.2648e-02, -7.0841e-03]],\n",
       "                           \n",
       "                                    [[-1.8303e-02,  1.7328e-02,  1.4971e-02],\n",
       "                                     [ 7.3921e-03, -1.6922e-03, -1.4509e-02],\n",
       "                                     [-1.2536e-02,  4.1249e-03, -2.5874e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-1.1496e-02,  7.4232e-03,  9.4337e-03],\n",
       "                                     [-2.0409e-02, -3.9193e-02, -2.0395e-02],\n",
       "                                     [-2.1895e-02, -2.6981e-02, -3.1500e-02]],\n",
       "                           \n",
       "                                    [[-3.9851e-03, -1.0399e-02,  8.9408e-04],\n",
       "                                     [-1.5432e-02, -1.1706e-02, -2.2544e-02],\n",
       "                                     [-6.0910e-03, -1.5697e-02, -3.0482e-02]],\n",
       "                           \n",
       "                                    [[-1.8584e-02, -2.1782e-02, -3.0264e-02],\n",
       "                                     [-1.4089e-02, -1.4829e-02,  2.1193e-03],\n",
       "                                     [ 1.5881e-02, -1.0079e-02, -3.6270e-02]]]], device='cuda:0')),\n",
       "                          ('model.3.conv.0.bias',\n",
       "                           tensor([-3.1263e-05, -2.4204e-05,  2.1462e-05,  1.3927e-05, -2.4820e-06,\n",
       "                                    2.7386e-05,  3.1544e-06,  4.7926e-05,  2.2497e-05,  1.8228e-05,\n",
       "                                    9.0429e-06,  1.0902e-05,  1.5502e-05,  3.6153e-05, -4.3119e-05,\n",
       "                                    1.6623e-05, -4.6058e-05,  1.5637e-05, -5.0744e-05,  3.8525e-05,\n",
       "                                    4.9037e-05,  2.4154e-05,  3.3874e-05, -2.0820e-05, -4.1143e-05,\n",
       "                                    4.7547e-05,  5.1040e-05, -1.9157e-06,  4.2861e-05,  6.5941e-06,\n",
       "                                   -3.4248e-05,  3.5501e-05], device='cuda:0')),\n",
       "                          ('model.3.conv.1.weight',\n",
       "                           tensor([0.1165, 0.1114, 0.1528, 0.1052, 0.1201, 0.1075, 0.1369, 0.0990, 0.0962,\n",
       "                                   0.1080, 0.1228, 0.1106, 0.1413, 0.1254, 0.1773, 0.1098, 0.1026, 0.0826,\n",
       "                                   0.1132, 0.1001, 0.1517, 0.1208, 0.1433, 0.1580, 0.1425, 0.1409, 0.1233,\n",
       "                                   0.1077, 0.1573, 0.1414, 0.1183, 0.1248], device='cuda:0')),\n",
       "                          ('model.3.conv.1.bias',\n",
       "                           tensor([-0.0187, -0.0200, -0.0324, -0.0206, -0.0255, -0.0303,  0.0119, -0.0132,\n",
       "                                   -0.0071, -0.0216, -0.0718, -0.0245, -0.0647, -0.0060, -0.0820, -0.0007,\n",
       "                                   -0.0240,  0.0399, -0.0372,  0.0242, -0.0360, -0.0010, -0.0705, -0.0658,\n",
       "                                   -0.0410, -0.0231, -0.0421, -0.0669, -0.0529, -0.0315, -0.0197, -0.0638],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.3.conv.1.running_mean',\n",
       "                           tensor([ 1.5097e-04, -2.5150e-02, -1.7932e-01, -1.0004e-01, -2.4562e-02,\n",
       "                                   -7.5749e-02, -1.7832e-01,  4.6363e-02,  1.8953e-02,  3.8662e-02,\n",
       "                                   -1.3110e-02,  8.6612e-03,  8.4778e-02, -6.3164e-02, -1.5089e-01,\n",
       "                                   -2.6319e-02, -1.4445e-02, -4.6012e-02, -1.6648e-02,  4.3951e-02,\n",
       "                                   -7.8591e-02,  1.4926e-02, -9.8780e-02, -5.6854e-02,  5.6661e-02,\n",
       "                                    8.3091e-03, -1.7279e-01,  8.1694e-02, -1.8819e-02, -5.9245e-02,\n",
       "                                   -8.8075e-02, -1.4473e-01], device='cuda:0')),\n",
       "                          ('model.3.conv.1.running_var',\n",
       "                           tensor([0.0084, 0.0067, 0.0155, 0.0108, 0.0080, 0.0082, 0.0161, 0.0113, 0.0075,\n",
       "                                   0.0089, 0.0097, 0.0070, 0.0122, 0.0104, 0.0142, 0.0079, 0.0062, 0.0112,\n",
       "                                   0.0086, 0.0129, 0.0181, 0.0088, 0.0088, 0.0109, 0.0121, 0.0199, 0.0140,\n",
       "                                   0.0071, 0.0106, 0.0091, 0.0112, 0.0075], device='cuda:0')),\n",
       "                          ('model.3.conv.1.num_batches_tracked',\n",
       "                           tensor(70314, device='cuda:0')),\n",
       "                          ('model.3.conv.3.weight',\n",
       "                           tensor([[[[-9.1707e-03, -3.0702e-03,  2.0920e-03],\n",
       "                                     [-7.2823e-03, -4.8393e-03, -7.5842e-03],\n",
       "                                     [-9.1792e-03,  1.5688e-03, -1.7270e-02]],\n",
       "                           \n",
       "                                    [[ 6.9077e-03,  3.6991e-02,  2.1727e-02],\n",
       "                                     [-4.4658e-03,  9.5891e-03, -2.5217e-04],\n",
       "                                     [-1.0395e-02,  2.5136e-02,  3.9467e-03]],\n",
       "                           \n",
       "                                    [[-1.8195e-02, -4.7438e-02, -3.5257e-02],\n",
       "                                     [-2.8360e-02, -2.9869e-02, -1.5091e-02],\n",
       "                                     [-2.5976e-02, -2.3946e-02, -1.4051e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-1.7414e-02,  2.6972e-02,  1.3069e-02],\n",
       "                                     [-1.7124e-02,  1.3568e-02, -7.4963e-03],\n",
       "                                     [-4.4022e-03, -1.4747e-02, -2.6675e-02]],\n",
       "                           \n",
       "                                    [[-4.5331e-03, -2.4357e-02,  1.2298e-02],\n",
       "                                     [-3.7305e-03, -1.3529e-02,  2.4192e-02],\n",
       "                                     [-7.4072e-03, -1.3177e-02, -1.5955e-02]],\n",
       "                           \n",
       "                                    [[ 1.2559e-03, -9.6637e-03,  6.6458e-03],\n",
       "                                     [ 9.8046e-03, -1.5236e-02,  8.8276e-03],\n",
       "                                     [ 1.3712e-02, -4.3934e-02, -1.3050e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 8.0411e-03, -5.7428e-03,  3.3926e-02],\n",
       "                                     [-8.1580e-03, -1.6079e-02,  1.9363e-02],\n",
       "                                     [-1.7537e-02, -2.4064e-02, -1.2833e-02]],\n",
       "                           \n",
       "                                    [[ 2.7922e-02,  1.0103e-02, -8.2999e-03],\n",
       "                                     [-1.7686e-03, -1.7673e-02, -8.4495e-03],\n",
       "                                     [-1.0606e-02, -1.8372e-03, -2.4769e-02]],\n",
       "                           \n",
       "                                    [[ 1.9909e-02,  3.8698e-02,  7.8567e-03],\n",
       "                                     [ 2.6616e-02,  3.8430e-02,  1.8565e-02],\n",
       "                                     [ 1.0397e-02, -9.7880e-05, -1.3855e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-1.1824e-02,  3.1936e-03, -2.2281e-02],\n",
       "                                     [-1.4167e-02,  1.7776e-02,  3.1239e-02],\n",
       "                                     [ 7.7937e-03,  2.4463e-02,  2.2500e-02]],\n",
       "                           \n",
       "                                    [[-7.5339e-05,  1.6178e-02,  9.2128e-03],\n",
       "                                     [ 1.1487e-02, -4.0771e-02,  1.1398e-03],\n",
       "                                     [-1.4259e-02, -3.5605e-02, -1.0818e-02]],\n",
       "                           \n",
       "                                    [[-1.0448e-02,  4.0135e-05,  6.2728e-03],\n",
       "                                     [ 2.9388e-02,  1.4510e-03, -1.6211e-03],\n",
       "                                     [ 4.2068e-03,  3.1291e-02, -7.0830e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 8.9664e-04,  4.8308e-03,  1.4110e-02],\n",
       "                                     [ 1.7161e-04,  7.5778e-04,  1.6168e-02],\n",
       "                                     [-3.4658e-02, -1.0772e-02,  4.4114e-03]],\n",
       "                           \n",
       "                                    [[-2.6504e-02, -2.4344e-02, -1.9957e-02],\n",
       "                                     [-1.2680e-02, -5.6967e-03, -9.0063e-03],\n",
       "                                     [-2.2773e-02,  1.6635e-02,  1.5041e-02]],\n",
       "                           \n",
       "                                    [[-1.0428e-02,  1.3562e-02,  7.9037e-03],\n",
       "                                     [-2.0606e-03,  7.4982e-03,  1.0156e-02],\n",
       "                                     [-1.5518e-02,  1.2677e-02, -1.8695e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-1.8300e-02, -1.3236e-02, -9.9837e-04],\n",
       "                                     [ 1.0755e-02,  1.1335e-02, -2.2985e-02],\n",
       "                                     [-7.8024e-04,  1.2935e-02, -1.8233e-03]],\n",
       "                           \n",
       "                                    [[-6.2525e-04, -1.0141e-02, -2.0882e-03],\n",
       "                                     [-2.8115e-02, -8.6831e-03, -1.6783e-02],\n",
       "                                     [-1.8862e-02, -4.9363e-02, -2.8214e-02]],\n",
       "                           \n",
       "                                    [[ 7.5625e-03,  1.5567e-03, -4.6308e-03],\n",
       "                                     [ 1.1878e-02,  7.1661e-03,  1.2583e-03],\n",
       "                                     [-1.2849e-02, -2.1244e-02, -3.2604e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   ...,\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 1.3175e-02,  6.8949e-03,  1.3883e-02],\n",
       "                                     [ 1.5121e-02,  3.9975e-03, -1.3360e-02],\n",
       "                                     [ 5.0435e-02,  1.1627e-02,  3.2793e-02]],\n",
       "                           \n",
       "                                    [[-1.0240e-02, -7.4992e-04, -1.1456e-02],\n",
       "                                     [-8.3754e-03, -1.8276e-03, -1.5780e-03],\n",
       "                                     [-1.9622e-03, -2.8962e-02, -1.7058e-02]],\n",
       "                           \n",
       "                                    [[-2.4955e-02, -1.3983e-02, -4.6188e-03],\n",
       "                                     [-1.6025e-02, -3.0504e-03, -9.6239e-03],\n",
       "                                     [-8.5637e-04,  7.6626e-03, -9.0345e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-2.7417e-02,  3.1154e-02, -2.0824e-02],\n",
       "                                     [-1.6862e-02, -1.0390e-02,  7.2500e-02],\n",
       "                                     [-6.9471e-03,  2.9802e-02,  1.5694e-02]],\n",
       "                           \n",
       "                                    [[-1.1280e-02, -1.7325e-02, -2.5681e-02],\n",
       "                                     [-2.6491e-02, -1.2383e-02, -5.1248e-02],\n",
       "                                     [-1.6806e-02, -3.9541e-02, -2.6872e-02]],\n",
       "                           \n",
       "                                    [[ 5.2422e-03, -1.4290e-02, -2.8525e-02],\n",
       "                                     [-7.2498e-03, -3.3466e-02, -1.8939e-02],\n",
       "                                     [-8.7715e-03, -7.2115e-03, -2.7129e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 1.2958e-02, -5.5580e-03, -7.8374e-03],\n",
       "                                     [-4.2997e-03, -1.7859e-02, -9.3983e-03],\n",
       "                                     [ 2.4324e-03,  3.2376e-03,  5.4492e-03]],\n",
       "                           \n",
       "                                    [[-1.4872e-03,  1.5610e-02,  8.4169e-03],\n",
       "                                     [ 1.5939e-02, -3.5326e-02, -1.6019e-02],\n",
       "                                     [ 4.0717e-02,  7.6218e-03, -1.1233e-02]],\n",
       "                           \n",
       "                                    [[-2.3270e-02, -1.5351e-02, -2.0551e-03],\n",
       "                                     [-1.9014e-02,  1.1666e-02, -6.6765e-03],\n",
       "                                     [-1.3227e-02,  4.9735e-03,  2.2856e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-1.6948e-03, -3.2310e-02,  3.0641e-02],\n",
       "                                     [ 8.2649e-03, -1.9092e-02, -3.9242e-02],\n",
       "                                     [ 2.4363e-02, -1.3273e-02,  7.6469e-03]],\n",
       "                           \n",
       "                                    [[-1.5154e-02, -2.0663e-02, -9.9835e-03],\n",
       "                                     [ 1.8852e-03,  2.7924e-02, -8.7970e-03],\n",
       "                                     [-4.5296e-03,  3.5434e-02, -4.8699e-03]],\n",
       "                           \n",
       "                                    [[-2.5493e-02, -1.0762e-02, -1.6858e-02],\n",
       "                                     [-3.3913e-03, -9.3662e-03, -1.6355e-02],\n",
       "                                     [ 1.1866e-02, -2.4878e-02,  1.9282e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-1.4938e-02, -6.5739e-03,  1.0746e-02],\n",
       "                                     [-6.5927e-03,  9.2825e-03, -1.6074e-03],\n",
       "                                     [ 2.1824e-02,  3.6553e-03,  3.0012e-02]],\n",
       "                           \n",
       "                                    [[-2.8804e-02,  3.9047e-03, -1.4726e-02],\n",
       "                                     [-1.9571e-02, -6.0595e-04, -1.0569e-03],\n",
       "                                     [ 3.9144e-02,  1.0133e-02,  1.6482e-02]],\n",
       "                           \n",
       "                                    [[ 4.1805e-02,  2.3547e-02,  2.3188e-02],\n",
       "                                     [ 4.5803e-03, -6.3451e-03, -6.9154e-03],\n",
       "                                     [-9.3817e-03, -1.9862e-02, -3.7515e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 1.0588e-02, -1.3577e-02, -1.7806e-02],\n",
       "                                     [-4.0850e-03, -2.5832e-02,  8.5239e-04],\n",
       "                                     [ 1.8124e-02, -6.9725e-03,  6.5846e-03]],\n",
       "                           \n",
       "                                    [[-1.9246e-02, -1.5957e-02, -1.6810e-02],\n",
       "                                     [ 3.8042e-03, -8.1131e-03,  1.5741e-02],\n",
       "                                     [-1.9229e-02,  1.4598e-05,  1.9941e-03]],\n",
       "                           \n",
       "                                    [[ 1.5604e-02,  1.8998e-02, -1.9632e-02],\n",
       "                                     [ 2.2334e-04, -1.4823e-02, -2.6407e-02],\n",
       "                                     [-2.7790e-02, -1.7483e-03, -2.2242e-02]]]], device='cuda:0')),\n",
       "                          ('model.3.conv.3.bias',\n",
       "                           tensor([ 2.4137e-06, -1.7362e-05, -1.6929e-05, -2.9787e-05, -1.9039e-05,\n",
       "                                   -1.9578e-05,  9.2579e-06, -4.0256e-05, -2.2377e-05, -1.4843e-05,\n",
       "                                   -4.0440e-05,  9.9791e-06, -4.7353e-05, -5.1296e-05, -4.2589e-05,\n",
       "                                   -5.0268e-05,  4.1582e-05,  2.2766e-05, -2.1615e-05,  4.1642e-05,\n",
       "                                   -4.6411e-05,  4.5100e-05,  1.3828e-05, -1.2923e-06, -1.9269e-05,\n",
       "                                   -3.7863e-05, -3.0752e-05,  4.0580e-05,  4.3838e-05, -1.6773e-05,\n",
       "                                   -3.4876e-05,  1.4545e-05], device='cuda:0')),\n",
       "                          ('model.3.conv.4.weight',\n",
       "                           tensor([0.0964, 0.1309, 0.1461, 0.0882, 0.1096, 0.1295, 0.1298, 0.1378, 0.1941,\n",
       "                                   0.1408, 0.1353, 0.1044, 0.1002, 0.1225, 0.1349, 0.1004, 0.1400, 0.1106,\n",
       "                                   0.1624, 0.1800, 0.1049, 0.1352, 0.1439, 0.2055, 0.1363, 0.0995, 0.0962,\n",
       "                                   0.1385, 0.1590, 0.1603, 0.1457, 0.1388], device='cuda:0')),\n",
       "                          ('model.3.conv.4.bias',\n",
       "                           tensor([-0.0509, -0.0331, -0.0353, -0.0161,  0.0115, -0.0428, -0.0061, -0.0422,\n",
       "                                   -0.0707, -0.0149,  0.0806, -0.0248,  0.0367, -0.0023, -0.0503, -0.0271,\n",
       "                                   -0.0203, -0.0697, -0.0442, -0.0762, -0.0107,  0.0161, -0.0755,  0.0304,\n",
       "                                    0.0033, -0.0092, -0.0246, -0.0355, -0.0366, -0.0290, -0.0402, -0.0568],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.3.conv.4.running_mean',\n",
       "                           tensor([-0.0160, -0.0263,  0.0052, -0.0002,  0.0141, -0.0034, -0.0025, -0.0417,\n",
       "                                    0.0004,  0.0062, -0.0119, -0.0193, -0.0004, -0.0180,  0.0074, -0.0119,\n",
       "                                   -0.0022, -0.0260, -0.0379,  0.0019, -0.0268, -0.0089,  0.0134, -0.0012,\n",
       "                                    0.0148, -0.0015, -0.0269, -0.0162, -0.0029, -0.0094, -0.0149, -0.0118],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.3.conv.4.running_var',\n",
       "                           tensor([0.0004, 0.0009, 0.0008, 0.0008, 0.0006, 0.0004, 0.0008, 0.0007, 0.0008,\n",
       "                                   0.0012, 0.0013, 0.0006, 0.0005, 0.0009, 0.0007, 0.0008, 0.0007, 0.0005,\n",
       "                                   0.0015, 0.0009, 0.0008, 0.0007, 0.0006, 0.0013, 0.0007, 0.0005, 0.0007,\n",
       "                                   0.0005, 0.0006, 0.0006, 0.0006, 0.0006], device='cuda:0')),\n",
       "                          ('model.3.conv.4.num_batches_tracked',\n",
       "                           tensor(70314, device='cuda:0')),\n",
       "                          ('model.4.conv.0.weight',\n",
       "                           tensor([[[[ 0.0124,  0.0071,  0.0032],\n",
       "                                     [-0.0305, -0.0043,  0.0113],\n",
       "                                     [-0.0318, -0.0164,  0.0174]],\n",
       "                           \n",
       "                                    [[-0.0150,  0.0607, -0.0095],\n",
       "                                     [ 0.0187,  0.0025,  0.0131],\n",
       "                                     [ 0.0047,  0.0139,  0.0173]],\n",
       "                           \n",
       "                                    [[-0.0065,  0.0056, -0.0066],\n",
       "                                     [ 0.0016,  0.0146, -0.0007],\n",
       "                                     [ 0.0006,  0.0053, -0.0008]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-0.0322,  0.0647,  0.0053],\n",
       "                                     [-0.0081,  0.0119,  0.0134],\n",
       "                                     [-0.0253,  0.0279, -0.0401]],\n",
       "                           \n",
       "                                    [[ 0.0381,  0.0216,  0.0169],\n",
       "                                     [-0.0002,  0.0086, -0.0261],\n",
       "                                     [ 0.0142, -0.0070,  0.0129]],\n",
       "                           \n",
       "                                    [[ 0.0125,  0.0154,  0.0087],\n",
       "                                     [ 0.0296, -0.0210,  0.0099],\n",
       "                                     [-0.0024,  0.0092, -0.0229]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 0.0163,  0.0004, -0.0126],\n",
       "                                     [ 0.0024,  0.0051, -0.0074],\n",
       "                                     [-0.0168,  0.0034,  0.0003]],\n",
       "                           \n",
       "                                    [[ 0.0157, -0.0130, -0.0110],\n",
       "                                     [-0.0240,  0.0085, -0.0023],\n",
       "                                     [-0.0151, -0.0097,  0.0099]],\n",
       "                           \n",
       "                                    [[ 0.0222,  0.0017, -0.0048],\n",
       "                                     [-0.0469, -0.0057,  0.0343],\n",
       "                                     [-0.0200,  0.0035, -0.0179]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-0.0307, -0.0237,  0.0107],\n",
       "                                     [-0.0043, -0.0125,  0.0127],\n",
       "                                     [-0.0232, -0.0254, -0.0062]],\n",
       "                           \n",
       "                                    [[ 0.0049,  0.0001, -0.0112],\n",
       "                                     [ 0.0214,  0.0197, -0.0024],\n",
       "                                     [ 0.0143,  0.0223,  0.0005]],\n",
       "                           \n",
       "                                    [[-0.0189, -0.0178,  0.0198],\n",
       "                                     [-0.0129, -0.0092, -0.0378],\n",
       "                                     [-0.0054, -0.0068,  0.0193]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 0.0040,  0.0073, -0.0064],\n",
       "                                     [ 0.0087,  0.0085, -0.0212],\n",
       "                                     [ 0.0141,  0.0144,  0.0147]],\n",
       "                           \n",
       "                                    [[-0.0255, -0.0502, -0.0068],\n",
       "                                     [-0.0485, -0.0237, -0.0224],\n",
       "                                     [-0.0370, -0.0425, -0.0181]],\n",
       "                           \n",
       "                                    [[-0.0512, -0.0168, -0.0168],\n",
       "                                     [-0.0183, -0.0206,  0.0047],\n",
       "                                     [-0.0363, -0.0248, -0.0234]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 0.0179, -0.0270,  0.0320],\n",
       "                                     [ 0.0015, -0.0013, -0.0500],\n",
       "                                     [-0.0162, -0.0103,  0.0434]],\n",
       "                           \n",
       "                                    [[-0.0006, -0.0009, -0.0215],\n",
       "                                     [ 0.0173, -0.0051,  0.0114],\n",
       "                                     [-0.0395,  0.0333, -0.0201]],\n",
       "                           \n",
       "                                    [[-0.0036,  0.0072,  0.0230],\n",
       "                                     [ 0.0167, -0.0192,  0.0111],\n",
       "                                     [ 0.0103, -0.0090, -0.0040]]],\n",
       "                           \n",
       "                           \n",
       "                                   ...,\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 0.0031,  0.0153, -0.0037],\n",
       "                                     [ 0.0015,  0.0258,  0.0146],\n",
       "                                     [ 0.0013,  0.0046, -0.0044]],\n",
       "                           \n",
       "                                    [[-0.0136,  0.0267, -0.0021],\n",
       "                                     [ 0.0101,  0.0358,  0.0291],\n",
       "                                     [-0.0366, -0.0181,  0.0152]],\n",
       "                           \n",
       "                                    [[-0.0038, -0.0135, -0.0236],\n",
       "                                     [-0.0151, -0.0126,  0.0102],\n",
       "                                     [-0.0012,  0.0042, -0.0217]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-0.0094,  0.0015, -0.0295],\n",
       "                                     [-0.0004,  0.0045, -0.0044],\n",
       "                                     [ 0.0044, -0.0040, -0.0075]],\n",
       "                           \n",
       "                                    [[-0.0060, -0.0109, -0.0131],\n",
       "                                     [ 0.0175, -0.0097, -0.0108],\n",
       "                                     [-0.0073, -0.0105, -0.0089]],\n",
       "                           \n",
       "                                    [[-0.0255,  0.0072, -0.0108],\n",
       "                                     [-0.0167, -0.0054, -0.0049],\n",
       "                                     [-0.0057,  0.0129,  0.0155]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-0.0021, -0.0030,  0.0133],\n",
       "                                     [-0.0311,  0.0034, -0.0118],\n",
       "                                     [-0.0102,  0.0021,  0.0061]],\n",
       "                           \n",
       "                                    [[-0.0069,  0.0262, -0.0420],\n",
       "                                     [ 0.0337,  0.0279, -0.0179],\n",
       "                                     [-0.0375,  0.0259,  0.0025]],\n",
       "                           \n",
       "                                    [[-0.0118, -0.0005, -0.0046],\n",
       "                                     [-0.0148, -0.0052, -0.0047],\n",
       "                                     [-0.0188, -0.0120, -0.0068]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-0.0034, -0.0286, -0.0056],\n",
       "                                     [ 0.0065, -0.0130,  0.0008],\n",
       "                                     [ 0.0251, -0.0116,  0.0013]],\n",
       "                           \n",
       "                                    [[-0.0270, -0.0307,  0.0185],\n",
       "                                     [ 0.0021, -0.0205, -0.0078],\n",
       "                                     [-0.0147, -0.0052, -0.0214]],\n",
       "                           \n",
       "                                    [[ 0.0311,  0.0163,  0.0116],\n",
       "                                     [-0.0136, -0.0109, -0.0112],\n",
       "                                     [-0.0084,  0.0155, -0.0080]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-0.0160, -0.0103, -0.0225],\n",
       "                                     [ 0.0279, -0.0229, -0.0164],\n",
       "                                     [ 0.0082, -0.0282, -0.0055]],\n",
       "                           \n",
       "                                    [[ 0.0410,  0.0063, -0.0232],\n",
       "                                     [-0.0083, -0.0284,  0.0319],\n",
       "                                     [ 0.0287,  0.0521,  0.0580]],\n",
       "                           \n",
       "                                    [[ 0.0322,  0.0270,  0.0022],\n",
       "                                     [ 0.0324, -0.0174, -0.0048],\n",
       "                                     [ 0.0225,  0.0102,  0.0225]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 0.0062, -0.0084, -0.0132],\n",
       "                                     [-0.0227, -0.0216, -0.0035],\n",
       "                                     [ 0.0074,  0.0061,  0.0308]],\n",
       "                           \n",
       "                                    [[-0.0094,  0.0059, -0.0346],\n",
       "                                     [-0.0123, -0.0007, -0.0188],\n",
       "                                     [-0.0154,  0.0092, -0.0241]],\n",
       "                           \n",
       "                                    [[-0.0036, -0.0017,  0.0123],\n",
       "                                     [-0.0098,  0.0126, -0.0044],\n",
       "                                     [-0.0183, -0.0073,  0.0224]]]], device='cuda:0')),\n",
       "                          ('model.4.conv.0.bias',\n",
       "                           tensor([ 2.2277e-05,  1.0901e-05,  2.7969e-05, -3.8921e-05, -3.9229e-05,\n",
       "                                    4.1604e-05, -3.0928e-05, -4.7567e-05, -5.0945e-06,  1.8805e-05,\n",
       "                                    5.3264e-06, -2.0264e-05,  4.8662e-05,  1.6023e-05,  2.7959e-05,\n",
       "                                   -3.7696e-05,  3.3178e-05, -2.6415e-05, -6.0793e-06, -4.1863e-05,\n",
       "                                    3.8041e-05,  2.6959e-05, -4.9481e-05, -3.9650e-05,  9.4048e-06,\n",
       "                                    2.2314e-05, -1.8082e-05,  3.0918e-05, -4.9311e-05, -2.4128e-05,\n",
       "                                   -2.8340e-05, -3.5382e-05], device='cuda:0')),\n",
       "                          ('model.4.conv.1.weight',\n",
       "                           tensor([0.1084, 0.1076, 0.1354, 0.1167, 0.1479, 0.0922, 0.1212, 0.1226, 0.1085,\n",
       "                                   0.1078, 0.1121, 0.1121, 0.1016, 0.1269, 0.1036, 0.1268, 0.1179, 0.1170,\n",
       "                                   0.1202, 0.1130, 0.1265, 0.1438, 0.0924, 0.1084, 0.1183, 0.0967, 0.1124,\n",
       "                                   0.1397, 0.0866, 0.1042, 0.1344, 0.1362], device='cuda:0')),\n",
       "                          ('model.4.conv.1.bias',\n",
       "                           tensor([-0.0151, -0.0360, -0.0466, -0.0594, -0.0400, -0.0211, -0.0712, -0.0496,\n",
       "                                   -0.0524, -0.0359, -0.0564,  0.0247,  0.0094, -0.0365, -0.0335, -0.0460,\n",
       "                                   -0.0165, -0.0476, -0.0229,  0.0053, -0.0492, -0.0428, -0.0220, -0.0453,\n",
       "                                   -0.0858,  0.0182, -0.0497, -0.0767, -0.0006, -0.0276, -0.0652, -0.0348],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.4.conv.1.running_mean',\n",
       "                           tensor([ 0.0230, -0.0191, -0.1255, -0.0507, -0.1738,  0.0152, -0.0921, -0.0327,\n",
       "                                    0.1323, -0.0430,  0.0337, -0.1038,  0.0418, -0.1468, -0.0121, -0.0729,\n",
       "                                   -0.0973,  0.0368, -0.0098, -0.0600,  0.1050, -0.2183,  0.0296,  0.0293,\n",
       "                                    0.0614, -0.1471, -0.0272, -0.1094, -0.0330, -0.1175, -0.0169, -0.1351],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.4.conv.1.running_var',\n",
       "                           tensor([0.0073, 0.0068, 0.0162, 0.0106, 0.0195, 0.0041, 0.0104, 0.0113, 0.0087,\n",
       "                                   0.0079, 0.0082, 0.0157, 0.0124, 0.0158, 0.0093, 0.0110, 0.0103, 0.0065,\n",
       "                                   0.0086, 0.0153, 0.0126, 0.0123, 0.0107, 0.0081, 0.0089, 0.0146, 0.0113,\n",
       "                                   0.0104, 0.0138, 0.0101, 0.0075, 0.0116], device='cuda:0')),\n",
       "                          ('model.4.conv.1.num_batches_tracked',\n",
       "                           tensor(70314, device='cuda:0')),\n",
       "                          ('model.4.conv.3.weight',\n",
       "                           tensor([[[[-0.0157, -0.0404, -0.0023],\n",
       "                                     [-0.0112, -0.0126, -0.0091],\n",
       "                                     [-0.0185, -0.0024,  0.0281]],\n",
       "                           \n",
       "                                    [[-0.0023, -0.0071, -0.0008],\n",
       "                                     [-0.0219,  0.0112, -0.0066],\n",
       "                                     [ 0.0104,  0.0093, -0.0064]],\n",
       "                           \n",
       "                                    [[ 0.0072, -0.0012, -0.0235],\n",
       "                                     [-0.0056,  0.0016, -0.0098],\n",
       "                                     [-0.0248,  0.0073, -0.0084]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 0.0202,  0.0373, -0.0131],\n",
       "                                     [ 0.0179,  0.0096,  0.0075],\n",
       "                                     [ 0.0106,  0.0390, -0.0083]],\n",
       "                           \n",
       "                                    [[ 0.0005,  0.0063, -0.0063],\n",
       "                                     [-0.0004,  0.0073, -0.0195],\n",
       "                                     [ 0.0335,  0.0124,  0.0183]],\n",
       "                           \n",
       "                                    [[-0.0164, -0.0147,  0.0006],\n",
       "                                     [ 0.0115, -0.0146,  0.0208],\n",
       "                                     [-0.0141, -0.0103, -0.0016]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-0.0287, -0.0153, -0.0178],\n",
       "                                     [-0.0283, -0.0476,  0.0154],\n",
       "                                     [ 0.0004,  0.0783,  0.0005]],\n",
       "                           \n",
       "                                    [[-0.0067,  0.0138, -0.0045],\n",
       "                                     [-0.0019,  0.0026, -0.0307],\n",
       "                                     [ 0.0257, -0.0084,  0.0504]],\n",
       "                           \n",
       "                                    [[-0.0158, -0.0163,  0.0208],\n",
       "                                     [-0.0104, -0.0088, -0.0052],\n",
       "                                     [ 0.0088,  0.0028, -0.0072]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 0.0119, -0.0091, -0.0235],\n",
       "                                     [-0.0096, -0.0015, -0.0284],\n",
       "                                     [-0.0178,  0.0033, -0.0127]],\n",
       "                           \n",
       "                                    [[ 0.0420, -0.0107, -0.0261],\n",
       "                                     [ 0.0374, -0.0030,  0.0426],\n",
       "                                     [-0.0306,  0.0162, -0.0285]],\n",
       "                           \n",
       "                                    [[-0.0226, -0.0517, -0.0077],\n",
       "                                     [ 0.0056,  0.0100, -0.0424],\n",
       "                                     [ 0.0044, -0.0358, -0.0304]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-0.0058, -0.0159, -0.0241],\n",
       "                                     [-0.0089,  0.0025, -0.0039],\n",
       "                                     [ 0.0033, -0.0059,  0.0138]],\n",
       "                           \n",
       "                                    [[-0.0213,  0.0082,  0.0255],\n",
       "                                     [-0.0005,  0.0038,  0.0053],\n",
       "                                     [-0.0129,  0.0050,  0.0423]],\n",
       "                           \n",
       "                                    [[-0.0148, -0.0046, -0.0146],\n",
       "                                     [-0.0071,  0.0063, -0.0082],\n",
       "                                     [-0.0075,  0.0088, -0.0041]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-0.0309, -0.0105, -0.0215],\n",
       "                                     [-0.0070, -0.0081, -0.0157],\n",
       "                                     [-0.0027,  0.0022,  0.0117]],\n",
       "                           \n",
       "                                    [[ 0.0044,  0.0060,  0.0229],\n",
       "                                     [ 0.0073,  0.0041,  0.0060],\n",
       "                                     [-0.0196, -0.0051,  0.0043]],\n",
       "                           \n",
       "                                    [[ 0.0188, -0.0087,  0.0255],\n",
       "                                     [ 0.0208, -0.0232,  0.0081],\n",
       "                                     [ 0.0202,  0.0211,  0.0056]]],\n",
       "                           \n",
       "                           \n",
       "                                   ...,\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 0.0061,  0.0222, -0.0087],\n",
       "                                     [ 0.0037,  0.0107,  0.0089],\n",
       "                                     [-0.0032,  0.0168, -0.0208]],\n",
       "                           \n",
       "                                    [[-0.0078, -0.0367, -0.0093],\n",
       "                                     [ 0.0308,  0.0073,  0.0165],\n",
       "                                     [ 0.0103, -0.0128, -0.0088]],\n",
       "                           \n",
       "                                    [[ 0.0001, -0.0045, -0.0188],\n",
       "                                     [ 0.0024, -0.0072, -0.0004],\n",
       "                                     [-0.0097, -0.0108, -0.0025]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-0.0052,  0.0016,  0.0109],\n",
       "                                     [-0.0001, -0.0155, -0.0126],\n",
       "                                     [-0.0137, -0.0118, -0.0168]],\n",
       "                           \n",
       "                                    [[ 0.0072, -0.0009,  0.0101],\n",
       "                                     [-0.0019,  0.0215, -0.0235],\n",
       "                                     [-0.0254, -0.0092, -0.0024]],\n",
       "                           \n",
       "                                    [[-0.0130, -0.0059,  0.0095],\n",
       "                                     [-0.0114,  0.0075, -0.0132],\n",
       "                                     [-0.0128, -0.0067, -0.0469]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 0.0027,  0.0068,  0.0174],\n",
       "                                     [-0.0175, -0.0099, -0.0219],\n",
       "                                     [-0.0168,  0.0110, -0.0142]],\n",
       "                           \n",
       "                                    [[-0.0381, -0.0172,  0.0019],\n",
       "                                     [-0.0293, -0.0027, -0.0238],\n",
       "                                     [-0.0367,  0.0029, -0.0325]],\n",
       "                           \n",
       "                                    [[-0.0080, -0.0077, -0.0052],\n",
       "                                     [ 0.0036, -0.0073, -0.0033],\n",
       "                                     [-0.0132, -0.0192,  0.0110]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 0.0095,  0.0022,  0.0218],\n",
       "                                     [-0.0230, -0.0121,  0.0180],\n",
       "                                     [-0.0381, -0.0289, -0.0188]],\n",
       "                           \n",
       "                                    [[ 0.0184,  0.0066,  0.0268],\n",
       "                                     [-0.0223,  0.0009,  0.0159],\n",
       "                                     [ 0.0225, -0.0052,  0.0069]],\n",
       "                           \n",
       "                                    [[-0.0013, -0.0331, -0.0060],\n",
       "                                     [-0.0218, -0.0071, -0.0057],\n",
       "                                     [-0.0194,  0.0238, -0.0099]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 0.0094, -0.0025,  0.0080],\n",
       "                                     [ 0.0114,  0.0092, -0.0106],\n",
       "                                     [-0.0249,  0.0216,  0.0011]],\n",
       "                           \n",
       "                                    [[-0.0165, -0.0128, -0.0038],\n",
       "                                     [ 0.0029,  0.0094, -0.0172],\n",
       "                                     [-0.0166,  0.0078, -0.0214]],\n",
       "                           \n",
       "                                    [[ 0.0101, -0.0269, -0.0047],\n",
       "                                     [ 0.0055,  0.0142, -0.0087],\n",
       "                                     [ 0.0029, -0.0188,  0.0023]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-0.0002, -0.0205, -0.0011],\n",
       "                                     [-0.0172, -0.0225,  0.0095],\n",
       "                                     [-0.0088, -0.0048, -0.0088]],\n",
       "                           \n",
       "                                    [[ 0.0321,  0.0176, -0.0037],\n",
       "                                     [ 0.0173, -0.0016,  0.0115],\n",
       "                                     [ 0.0521,  0.0288,  0.0207]],\n",
       "                           \n",
       "                                    [[ 0.0343, -0.0013,  0.0344],\n",
       "                                     [-0.0078,  0.0112,  0.0105],\n",
       "                                     [-0.0075, -0.0190, -0.0007]]]], device='cuda:0')),\n",
       "                          ('model.4.conv.3.bias',\n",
       "                           tensor([ 5.1511e-05, -1.4442e-05, -4.5132e-05, -3.5925e-05,  2.6670e-05,\n",
       "                                   -2.9839e-05,  4.0392e-05,  3.3773e-05,  4.4177e-05,  2.5150e-05,\n",
       "                                    1.9253e-05,  7.8155e-06, -3.7320e-05, -3.5048e-05, -3.2329e-05,\n",
       "                                   -4.0712e-05,  2.0848e-05,  1.6057e-05,  3.6087e-06,  2.2508e-05,\n",
       "                                    2.2945e-06,  2.2286e-05,  1.6817e-06,  1.9024e-05,  8.3456e-06,\n",
       "                                    2.3827e-05, -2.9923e-05,  3.9186e-05, -4.3120e-05, -1.8179e-05,\n",
       "                                   -7.8884e-06,  1.8631e-05], device='cuda:0')),\n",
       "                          ('model.4.conv.4.weight',\n",
       "                           tensor([0.1209, 0.1737, 0.1362, 0.1441, 0.1760, 0.1796, 0.1591, 0.1454, 0.1266,\n",
       "                                   0.1379, 0.1705, 0.1452, 0.1278, 0.1611, 0.1935, 0.1068, 0.1241, 0.1806,\n",
       "                                   0.1452, 0.1333, 0.1656, 0.1602, 0.1572, 0.1130, 0.1265, 0.1818, 0.1327,\n",
       "                                   0.2169, 0.1353, 0.1147, 0.1818, 0.1497], device='cuda:0')),\n",
       "                          ('model.4.conv.4.bias',\n",
       "                           tensor([-0.0583, -0.0492, -0.0536, -0.0553, -0.0170, -0.0750, -0.0614, -0.0492,\n",
       "                                   -0.0521, -0.0385, -0.0290, -0.0681, -0.0486, -0.0820, -0.0718, -0.0815,\n",
       "                                   -0.0441, -0.0370, -0.1013, -0.0669, -0.0350, -0.0191, -0.0581, -0.0549,\n",
       "                                   -0.0341, -0.0333, -0.0912, -0.0864, -0.0606, -0.0655, -0.0805, -0.0598],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.4.conv.4.running_mean',\n",
       "                           tensor([-0.0059, -0.0155,  0.0022, -0.0094,  0.0072, -0.0144, -0.0152, -0.0155,\n",
       "                                   -0.0023, -0.0153, -0.0046, -0.0080, -0.0088,  0.0018,  0.0056, -0.0044,\n",
       "                                   -0.0100, -0.0164,  0.0249, -0.0052, -0.0346, -0.0164,  0.0098, -0.0139,\n",
       "                                   -0.0171,  0.0251, -0.0137, -0.0358,  0.0033, -0.0022, -0.0149, -0.0175],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.4.conv.4.running_var',\n",
       "                           tensor([0.0004, 0.0005, 0.0005, 0.0006, 0.0010, 0.0005, 0.0007, 0.0004, 0.0003,\n",
       "                                   0.0006, 0.0008, 0.0004, 0.0004, 0.0009, 0.0008, 0.0003, 0.0003, 0.0007,\n",
       "                                   0.0009, 0.0005, 0.0006, 0.0005, 0.0005, 0.0006, 0.0004, 0.0010, 0.0005,\n",
       "                                   0.0005, 0.0003, 0.0003, 0.0004, 0.0004], device='cuda:0')),\n",
       "                          ('model.4.conv.4.num_batches_tracked',\n",
       "                           tensor(70314, device='cuda:0')),\n",
       "                          ('model.5.conv.0.weight',\n",
       "                           tensor([[[[ 1.0402e-02,  1.7696e-02, -1.2795e-02],\n",
       "                                     [ 5.1382e-03, -2.6844e-03, -2.8595e-02],\n",
       "                                     [-1.7356e-02,  2.2456e-03,  9.9519e-03]],\n",
       "                           \n",
       "                                    [[ 1.9892e-02, -3.2397e-02, -1.3730e-02],\n",
       "                                     [-2.3121e-02,  1.4333e-02,  1.0671e-02],\n",
       "                                     [ 1.3857e-03,  2.4304e-02,  1.8764e-02]],\n",
       "                           \n",
       "                                    [[-1.3452e-02, -1.5665e-02, -1.9008e-02],\n",
       "                                     [ 2.2219e-02, -3.1727e-03, -1.4435e-02],\n",
       "                                     [-1.6478e-02, -1.2470e-02, -2.8386e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-4.8455e-03, -2.5553e-03, -5.0177e-03],\n",
       "                                     [-2.1088e-02, -3.2465e-03,  2.3615e-03],\n",
       "                                     [-2.0151e-02, -1.0780e-02,  1.0717e-02]],\n",
       "                           \n",
       "                                    [[-2.8386e-02,  2.0247e-02,  1.4610e-02],\n",
       "                                     [-3.7920e-02, -1.2683e-02,  1.4752e-02],\n",
       "                                     [ 2.9922e-03, -1.7538e-02, -1.1161e-02]],\n",
       "                           \n",
       "                                    [[-4.9887e-03,  2.1262e-02, -1.0639e-03],\n",
       "                                     [-8.7602e-03,  1.0926e-02, -1.7984e-02],\n",
       "                                     [ 2.1044e-02,  2.6300e-02, -2.0165e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 1.4670e-02, -3.4837e-03,  9.4202e-03],\n",
       "                                     [ 4.3285e-02, -6.8582e-03,  7.8402e-03],\n",
       "                                     [ 3.5932e-03,  2.3178e-02, -1.2174e-02]],\n",
       "                           \n",
       "                                    [[ 1.2693e-02, -3.1350e-02, -6.5436e-03],\n",
       "                                     [-7.7824e-03,  7.0813e-02, -8.1911e-03],\n",
       "                                     [ 2.5329e-02, -1.8173e-02,  1.9023e-02]],\n",
       "                           \n",
       "                                    [[ 2.9748e-02,  2.3492e-03, -3.2611e-03],\n",
       "                                     [ 1.4614e-02, -2.3021e-02, -1.7874e-02],\n",
       "                                     [ 3.1035e-02,  3.1020e-02, -4.4505e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 4.1631e-02, -1.5076e-03, -1.0510e-02],\n",
       "                                     [ 5.3290e-04, -2.2091e-03,  9.6113e-03],\n",
       "                                     [ 1.3223e-03, -1.0455e-02,  2.2495e-02]],\n",
       "                           \n",
       "                                    [[-3.7541e-02,  2.6079e-02,  1.7924e-02],\n",
       "                                     [ 1.1963e-02, -1.5063e-02, -2.2621e-02],\n",
       "                                     [ 1.2480e-02,  8.1845e-03,  3.2658e-02]],\n",
       "                           \n",
       "                                    [[ 9.5421e-03, -1.8045e-02,  1.3960e-03],\n",
       "                                     [ 2.1777e-02,  1.0688e-02,  1.0501e-02],\n",
       "                                     [-2.7165e-03, -1.2456e-02,  3.4210e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 1.7727e-02,  3.6946e-02,  2.7929e-02],\n",
       "                                     [ 3.3779e-02,  6.1442e-02,  2.5010e-02],\n",
       "                                     [-7.9477e-03,  5.5064e-02,  1.5818e-02]],\n",
       "                           \n",
       "                                    [[ 2.9217e-03, -2.4762e-02,  5.2445e-03],\n",
       "                                     [-2.0669e-02,  4.2246e-02,  2.0765e-02],\n",
       "                                     [-2.4249e-03,  2.2065e-02,  2.6587e-02]],\n",
       "                           \n",
       "                                    [[ 3.0445e-04, -1.3745e-03, -1.4334e-02],\n",
       "                                     [ 9.9664e-03, -7.1335e-03, -9.7436e-03],\n",
       "                                     [ 1.6536e-02,  2.0669e-02,  2.5964e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-2.9614e-03,  4.4316e-03, -9.6394e-03],\n",
       "                                     [ 1.1041e-02, -6.9560e-03, -1.6563e-02],\n",
       "                                     [-2.4579e-02, -1.0696e-02, -1.6492e-02]],\n",
       "                           \n",
       "                                    [[ 5.4304e-03,  9.7495e-03,  1.0851e-02],\n",
       "                                     [ 1.1070e-02, -1.6369e-02, -2.6912e-02],\n",
       "                                     [ 1.7238e-02,  1.6585e-02, -2.1859e-02]],\n",
       "                           \n",
       "                                    [[ 2.2325e-02, -5.2801e-02, -6.0843e-02],\n",
       "                                     [ 1.3603e-02,  3.0596e-03, -9.9414e-03],\n",
       "                                     [ 2.8706e-03, -8.6245e-03,  9.4048e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   ...,\n",
       "                           \n",
       "                           \n",
       "                                   [[[-1.7833e-02, -1.0976e-02, -4.4068e-03],\n",
       "                                     [-1.0753e-02, -1.0746e-02,  2.5962e-03],\n",
       "                                     [-2.1019e-02,  1.2013e-02,  9.9017e-03]],\n",
       "                           \n",
       "                                    [[-5.7970e-03,  1.0769e-02, -2.6246e-02],\n",
       "                                     [-1.0313e-02, -2.2555e-02,  1.6789e-02],\n",
       "                                     [ 1.3134e-03, -3.6202e-03,  2.9887e-03]],\n",
       "                           \n",
       "                                    [[ 1.4718e-02, -3.9217e-03,  1.2145e-02],\n",
       "                                     [ 5.9038e-03, -2.3662e-03, -1.3218e-02],\n",
       "                                     [ 1.4795e-02, -5.1034e-03,  7.0318e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-9.7103e-03, -2.8849e-02, -1.3307e-02],\n",
       "                                     [-5.1653e-03, -2.0569e-02, -7.1247e-03],\n",
       "                                     [-2.7282e-02,  1.3596e-02, -4.5702e-02]],\n",
       "                           \n",
       "                                    [[-1.3187e-02,  1.9284e-02, -1.0352e-02],\n",
       "                                     [-1.1913e-02,  2.9243e-02, -2.0950e-02],\n",
       "                                     [-2.0305e-02, -2.9197e-02, -9.3682e-03]],\n",
       "                           \n",
       "                                    [[ 1.6141e-02, -1.7705e-02,  5.8229e-03],\n",
       "                                     [ 1.0755e-02, -8.7071e-03, -4.2868e-02],\n",
       "                                     [ 6.2974e-03, -7.4671e-03, -1.7685e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 3.8595e-03,  1.8307e-03,  1.0404e-02],\n",
       "                                     [ 1.6686e-02, -8.6141e-03,  1.6949e-02],\n",
       "                                     [-7.0757e-03, -6.5626e-03, -1.5195e-03]],\n",
       "                           \n",
       "                                    [[-7.8463e-03, -2.0463e-02, -2.3954e-02],\n",
       "                                     [-5.5925e-03, -1.3013e-02, -1.5965e-03],\n",
       "                                     [ 1.3213e-02, -8.6377e-04, -4.6615e-03]],\n",
       "                           \n",
       "                                    [[-1.1720e-02, -6.1521e-03, -2.0360e-02],\n",
       "                                     [ 8.4987e-03,  4.3512e-03, -4.8896e-03],\n",
       "                                     [-8.9769e-03, -1.3307e-03, -4.8233e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-9.0971e-03,  3.0655e-02, -3.7893e-03],\n",
       "                                     [ 9.9730e-03, -1.2918e-02, -8.6413e-03],\n",
       "                                     [-1.7213e-02,  7.1895e-03, -7.8446e-03]],\n",
       "                           \n",
       "                                    [[ 2.5838e-02,  1.3279e-02,  2.3912e-02],\n",
       "                                     [-1.7819e-04,  1.4729e-03,  2.4776e-02],\n",
       "                                     [ 1.9155e-02,  1.6675e-02,  9.6534e-04]],\n",
       "                           \n",
       "                                    [[-1.1345e-02, -1.0056e-02,  6.4791e-03],\n",
       "                                     [-4.4553e-03,  2.5025e-03, -7.0959e-03],\n",
       "                                     [-9.6830e-03, -2.0413e-02, -1.4389e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 2.0950e-02,  1.9437e-02,  2.1231e-02],\n",
       "                                     [ 2.4819e-03, -3.3824e-03,  1.1616e-02],\n",
       "                                     [-2.2495e-02, -1.7727e-02, -6.6614e-03]],\n",
       "                           \n",
       "                                    [[-1.2796e-02,  3.8890e-03, -7.8122e-03],\n",
       "                                     [ 3.8743e-03,  1.1583e-02, -1.5651e-02],\n",
       "                                     [-1.5967e-02,  9.8437e-03,  7.7420e-03]],\n",
       "                           \n",
       "                                    [[-4.4221e-03, -7.4177e-03, -7.6211e-03],\n",
       "                                     [ 5.8351e-03,  3.1676e-03,  1.0889e-03],\n",
       "                                     [-5.8773e-04,  5.4947e-03, -6.6619e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 6.8921e-04, -2.3260e-03,  1.2608e-03],\n",
       "                                     [ 1.2118e-02,  1.0942e-02,  2.2658e-03],\n",
       "                                     [ 7.1801e-05,  1.3271e-02,  1.2677e-03]],\n",
       "                           \n",
       "                                    [[-4.5107e-03, -1.1409e-02, -7.4574e-03],\n",
       "                                     [ 1.1125e-02, -1.7333e-03, -1.6942e-02],\n",
       "                                     [-1.2101e-02, -3.8290e-02, -2.3120e-02]],\n",
       "                           \n",
       "                                    [[-5.3703e-03,  1.1093e-02, -1.4334e-03],\n",
       "                                     [ 2.8505e-03,  9.8407e-03,  4.4886e-03],\n",
       "                                     [-6.3557e-03,  1.3499e-02, -2.5819e-03]]]], device='cuda:0')),\n",
       "                          ('model.5.conv.0.bias',\n",
       "                           tensor([ 5.1467e-05,  2.6395e-05, -4.9356e-05,  8.3237e-06,  1.9767e-05,\n",
       "                                    2.7073e-05, -9.4307e-06, -1.6116e-05, -3.9708e-05, -4.2979e-05,\n",
       "                                   -4.9580e-05, -4.3820e-05, -4.2449e-05,  2.5758e-05,  2.6802e-05,\n",
       "                                   -4.7086e-05,  4.1167e-05,  1.2713e-05, -3.6847e-05, -2.2586e-05,\n",
       "                                    2.5088e-05,  3.8195e-05, -3.9342e-06, -3.1227e-05,  2.6765e-05,\n",
       "                                    8.5595e-07, -3.7377e-05, -2.4146e-05, -2.8676e-05,  3.3115e-05,\n",
       "                                    1.0466e-05,  4.9049e-05], device='cuda:0')),\n",
       "                          ('model.5.conv.1.weight',\n",
       "                           tensor([0.0934, 0.1063, 0.1331, 0.1123, 0.0914, 0.1294, 0.0952, 0.1052, 0.1219,\n",
       "                                   0.1054, 0.0882, 0.1048, 0.1005, 0.1127, 0.1194, 0.1109, 0.1226, 0.1215,\n",
       "                                   0.1267, 0.0971, 0.1122, 0.0887, 0.1115, 0.0942, 0.0948, 0.1151, 0.1183,\n",
       "                                   0.1204, 0.1280, 0.1039, 0.1218, 0.1163], device='cuda:0')),\n",
       "                          ('model.5.conv.1.bias',\n",
       "                           tensor([-0.0402, -0.0570, -0.1194, -0.0571, -0.0609, -0.1374, -0.0468, -0.0541,\n",
       "                                   -0.0837, -0.0568, -0.0308, -0.0830, -0.0674, -0.0857, -0.0699, -0.0846,\n",
       "                                   -0.0823, -0.1331, -0.1158, -0.0413, -0.0661, -0.0186, -0.0892,  0.0060,\n",
       "                                   -0.0293, -0.0878, -0.0805, -0.0330, -0.0829, -0.0416, -0.0996, -0.0408],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.5.conv.1.running_mean',\n",
       "                           tensor([-0.0138, -0.0434, -0.0506, -0.0720, -0.0288, -0.2101, -0.0447,  0.0757,\n",
       "                                    0.1065, -0.0780, -0.0016, -0.0015, -0.0089,  0.0347, -0.1318,  0.0219,\n",
       "                                   -0.0411,  0.0612, -0.0455, -0.0551, -0.0189,  0.0022,  0.0026, -0.0726,\n",
       "                                   -0.0667, -0.0088,  0.0625, -0.1126, -0.0734, -0.1368, -0.0230, -0.0776],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.5.conv.1.running_var',\n",
       "                           tensor([0.0075, 0.0075, 0.0245, 0.0103, 0.0059, 0.0132, 0.0052, 0.0091, 0.0181,\n",
       "                                   0.0092, 0.0075, 0.0074, 0.0083, 0.0081, 0.0112, 0.0091, 0.0082, 0.0146,\n",
       "                                   0.0134, 0.0067, 0.0100, 0.0099, 0.0073, 0.0098, 0.0073, 0.0099, 0.0149,\n",
       "                                   0.0116, 0.0168, 0.0090, 0.0124, 0.0258], device='cuda:0')),\n",
       "                          ('model.5.conv.1.num_batches_tracked',\n",
       "                           tensor(70314, device='cuda:0')),\n",
       "                          ('model.5.conv.3.weight',\n",
       "                           tensor([[[[-5.7583e-04, -8.2699e-03, -1.8756e-02],\n",
       "                                     [-1.5853e-02, -2.1493e-02,  2.3961e-03],\n",
       "                                     [ 1.5162e-02,  8.9945e-03,  8.2229e-03]],\n",
       "                           \n",
       "                                    [[ 3.6844e-03,  4.0202e-02,  1.1037e-02],\n",
       "                                     [ 2.4689e-03, -1.7899e-02, -1.4549e-04],\n",
       "                                     [-1.9853e-02, -1.7841e-02, -2.3661e-02]],\n",
       "                           \n",
       "                                    [[ 3.8824e-03,  3.7722e-02, -1.0166e-02],\n",
       "                                     [ 6.3552e-03,  1.7287e-02, -9.0826e-04],\n",
       "                                     [-1.4335e-02, -2.4905e-02, -1.8109e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-1.0759e-02,  1.4085e-02,  2.3324e-03],\n",
       "                                     [ 2.5791e-02, -1.9249e-02,  2.7095e-05],\n",
       "                                     [ 2.3681e-03, -5.2895e-03, -1.3172e-02]],\n",
       "                           \n",
       "                                    [[-1.8915e-02, -9.1474e-03, -1.7785e-02],\n",
       "                                     [ 1.0494e-03, -1.6447e-02, -9.5244e-03],\n",
       "                                     [-8.1938e-03, -1.6765e-02, -1.8215e-03]],\n",
       "                           \n",
       "                                    [[ 1.7021e-03, -1.5867e-02, -2.0278e-02],\n",
       "                                     [-1.1511e-02, -1.7580e-02, -1.0436e-02],\n",
       "                                     [-1.0865e-02, -3.7113e-03, -8.9732e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 3.0818e-03,  3.0329e-02,  2.2919e-02],\n",
       "                                     [-2.0134e-02, -6.5746e-03, -1.5975e-02],\n",
       "                                     [-1.7227e-02, -1.1430e-02, -3.1215e-04]],\n",
       "                           \n",
       "                                    [[-1.7911e-02, -1.6533e-03,  1.7411e-02],\n",
       "                                     [-2.9638e-02,  6.3066e-02, -1.1595e-02],\n",
       "                                     [ 9.1238e-03, -7.2403e-03, -5.4247e-03]],\n",
       "                           \n",
       "                                    [[ 2.3768e-02,  1.0252e-02, -1.1922e-02],\n",
       "                                     [-1.0582e-02,  1.9548e-02,  3.8709e-03],\n",
       "                                     [-9.1030e-03, -1.6521e-02,  4.5080e-04]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-2.3986e-02, -7.4207e-03, -1.2977e-02],\n",
       "                                     [-1.7784e-02, -1.6737e-02,  3.7796e-03],\n",
       "                                     [-4.5746e-03, -2.7166e-02, -2.0763e-02]],\n",
       "                           \n",
       "                                    [[-5.2549e-04, -7.8369e-03, -7.2579e-03],\n",
       "                                     [ 1.1627e-02, -9.1300e-03, -5.7150e-03],\n",
       "                                     [ 1.2196e-03,  6.3393e-03, -3.5384e-03]],\n",
       "                           \n",
       "                                    [[-3.4812e-03, -3.0721e-02, -1.2986e-02],\n",
       "                                     [-2.7841e-03, -1.9419e-02, -1.4196e-02],\n",
       "                                     [ 9.7895e-03, -3.8175e-03, -7.9704e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-1.4429e-02, -1.2444e-03, -1.6332e-02],\n",
       "                                     [-3.8685e-03,  4.3065e-04,  4.9205e-04],\n",
       "                                     [-9.5783e-03,  8.7496e-03, -8.2719e-03]],\n",
       "                           \n",
       "                                    [[-1.0443e-03,  3.2351e-02,  1.6137e-03],\n",
       "                                     [ 4.7997e-04, -2.0272e-02, -7.9430e-03],\n",
       "                                     [ 1.1999e-02,  4.2747e-03, -5.3321e-03]],\n",
       "                           \n",
       "                                    [[-1.0209e-02,  1.4992e-02,  1.0033e-02],\n",
       "                                     [ 1.9019e-02,  3.6089e-02,  3.3680e-02],\n",
       "                                     [ 2.0909e-02,  1.6237e-02,  2.5542e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 2.3564e-02, -1.5068e-02, -2.4946e-02],\n",
       "                                     [-1.7888e-03, -8.3174e-03, -1.0763e-02],\n",
       "                                     [ 1.4765e-02, -9.5572e-03, -1.3115e-02]],\n",
       "                           \n",
       "                                    [[ 9.9325e-03, -4.4926e-03,  1.2126e-02],\n",
       "                                     [-1.0358e-02, -1.0199e-02,  2.3389e-03],\n",
       "                                     [ 4.0106e-03, -9.0025e-03, -6.2825e-03]],\n",
       "                           \n",
       "                                    [[-9.1957e-03,  4.5347e-03,  3.4078e-03],\n",
       "                                     [ 6.9868e-03,  2.5565e-03,  1.0285e-02],\n",
       "                                     [ 1.8714e-02,  7.8422e-04,  5.3638e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   ...,\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 6.2974e-03, -8.7461e-03, -2.0751e-02],\n",
       "                                     [ 1.4303e-02,  1.7204e-02,  5.4533e-03],\n",
       "                                     [ 7.0957e-03,  7.6338e-03, -7.8504e-03]],\n",
       "                           \n",
       "                                    [[ 9.0028e-03, -1.3568e-02, -2.5367e-02],\n",
       "                                     [-2.6933e-03, -4.2294e-02,  1.4575e-02],\n",
       "                                     [ 1.4509e-02,  1.2272e-02, -5.5686e-03]],\n",
       "                           \n",
       "                                    [[-9.7138e-03,  1.8118e-02,  1.5463e-02],\n",
       "                                     [-1.2525e-02,  9.0689e-03,  1.5414e-02],\n",
       "                                     [-1.1232e-02,  4.9778e-03, -8.2325e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-5.9591e-03, -1.6283e-02, -1.6778e-02],\n",
       "                                     [ 2.4624e-03, -7.1060e-03, -1.6619e-02],\n",
       "                                     [-1.2566e-02, -1.2447e-02, -4.6289e-02]],\n",
       "                           \n",
       "                                    [[-6.9398e-03, -5.6872e-03, -6.4900e-03],\n",
       "                                     [-7.8279e-03, -9.1500e-03, -7.9807e-03],\n",
       "                                     [-2.7714e-03,  3.0731e-02, -6.4441e-03]],\n",
       "                           \n",
       "                                    [[-3.1538e-03,  3.4773e-03,  4.1384e-04],\n",
       "                                     [-2.1835e-02, -3.3116e-03, -2.0097e-03],\n",
       "                                     [-6.8056e-03, -8.7614e-03, -1.4644e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 7.9708e-03, -4.9119e-03,  2.4031e-02],\n",
       "                                     [ 2.7383e-02, -3.3848e-03,  7.1040e-05],\n",
       "                                     [ 1.1741e-02,  1.8271e-02,  1.0733e-02]],\n",
       "                           \n",
       "                                    [[ 3.3686e-02,  2.1614e-02, -2.0165e-02],\n",
       "                                     [-2.9573e-02, -1.2007e-02,  2.2785e-02],\n",
       "                                     [ 1.7610e-02, -1.5767e-02, -1.7179e-03]],\n",
       "                           \n",
       "                                    [[ 1.9920e-02,  9.7613e-03,  1.5162e-02],\n",
       "                                     [-1.2027e-05, -1.0489e-02,  1.5186e-02],\n",
       "                                     [ 5.0510e-03,  1.0201e-02,  8.8204e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-2.2299e-04,  7.4439e-03,  1.2154e-02],\n",
       "                                     [-2.2090e-02, -4.4675e-03, -1.0067e-02],\n",
       "                                     [ 5.0196e-03,  4.0738e-02, -1.7579e-02]],\n",
       "                           \n",
       "                                    [[-2.2624e-03, -7.0302e-03, -6.0760e-03],\n",
       "                                     [-4.8984e-04,  4.6624e-03,  5.9873e-04],\n",
       "                                     [-1.1413e-02, -8.7042e-03,  1.3920e-03]],\n",
       "                           \n",
       "                                    [[ 6.0738e-05,  8.1987e-03,  2.6791e-03],\n",
       "                                     [-1.7062e-02,  3.3445e-03, -6.3162e-03],\n",
       "                                     [-1.1267e-02, -9.3679e-03, -2.5237e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 4.6328e-03,  3.1642e-02,  1.2052e-02],\n",
       "                                     [-6.8530e-03,  7.2534e-03, -9.6570e-03],\n",
       "                                     [-2.9509e-03,  5.1028e-02, -1.5807e-02]],\n",
       "                           \n",
       "                                    [[-8.5295e-03, -3.4926e-02, -2.9214e-02],\n",
       "                                     [-1.6363e-02, -1.2647e-02, -9.5955e-03],\n",
       "                                     [ 4.2987e-03,  3.6390e-03, -3.7836e-02]],\n",
       "                           \n",
       "                                    [[ 1.5724e-02, -3.7566e-03,  1.3911e-02],\n",
       "                                     [-1.4066e-03, -6.9277e-03,  6.7140e-03],\n",
       "                                     [-4.2836e-03, -6.3746e-03, -2.1877e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-3.4927e-03, -9.8714e-03,  3.1271e-02],\n",
       "                                     [-1.1081e-02,  9.8784e-03,  1.3294e-02],\n",
       "                                     [ 3.9410e-03, -1.3092e-02, -4.3570e-04]],\n",
       "                           \n",
       "                                    [[ 2.2505e-04, -1.4169e-02, -1.0286e-02],\n",
       "                                     [-9.4805e-03, -1.9274e-02, -1.8578e-02],\n",
       "                                     [ 2.7328e-03, -2.6215e-02, -8.0876e-03]],\n",
       "                           \n",
       "                                    [[-9.0501e-03, -2.3718e-02, -1.6883e-02],\n",
       "                                     [-1.1983e-02, -2.1167e-02, -1.4712e-02],\n",
       "                                     [ 6.8115e-03, -7.2212e-03, -1.5523e-02]]]], device='cuda:0')),\n",
       "                          ('model.5.conv.3.bias',\n",
       "                           tensor([-9.7188e-06,  4.0576e-05, -3.6990e-05,  3.6223e-05, -2.2695e-06,\n",
       "                                   -2.6048e-05, -1.5625e-06, -1.4413e-05, -2.9028e-05, -3.6149e-05,\n",
       "                                   -2.2800e-05,  2.0714e-05, -2.9388e-05,  3.3322e-05,  3.9283e-06,\n",
       "                                   -4.7482e-05,  5.0437e-05,  3.4184e-05, -4.5590e-05, -1.5587e-05,\n",
       "                                   -2.0628e-06,  3.6918e-05,  2.0597e-05,  1.8664e-06,  5.0410e-05,\n",
       "                                    1.9170e-05,  2.1998e-06,  1.4130e-05, -1.8281e-05, -3.4510e-05,\n",
       "                                   -1.6836e-05, -1.5819e-05], device='cuda:0')),\n",
       "                          ('model.5.conv.4.weight',\n",
       "                           tensor([0.1300, 0.1585, 0.1309, 0.1318, 0.1458, 0.1688, 0.1057, 0.1773, 0.1164,\n",
       "                                   0.1348, 0.1308, 0.1438, 0.1162, 0.1838, 0.1528, 0.1435, 0.1974, 0.1397,\n",
       "                                   0.0455, 0.1588, 0.1414, 0.1112, 0.1399, 0.1370, 0.1810, 0.1285, 0.1418,\n",
       "                                   0.1826, 0.2248, 0.1308, 0.1407, 0.1285], device='cuda:0')),\n",
       "                          ('model.5.conv.4.bias',\n",
       "                           tensor([-0.0410, -0.1014, -0.0612, -0.0863, -0.0546, -0.0734, -0.0630, -0.0731,\n",
       "                                   -0.0676, -0.0664, -0.0905, -0.0624, -0.0883, -0.0923, -0.0963, -0.1218,\n",
       "                                   -0.0894, -0.0402, -0.0627, -0.0592, -0.0836, -0.1351, -0.0830, -0.0950,\n",
       "                                   -0.0853, -0.0483, -0.1133, -0.0682, -0.0657, -0.0807, -0.0866, -0.0557],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.5.conv.4.running_mean',\n",
       "                           tensor([ 0.0020, -0.0076, -0.0047, -0.0001, -0.0013, -0.0127, -0.0099, -0.0104,\n",
       "                                   -0.0042, -0.0022,  0.0069, -0.0001, -0.0004,  0.0072,  0.0024,  0.0008,\n",
       "                                    0.0031,  0.0081, -0.0003,  0.0090, -0.0143,  0.0003, -0.0163,  0.0048,\n",
       "                                   -0.0069,  0.0072, -0.0138, -0.0147, -0.0007, -0.0083, -0.0045, -0.0075],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.5.conv.4.running_var',\n",
       "                           tensor([0.0002, 0.0002, 0.0003, 0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0001,\n",
       "                                   0.0002, 0.0004, 0.0002, 0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0005,\n",
       "                                   0.0001, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002,\n",
       "                                   0.0002, 0.0003, 0.0002, 0.0001, 0.0002], device='cuda:0')),\n",
       "                          ('model.5.conv.4.num_batches_tracked',\n",
       "                           tensor(70314, device='cuda:0')),\n",
       "                          ('model.6.conv.0.weight',\n",
       "                           tensor([[[[ 6.9920e-04, -1.5599e-02,  1.0901e-02],\n",
       "                                     [ 2.8380e-04,  2.1561e-02, -9.4025e-03],\n",
       "                                     [ 9.3067e-03,  1.0226e-02,  3.2209e-03]],\n",
       "                           \n",
       "                                    [[-5.8419e-03,  7.9149e-04, -2.1654e-03],\n",
       "                                     [-9.0521e-03,  1.6428e-02, -4.9957e-03],\n",
       "                                     [-1.6618e-02, -1.3020e-02,  3.0603e-03]],\n",
       "                           \n",
       "                                    [[-5.1602e-03,  4.6970e-03,  1.5034e-02],\n",
       "                                     [-1.0703e-02,  1.0497e-04,  5.9917e-03],\n",
       "                                     [-4.7480e-03, -7.3693e-03, -8.6774e-04]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-7.3642e-03,  1.0792e-02,  1.8387e-03],\n",
       "                                     [ 1.5548e-03,  2.6197e-02, -4.5764e-03],\n",
       "                                     [-2.4200e-02,  2.0940e-02,  6.0658e-03]],\n",
       "                           \n",
       "                                    [[-4.3514e-02, -1.5337e-02, -1.1953e-02],\n",
       "                                     [ 2.3255e-02, -1.6812e-02,  8.0728e-03],\n",
       "                                     [ 6.8897e-03, -1.1486e-02, -1.4333e-02]],\n",
       "                           \n",
       "                                    [[-4.9593e-03, -2.1929e-02, -8.9487e-03],\n",
       "                                     [-9.3422e-04, -3.4138e-03, -7.3530e-04],\n",
       "                                     [ 5.3009e-03, -3.5640e-03,  2.5066e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-5.5888e-03,  3.4770e-03, -4.0752e-04],\n",
       "                                     [ 1.0366e-02,  1.9735e-03, -5.2623e-03],\n",
       "                                     [-1.1733e-02,  2.2689e-02, -8.3471e-03]],\n",
       "                           \n",
       "                                    [[-2.2229e-02,  1.1608e-02, -2.0704e-02],\n",
       "                                     [ 6.2830e-03,  4.8365e-03, -9.8873e-03],\n",
       "                                     [-5.1907e-03,  4.1570e-03, -1.0714e-02]],\n",
       "                           \n",
       "                                    [[-7.5543e-03,  1.6438e-02, -2.8793e-03],\n",
       "                                     [-6.8137e-03, -2.8570e-03,  7.8932e-03],\n",
       "                                     [-1.7132e-02, -2.3530e-03, -2.4156e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-8.7627e-03,  8.3460e-03, -9.9273e-03],\n",
       "                                     [-8.1600e-03, -2.7923e-03,  2.4307e-02],\n",
       "                                     [ 1.2354e-02, -1.1025e-02, -1.5082e-02]],\n",
       "                           \n",
       "                                    [[ 1.5750e-02,  1.4469e-03,  1.0189e-02],\n",
       "                                     [-7.6386e-03,  6.7686e-03,  5.0824e-03],\n",
       "                                     [ 5.6475e-03, -1.0175e-02, -1.0071e-03]],\n",
       "                           \n",
       "                                    [[ 1.0959e-02, -1.5817e-02,  3.7127e-03],\n",
       "                                     [-2.1649e-02,  7.9882e-04, -3.9978e-03],\n",
       "                                     [-1.2494e-03, -2.4250e-03,  2.8706e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-5.8951e-03, -1.4674e-02,  2.1978e-02],\n",
       "                                     [ 9.7375e-03,  3.2859e-03, -1.9181e-02],\n",
       "                                     [-3.7996e-03, -1.4005e-02,  1.6885e-02]],\n",
       "                           \n",
       "                                    [[-1.1204e-02, -1.4146e-02,  1.8414e-02],\n",
       "                                     [ 2.8901e-02, -1.0510e-02,  1.7555e-02],\n",
       "                                     [-1.3645e-02, -3.4680e-03,  2.5460e-02]],\n",
       "                           \n",
       "                                    [[ 7.5652e-03,  6.7041e-03,  2.7647e-02],\n",
       "                                     [ 3.5920e-02, -1.3940e-02, -2.7337e-04],\n",
       "                                     [ 3.6352e-02,  8.2390e-03,  3.3067e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 4.0455e-02,  2.8998e-02,  2.1572e-03],\n",
       "                                     [-6.9088e-03,  1.4019e-03,  1.0045e-02],\n",
       "                                     [ 4.5732e-03,  1.3642e-02,  2.2897e-02]],\n",
       "                           \n",
       "                                    [[ 8.5796e-03,  2.1480e-02, -1.7573e-02],\n",
       "                                     [-1.3208e-02,  2.9699e-02,  1.4040e-02],\n",
       "                                     [ 1.0323e-02,  1.6177e-02,  2.2768e-02]],\n",
       "                           \n",
       "                                    [[-3.4281e-03,  1.8067e-02, -3.1618e-03],\n",
       "                                     [ 5.7243e-03,  1.7203e-02, -6.4563e-03],\n",
       "                                     [ 1.9918e-02,  2.3302e-02,  2.1977e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   ...,\n",
       "                           \n",
       "                           \n",
       "                                   [[[-5.0446e-05,  2.2093e-02, -7.3639e-03],\n",
       "                                     [ 4.1136e-03,  1.5741e-03, -3.3383e-03],\n",
       "                                     [-1.0372e-02,  4.4290e-02, -9.1967e-03]],\n",
       "                           \n",
       "                                    [[-2.1355e-02,  7.6016e-03,  5.2148e-03],\n",
       "                                     [-1.2845e-02,  1.9032e-02, -1.2505e-02],\n",
       "                                     [ 7.4036e-03, -1.0377e-02, -7.9510e-04]],\n",
       "                           \n",
       "                                    [[-3.4447e-03, -6.7354e-03, -9.4356e-03],\n",
       "                                     [ 9.7531e-03, -7.1961e-03, -3.1067e-03],\n",
       "                                     [ 8.7185e-03, -1.6276e-02, -4.2363e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 1.4218e-02,  2.2988e-02, -2.2149e-02],\n",
       "                                     [ 2.1045e-02,  1.6704e-02, -9.5137e-03],\n",
       "                                     [ 8.7173e-04, -1.2691e-02,  6.2589e-03]],\n",
       "                           \n",
       "                                    [[-1.1216e-02, -1.2077e-02,  2.2811e-03],\n",
       "                                     [-7.7558e-03, -4.9389e-03, -6.7083e-04],\n",
       "                                     [ 7.5067e-03, -1.6795e-04, -4.3648e-03]],\n",
       "                           \n",
       "                                    [[ 6.3288e-04, -1.3031e-02,  1.2365e-02],\n",
       "                                     [-1.3459e-02,  9.5360e-03,  1.3558e-02],\n",
       "                                     [-2.4386e-02, -1.0358e-02,  2.1117e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-6.6127e-03, -1.1622e-02,  7.5008e-03],\n",
       "                                     [-1.0541e-02,  3.1743e-02,  5.3373e-02],\n",
       "                                     [ 3.3230e-02,  1.6307e-02,  1.1472e-02]],\n",
       "                           \n",
       "                                    [[-7.0299e-03, -2.0148e-02, -8.2319e-03],\n",
       "                                     [ 3.8708e-02, -1.7627e-02, -3.2410e-02],\n",
       "                                     [ 4.7171e-03, -3.7478e-02, -1.0848e-02]],\n",
       "                           \n",
       "                                    [[ 3.0317e-03,  1.8800e-02,  2.0095e-02],\n",
       "                                     [ 1.7510e-02,  1.6486e-03, -1.2231e-02],\n",
       "                                     [-1.6713e-02, -1.6578e-02, -1.4947e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 6.1629e-03, -6.1280e-03, -1.7843e-02],\n",
       "                                     [-1.2793e-02, -5.8545e-03, -4.3204e-02],\n",
       "                                     [ 1.8475e-02,  1.1559e-02,  4.2434e-02]],\n",
       "                           \n",
       "                                    [[ 1.1804e-02, -1.1710e-02,  7.4001e-04],\n",
       "                                     [-9.0705e-03, -1.4874e-02, -2.5298e-02],\n",
       "                                     [-8.9963e-03,  1.4264e-02, -1.1052e-02]],\n",
       "                           \n",
       "                                    [[-2.9041e-03,  2.9554e-05,  1.5568e-05],\n",
       "                                     [-1.7881e-02, -1.1560e-02, -9.6445e-03],\n",
       "                                     [-1.1814e-02, -3.0414e-02,  4.2387e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-3.8646e-03, -2.8054e-03, -9.5791e-03],\n",
       "                                     [ 3.1242e-02,  6.4116e-02, -9.6474e-03],\n",
       "                                     [ 9.2164e-03,  3.1503e-03, -5.8412e-03]],\n",
       "                           \n",
       "                                    [[ 6.9847e-03, -1.0455e-02, -3.0167e-02],\n",
       "                                     [ 2.2813e-02,  3.8354e-03, -3.3806e-02],\n",
       "                                     [ 2.5203e-02,  4.3389e-03, -3.7689e-02]],\n",
       "                           \n",
       "                                    [[-7.3631e-03, -1.3426e-02,  1.3107e-02],\n",
       "                                     [-5.8294e-03, -2.9783e-02, -3.6869e-02],\n",
       "                                     [-3.1904e-03, -8.4772e-03,  7.7155e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-2.7486e-03, -1.2278e-03, -1.5168e-02],\n",
       "                                     [ 1.6267e-02, -1.3577e-02,  3.6228e-03],\n",
       "                                     [ 1.3363e-02, -1.6316e-03, -4.7355e-03]],\n",
       "                           \n",
       "                                    [[-1.3294e-02, -9.4946e-03,  1.8491e-02],\n",
       "                                     [-2.2141e-02,  2.6023e-03, -1.5982e-02],\n",
       "                                     [-3.6634e-02,  2.9932e-02, -1.0920e-03]],\n",
       "                           \n",
       "                                    [[-2.1724e-02, -3.3395e-02, -1.5376e-03],\n",
       "                                     [-1.7558e-02,  3.2152e-02,  1.1809e-02],\n",
       "                                     [-2.0287e-04,  2.0670e-02, -5.0787e-03]]]], device='cuda:0')),\n",
       "                          ('model.6.conv.0.bias',\n",
       "                           tensor([ 2.7584e-05,  3.5679e-05,  8.0648e-06,  1.1330e-05,  4.1048e-05,\n",
       "                                   -1.8733e-05, -3.5897e-05, -2.6308e-05,  3.3725e-05,  1.0960e-05,\n",
       "                                   -5.4714e-06, -2.0112e-05,  4.0973e-05,  2.5213e-07, -4.5972e-05,\n",
       "                                    4.1851e-05,  2.3024e-05,  9.6426e-06, -6.0357e-06,  3.0588e-05,\n",
       "                                   -3.7768e-05,  4.5825e-06, -2.5462e-05, -2.9933e-05, -5.0314e-05,\n",
       "                                   -1.1819e-05, -3.4648e-05,  3.0105e-05, -3.7712e-05, -9.9954e-06,\n",
       "                                   -3.9079e-05,  2.8581e-05], device='cuda:0')),\n",
       "                          ('model.6.conv.1.weight',\n",
       "                           tensor([0.0932, 0.0980, 0.1102, 0.0885, 0.0838, 0.0894, 0.0884, 0.0966, 0.0960,\n",
       "                                   0.0969, 0.1117, 0.0949, 0.0837, 0.1063, 0.0901, 0.0849, 0.0852, 0.1077,\n",
       "                                   0.0949, 0.0773, 0.0977, 0.0846, 0.0875, 0.1153, 0.0891, 0.0886, 0.1001,\n",
       "                                   0.0823, 0.0797, 0.0929, 0.0992, 0.1228], device='cuda:0')),\n",
       "                          ('model.6.conv.1.bias',\n",
       "                           tensor([-0.0438, -0.0638, -0.0456, -0.0977, -0.0354, -0.0597, -0.0637, -0.0377,\n",
       "                                   -0.0568, -0.0395, -0.0314, -0.0301, -0.0745, -0.0524, -0.0632, -0.0333,\n",
       "                                   -0.0393, -0.0717, -0.0854, -0.0737, -0.0525, -0.0456, -0.0598, -0.1063,\n",
       "                                   -0.0436, -0.0514, -0.0396, -0.0136, -0.0497, -0.0502, -0.0733, -0.0538],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.6.conv.1.running_mean',\n",
       "                           tensor([-0.0102, -0.0397, -0.0737, -0.0141, -0.0456,  0.0277, -0.1231, -0.0179,\n",
       "                                   -0.0206, -0.1077, -0.1472, -0.0783,  0.0669,  0.0085, -0.0704, -0.0135,\n",
       "                                   -0.0652, -0.0514,  0.0190, -0.0525, -0.0241, -0.0295, -0.0223, -0.0829,\n",
       "                                   -0.0088, -0.0476, -0.0397, -0.0967,  0.0093,  0.0041, -0.0655, -0.0559],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.6.conv.1.running_var',\n",
       "                           tensor([0.0055, 0.0073, 0.0113, 0.0088, 0.0055, 0.0142, 0.0089, 0.0072, 0.0088,\n",
       "                                   0.0075, 0.0122, 0.0072, 0.0104, 0.0080, 0.0077, 0.0079, 0.0057, 0.0237,\n",
       "                                   0.0071, 0.0070, 0.0146, 0.0055, 0.0085, 0.0106, 0.0063, 0.0054, 0.0084,\n",
       "                                   0.0072, 0.0071, 0.0059, 0.0102, 0.0080], device='cuda:0')),\n",
       "                          ('model.6.conv.1.num_batches_tracked',\n",
       "                           tensor(70314, device='cuda:0')),\n",
       "                          ('model.6.conv.3.weight',\n",
       "                           tensor([[[[-1.4364e-02,  1.4834e-02,  3.1706e-04],\n",
       "                                     [-9.1370e-03, -2.7768e-03, -6.6473e-03],\n",
       "                                     [-1.6545e-03, -1.1306e-02, -3.0602e-03]],\n",
       "                           \n",
       "                                    [[-3.0495e-03,  2.4154e-02,  2.0697e-04],\n",
       "                                     [-3.5526e-03,  7.4231e-04, -2.7292e-03],\n",
       "                                     [-5.0251e-03,  8.4735e-03, -1.1702e-02]],\n",
       "                           \n",
       "                                    [[ 1.6957e-02, -1.2694e-02,  7.8785e-03],\n",
       "                                     [-1.3277e-02, -4.1308e-03,  7.7330e-03],\n",
       "                                     [ 7.7781e-04, -1.5741e-02, -2.7517e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-6.3862e-03,  1.6076e-02, -3.4789e-03],\n",
       "                                     [-1.5655e-03,  1.0056e-02, -1.6644e-03],\n",
       "                                     [ 1.2486e-03,  1.6742e-02,  6.5669e-03]],\n",
       "                           \n",
       "                                    [[-1.1562e-02,  2.0901e-03,  4.9398e-03],\n",
       "                                     [ 2.5479e-02,  6.2065e-03, -1.1012e-02],\n",
       "                                     [-1.3079e-02, -9.3543e-03, -1.6554e-02]],\n",
       "                           \n",
       "                                    [[ 1.2361e-02,  4.9421e-03, -3.5291e-03],\n",
       "                                     [-4.3875e-03,  7.0043e-02,  4.7546e-02],\n",
       "                                     [ 7.0155e-03, -7.7404e-03, -2.0054e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 9.5272e-03, -1.0350e-02,  1.1694e-02],\n",
       "                                     [ 1.1466e-02,  1.1959e-02, -3.5822e-03],\n",
       "                                     [-5.2405e-03,  4.7192e-03,  1.6468e-02]],\n",
       "                           \n",
       "                                    [[ 1.8760e-02,  1.0835e-02,  1.1090e-02],\n",
       "                                     [ 6.8485e-03,  9.1276e-03,  1.7227e-02],\n",
       "                                     [ 8.1663e-03,  1.8884e-02,  4.8540e-04]],\n",
       "                           \n",
       "                                    [[ 1.3255e-02,  3.6639e-03,  9.5290e-03],\n",
       "                                     [-1.2058e-02,  7.7574e-03,  2.0337e-02],\n",
       "                                     [ 2.1463e-03, -3.8037e-03,  1.2884e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-1.5225e-02, -5.2496e-03, -1.3107e-02],\n",
       "                                     [ 1.0113e-03,  9.9141e-03, -1.0356e-02],\n",
       "                                     [-5.6963e-03,  3.1170e-03, -1.6090e-02]],\n",
       "                           \n",
       "                                    [[-1.3482e-02, -1.7624e-02,  3.7233e-03],\n",
       "                                     [-1.7815e-02, -2.5644e-03,  3.2503e-02],\n",
       "                                     [-1.5004e-02, -2.1468e-03, -8.4388e-03]],\n",
       "                           \n",
       "                                    [[-9.5708e-03,  2.6429e-02, -8.3770e-04],\n",
       "                                     [-1.4615e-02, -2.6908e-03, -6.8536e-03],\n",
       "                                     [-2.2300e-02, -2.8255e-03,  3.8379e-04]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-9.0493e-03, -7.0924e-03, -3.0376e-03],\n",
       "                                     [-2.9205e-03,  8.3038e-03, -1.1037e-02],\n",
       "                                     [-2.3133e-03, -1.1980e-03, -1.2328e-02]],\n",
       "                           \n",
       "                                    [[ 3.6556e-03,  7.7158e-03,  1.3400e-02],\n",
       "                                     [ 3.0214e-03,  1.9627e-02,  2.9955e-03],\n",
       "                                     [ 2.2832e-02,  8.9710e-03,  5.4134e-03]],\n",
       "                           \n",
       "                                    [[ 5.3215e-03, -1.2852e-02, -1.6793e-03],\n",
       "                                     [-1.4865e-02, -1.2755e-02,  7.0067e-04],\n",
       "                                     [ 2.3715e-03, -1.1676e-02, -1.1665e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-2.0664e-03, -1.1903e-02, -1.4059e-03],\n",
       "                                     [-1.5941e-02,  1.2187e-02,  3.4524e-03],\n",
       "                                     [ 3.9437e-04, -2.1573e-02, -1.0173e-03]],\n",
       "                           \n",
       "                                    [[-2.1864e-03, -7.4493e-03, -1.1355e-02],\n",
       "                                     [-2.3188e-03, -1.3638e-02, -5.7688e-03],\n",
       "                                     [ 3.2407e-03, -5.8005e-04, -1.3038e-02]],\n",
       "                           \n",
       "                                    [[ 2.0762e-03, -7.3219e-03,  2.2109e-02],\n",
       "                                     [-3.9535e-03, -5.8742e-03,  1.4835e-02],\n",
       "                                     [ 3.3805e-03, -2.6205e-02, -2.3408e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   ...,\n",
       "                           \n",
       "                           \n",
       "                                   [[[-1.7030e-02,  8.4217e-03,  5.8783e-03],\n",
       "                                     [ 2.3777e-02,  2.3549e-02,  8.4015e-03],\n",
       "                                     [-8.1570e-03,  3.5834e-03, -2.0756e-03]],\n",
       "                           \n",
       "                                    [[-4.3763e-03,  9.9203e-03,  9.8999e-03],\n",
       "                                     [ 2.9355e-02,  8.2954e-03, -1.2641e-02],\n",
       "                                     [-1.8799e-03,  1.6325e-02, -9.9751e-03]],\n",
       "                           \n",
       "                                    [[ 1.6567e-02,  2.2597e-02,  1.8719e-02],\n",
       "                                     [-4.7540e-03, -8.8469e-03,  3.5160e-03],\n",
       "                                     [ 4.5387e-03, -1.6367e-02,  1.0682e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 6.3576e-03,  1.3312e-03, -7.4170e-03],\n",
       "                                     [ 1.1315e-02,  6.8284e-04,  4.0032e-02],\n",
       "                                     [-9.7173e-03,  1.1133e-03,  2.4618e-02]],\n",
       "                           \n",
       "                                    [[ 1.9852e-02,  8.1651e-03,  6.6230e-03],\n",
       "                                     [ 3.3045e-04, -2.0285e-02, -2.1191e-02],\n",
       "                                     [-1.7754e-02, -1.2961e-02, -1.6645e-02]],\n",
       "                           \n",
       "                                    [[ 3.8508e-03, -3.1965e-03, -1.1777e-02],\n",
       "                                     [ 2.4113e-03,  1.1486e-02,  3.1613e-02],\n",
       "                                     [-5.1296e-03,  1.8940e-02, -1.0202e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-1.9330e-02,  1.8125e-03, -5.4443e-03],\n",
       "                                     [-5.0636e-03, -1.4122e-03,  1.1228e-02],\n",
       "                                     [ 1.2209e-02, -1.5917e-02,  7.6552e-03]],\n",
       "                           \n",
       "                                    [[-1.9158e-02, -6.5929e-03, -2.7637e-03],\n",
       "                                     [-1.9954e-02, -4.2049e-03, -1.0348e-02],\n",
       "                                     [ 1.4488e-04, -7.6510e-03, -1.9638e-03]],\n",
       "                           \n",
       "                                    [[-6.5774e-03,  8.3622e-03, -6.5741e-03],\n",
       "                                     [ 1.5627e-02,  1.4662e-02, -2.7019e-02],\n",
       "                                     [ 1.0906e-04, -8.2913e-04,  2.0057e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 4.3476e-03,  1.0059e-02, -1.0689e-02],\n",
       "                                     [ 1.8787e-02, -2.8139e-03, -1.0879e-02],\n",
       "                                     [-7.9631e-04, -1.8614e-03, -1.6476e-02]],\n",
       "                           \n",
       "                                    [[ 3.4705e-03,  3.3023e-02,  3.4746e-03],\n",
       "                                     [-2.1241e-02,  8.6689e-03,  1.8725e-02],\n",
       "                                     [ 1.3382e-02, -1.9878e-02,  1.1276e-02]],\n",
       "                           \n",
       "                                    [[ 6.7543e-03,  4.8114e-02, -1.2879e-02],\n",
       "                                     [-1.5791e-03, -7.7757e-03, -2.0609e-02],\n",
       "                                     [ 1.0026e-02, -2.1729e-03,  7.9941e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-1.5503e-02,  5.4900e-05, -3.8049e-03],\n",
       "                                     [-9.4828e-04,  5.3377e-03, -7.1849e-03],\n",
       "                                     [-1.6652e-02,  4.7911e-06, -1.1664e-02]],\n",
       "                           \n",
       "                                    [[ 2.4950e-02,  1.3164e-02,  2.0400e-02],\n",
       "                                     [-3.5830e-03,  1.5157e-02, -6.8611e-03],\n",
       "                                     [-7.4841e-03,  2.3988e-03,  2.3367e-03]],\n",
       "                           \n",
       "                                    [[ 1.5543e-02, -1.6972e-02, -1.0191e-02],\n",
       "                                     [-2.0007e-02, -1.9620e-02, -2.5714e-02],\n",
       "                                     [-1.0409e-02, -5.9163e-03,  5.9894e-05]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 1.1099e-02,  4.7560e-03, -4.1352e-03],\n",
       "                                     [-8.0404e-03, -6.0287e-03, -1.2202e-03],\n",
       "                                     [ 1.4738e-02, -1.0929e-02,  1.4215e-02]],\n",
       "                           \n",
       "                                    [[ 3.5181e-03, -2.8818e-03, -7.8942e-03],\n",
       "                                     [-2.0303e-02, -4.3235e-03, -2.1127e-03],\n",
       "                                     [ 5.7718e-03,  3.9526e-03,  1.2538e-02]],\n",
       "                           \n",
       "                                    [[-1.5189e-02,  1.9356e-02, -1.1101e-02],\n",
       "                                     [ 5.3148e-03,  4.1670e-02,  4.1819e-03],\n",
       "                                     [-1.6649e-02,  1.2705e-03, -3.4269e-05]]]], device='cuda:0')),\n",
       "                          ('model.6.conv.3.bias',\n",
       "                           tensor([ 4.4727e-05, -1.3672e-05,  2.7998e-05, -1.3877e-05, -2.3753e-05,\n",
       "                                   -2.6985e-05, -3.3462e-05, -1.5345e-05,  2.8333e-05, -2.1273e-05,\n",
       "                                   -1.9847e-05, -8.5613e-06,  3.6548e-05, -4.7605e-07, -2.8634e-06,\n",
       "                                   -1.0857e-06, -1.7080e-05,  3.6099e-05, -2.3252e-05,  4.8721e-05,\n",
       "                                   -3.3192e-05, -1.7459e-05,  4.7086e-05, -2.0577e-05,  1.4150e-05,\n",
       "                                    9.8960e-06, -2.1391e-05, -1.5400e-05,  2.5623e-06, -1.9191e-06,\n",
       "                                    3.4898e-05, -8.0501e-06], device='cuda:0')),\n",
       "                          ('model.6.conv.4.weight',\n",
       "                           tensor([0.1386, 0.1447, 0.1136, 0.1442, 0.1020, 0.1369, 0.1034, 0.1561, 0.1621,\n",
       "                                   0.1334, 0.1050, 0.1350, 0.1139, 0.1779, 0.1616, 0.1995, 0.1669, 0.1251,\n",
       "                                   0.1334, 0.1370, 0.0997, 0.1586, 0.1575, 0.1060, 0.1688, 0.0699, 0.1448,\n",
       "                                   0.1810, 0.1798, 0.1492, 0.1264, 0.1242], device='cuda:0')),\n",
       "                          ('model.6.conv.4.bias',\n",
       "                           tensor([-0.0867, -0.1348, -0.0827, -0.1083, -0.0879, -0.0882, -0.0664, -0.0975,\n",
       "                                   -0.0779, -0.0957, -0.0797, -0.0931, -0.0909, -0.0697, -0.1111, -0.1141,\n",
       "                                   -0.0914, -0.0703, -0.0720, -0.0903, -0.0802, -0.1094, -0.1022, -0.0602,\n",
       "                                   -0.1027, -0.0721, -0.1125, -0.0833, -0.1022, -0.0984, -0.0838, -0.0689],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.6.conv.4.running_mean',\n",
       "                           tensor([ 1.7262e-03,  7.6532e-03, -9.4493e-03, -2.5005e-03, -1.6498e-02,\n",
       "                                   -9.6307e-03,  3.8633e-05, -2.7211e-03, -2.8567e-03, -3.6477e-03,\n",
       "                                    4.2536e-03, -2.9880e-04,  3.6192e-03, -1.2363e-02,  1.4914e-03,\n",
       "                                   -1.1065e-03,  3.9350e-03,  7.5078e-03, -8.0796e-03,  3.8577e-03,\n",
       "                                   -4.8941e-03,  2.3365e-03, -9.4190e-03,  6.1525e-03, -1.0199e-02,\n",
       "                                   -2.2490e-03, -4.4436e-03, -1.8041e-02,  7.2468e-03,  5.0389e-05,\n",
       "                                   -8.7640e-04, -9.2089e-03], device='cuda:0')),\n",
       "                          ('model.6.conv.4.running_var',\n",
       "                           tensor([1.1585e-04, 1.6162e-04, 1.0333e-04, 1.5692e-04, 1.2027e-04, 9.9602e-05,\n",
       "                                   1.0574e-04, 1.3600e-04, 1.3728e-04, 1.1463e-04, 1.3564e-04, 1.0916e-04,\n",
       "                                   1.1895e-04, 4.2082e-04, 1.4609e-04, 1.1952e-04, 1.3771e-04, 2.5780e-04,\n",
       "                                   1.6125e-04, 1.3383e-04, 8.2222e-05, 1.1329e-04, 1.2270e-04, 1.9092e-04,\n",
       "                                   1.2787e-04, 6.4225e-05, 1.3227e-04, 1.8913e-04, 3.6273e-04, 1.2304e-04,\n",
       "                                   8.0287e-05, 1.3797e-04], device='cuda:0')),\n",
       "                          ('model.6.conv.4.num_batches_tracked',\n",
       "                           tensor(70314, device='cuda:0')),\n",
       "                          ('model.7.conv.0.weight',\n",
       "                           tensor([[[[-4.8984e-03, -2.0611e-02, -1.0263e-02],\n",
       "                                     [ 9.4498e-03,  8.9323e-03,  2.0434e-02],\n",
       "                                     [-1.4125e-02,  5.3934e-03, -6.5464e-03]],\n",
       "                           \n",
       "                                    [[-1.9122e-02,  2.8437e-03, -1.7348e-02],\n",
       "                                     [ 6.2473e-03,  1.4571e-02, -1.2791e-02],\n",
       "                                     [-7.9794e-03, -2.0662e-02, -2.3825e-02]],\n",
       "                           \n",
       "                                    [[-1.7932e-03,  9.0512e-03, -1.2412e-02],\n",
       "                                     [-8.7498e-03, -3.8423e-03, -6.3513e-03],\n",
       "                                     [-6.8106e-03, -6.3026e-03, -7.2096e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 6.1080e-03,  4.2273e-02, -5.8575e-03],\n",
       "                                     [ 1.3945e-02, -2.3618e-03, -4.4880e-03],\n",
       "                                     [-7.9929e-03, -6.2445e-03, -8.3783e-03]],\n",
       "                           \n",
       "                                    [[-5.7188e-04, -7.2966e-03,  2.3431e-02],\n",
       "                                     [-3.9305e-03, -3.4230e-03,  2.6759e-02],\n",
       "                                     [ 5.7238e-03, -8.4181e-03,  7.6598e-03]],\n",
       "                           \n",
       "                                    [[-1.0527e-02, -9.9506e-04,  1.4795e-02],\n",
       "                                     [-8.0980e-03,  2.2832e-02,  7.8477e-04],\n",
       "                                     [-1.7031e-02,  4.6868e-03, -7.3194e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-4.5150e-03, -7.7837e-03, -7.5232e-03],\n",
       "                                     [-9.0703e-03, -3.7578e-04,  2.8407e-04],\n",
       "                                     [ 8.3644e-04,  1.1843e-02,  1.7888e-03]],\n",
       "                           \n",
       "                                    [[-1.8504e-02, -8.5265e-03, -2.0547e-02],\n",
       "                                     [-1.2317e-02,  1.0134e-02, -2.7528e-03],\n",
       "                                     [-1.5512e-02, -1.3925e-03, -4.1750e-03]],\n",
       "                           \n",
       "                                    [[-1.6625e-02, -8.7081e-03,  7.3191e-03],\n",
       "                                     [-2.0437e-02, -4.0161e-03, -8.0913e-03],\n",
       "                                     [-5.6166e-03, -1.1105e-02, -1.2100e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-5.1586e-03, -2.1132e-02, -2.0991e-02],\n",
       "                                     [ 1.1421e-02,  9.6085e-04,  4.3547e-03],\n",
       "                                     [-8.2570e-03,  8.6227e-03,  8.0670e-03]],\n",
       "                           \n",
       "                                    [[-1.1036e-02,  2.9055e-02, -1.2524e-03],\n",
       "                                     [-2.3365e-02,  7.3281e-03, -5.4705e-03],\n",
       "                                     [ 2.0018e-02, -2.5776e-03,  9.5158e-03]],\n",
       "                           \n",
       "                                    [[-4.2654e-03, -2.3374e-03, -1.6280e-04],\n",
       "                                     [ 1.4561e-02,  7.6414e-03, -1.7527e-02],\n",
       "                                     [-9.4428e-05, -1.3720e-02, -4.0138e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-1.6470e-03, -1.0110e-02,  3.9365e-03],\n",
       "                                     [-4.3866e-03, -1.2752e-02, -8.6572e-03],\n",
       "                                     [-5.5619e-03, -2.5266e-02, -6.9670e-03]],\n",
       "                           \n",
       "                                    [[-7.8181e-03,  6.1451e-03, -8.0921e-03],\n",
       "                                     [-3.3241e-03, -1.2667e-02, -6.7263e-03],\n",
       "                                     [-1.9536e-04,  1.1414e-02, -8.0524e-03]],\n",
       "                           \n",
       "                                    [[ 6.5428e-03,  8.2835e-03,  1.0082e-02],\n",
       "                                     [ 1.0825e-02,  1.1105e-02,  9.7278e-03],\n",
       "                                     [ 1.2032e-02,  6.9685e-03,  6.7934e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 5.8491e-05, -5.5836e-03,  7.2650e-03],\n",
       "                                     [ 4.1784e-03, -2.0456e-03, -4.9826e-03],\n",
       "                                     [ 1.1336e-02, -6.8485e-03, -7.1700e-04]],\n",
       "                           \n",
       "                                    [[ 3.7259e-03,  7.8329e-03,  1.1246e-03],\n",
       "                                     [ 3.1263e-03,  4.1354e-04, -5.6482e-03],\n",
       "                                     [ 3.5649e-03, -4.0326e-03, -1.3538e-02]],\n",
       "                           \n",
       "                                    [[ 1.4180e-02, -4.7907e-03, -1.0381e-02],\n",
       "                                     [ 1.0629e-02,  2.9335e-03, -3.7885e-03],\n",
       "                                     [ 1.6415e-02,  1.1988e-02,  7.9327e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   ...,\n",
       "                           \n",
       "                           \n",
       "                                   [[[-2.0414e-03, -8.7343e-03, -1.7026e-02],\n",
       "                                     [-3.7169e-03, -1.1160e-02, -1.8663e-02],\n",
       "                                     [ 4.2390e-03, -1.3041e-02, -1.0892e-03]],\n",
       "                           \n",
       "                                    [[-7.7892e-03, -1.4400e-02, -6.8764e-03],\n",
       "                                     [ 9.8393e-03, -5.3740e-03, -1.6936e-02],\n",
       "                                     [ 1.3833e-02,  5.4701e-03, -6.6985e-03]],\n",
       "                           \n",
       "                                    [[-1.5298e-02, -1.0836e-02, -8.2197e-03],\n",
       "                                     [-8.8540e-03, -1.5326e-02,  6.5842e-03],\n",
       "                                     [-5.5442e-03, -5.6491e-03, -1.4158e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-2.7708e-02,  6.1304e-03, -1.2916e-02],\n",
       "                                     [ 2.5553e-03, -1.2470e-02, -6.9781e-03],\n",
       "                                     [-2.0363e-02,  2.2806e-02, -1.2289e-02]],\n",
       "                           \n",
       "                                    [[-1.9897e-02,  1.3519e-02, -6.5512e-03],\n",
       "                                     [ 9.7215e-04, -1.0904e-02, -2.7965e-02],\n",
       "                                     [ 1.7728e-02, -1.5131e-03, -1.8224e-02]],\n",
       "                           \n",
       "                                    [[ 2.6161e-02,  9.3436e-03, -1.2050e-02],\n",
       "                                     [ 3.9653e-03, -9.7729e-03, -2.4745e-02],\n",
       "                                     [ 2.6520e-02, -2.3993e-02, -1.0743e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 6.4412e-03,  1.7868e-02, -1.5646e-02],\n",
       "                                     [-1.6262e-02,  2.3896e-02, -1.3119e-02],\n",
       "                                     [ 2.4660e-02, -1.7954e-02,  9.8647e-03]],\n",
       "                           \n",
       "                                    [[ 1.2749e-03,  1.0998e-02,  1.8586e-02],\n",
       "                                     [-9.9894e-03,  5.0263e-02,  1.8827e-03],\n",
       "                                     [-1.9456e-02,  1.5838e-02, -5.8036e-03]],\n",
       "                           \n",
       "                                    [[ 8.8147e-03,  2.0516e-03, -6.0095e-03],\n",
       "                                     [-2.0636e-04,  5.4571e-02, -1.8318e-03],\n",
       "                                     [-1.0597e-02, -8.5241e-03,  1.0635e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 9.7162e-03, -1.1667e-02, -2.8080e-03],\n",
       "                                     [-1.7276e-02,  1.1223e-02, -5.5163e-03],\n",
       "                                     [ 2.9896e-02,  5.5104e-02,  1.2618e-02]],\n",
       "                           \n",
       "                                    [[-1.5056e-02,  2.5994e-03,  1.8704e-03],\n",
       "                                     [-2.8006e-04, -1.5512e-02,  2.4678e-02],\n",
       "                                     [ 2.3484e-03, -2.1585e-03, -1.8857e-02]],\n",
       "                           \n",
       "                                    [[ 2.6391e-03, -1.4776e-02, -1.6324e-02],\n",
       "                                     [-1.0890e-02, -1.0037e-02, -8.0293e-03],\n",
       "                                     [-1.8298e-03,  8.6456e-03, -4.9971e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 3.8813e-03,  5.5320e-03,  4.3970e-03],\n",
       "                                     [-1.8946e-02,  2.3925e-02,  6.1904e-03],\n",
       "                                     [-2.7718e-02, -4.5547e-03, -1.1205e-02]],\n",
       "                           \n",
       "                                    [[ 1.6754e-02,  5.6956e-02, -1.6645e-02],\n",
       "                                     [ 3.6269e-02, -1.1802e-02, -2.8951e-02],\n",
       "                                     [-5.7087e-03,  3.8568e-03,  1.0181e-02]],\n",
       "                           \n",
       "                                    [[ 1.9805e-02, -4.6649e-04,  6.8931e-03],\n",
       "                                     [-1.7228e-02,  2.4106e-03,  1.4755e-02],\n",
       "                                     [-3.5641e-03,  1.2010e-02, -1.3192e-02]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 1.5194e-02,  1.8042e-02,  3.0787e-03],\n",
       "                                     [-2.5004e-02,  4.3154e-03,  2.0883e-02],\n",
       "                                     [ 7.8480e-03, -2.8817e-03,  1.5885e-02]],\n",
       "                           \n",
       "                                    [[-2.1627e-02,  1.0991e-02, -7.4228e-03],\n",
       "                                     [-3.1786e-03,  1.6096e-02, -8.6669e-03],\n",
       "                                     [-1.5929e-02,  1.7637e-02, -1.7401e-02]],\n",
       "                           \n",
       "                                    [[ 2.4747e-04,  2.5465e-02,  8.2836e-03],\n",
       "                                     [-1.2733e-02, -7.2934e-03,  2.5428e-02],\n",
       "                                     [ 8.5436e-03, -6.9694e-03,  2.5591e-02]]]], device='cuda:0')),\n",
       "                          ('model.7.conv.0.bias',\n",
       "                           tensor([-1.0297e-06, -3.8313e-05, -3.2893e-05, -5.1313e-05, -4.9679e-05,\n",
       "                                    2.2625e-05,  3.9625e-05, -3.7586e-05, -3.1242e-05,  4.2205e-05,\n",
       "                                    7.7362e-06,  9.1514e-06, -1.7838e-05,  4.6042e-05, -4.6944e-05,\n",
       "                                   -3.5117e-05,  5.6053e-06,  1.1903e-05, -1.5451e-05, -9.2572e-06,\n",
       "                                   -5.0799e-05,  7.0369e-06,  4.0671e-05, -5.1058e-06,  4.8622e-05,\n",
       "                                   -1.3752e-05, -1.7878e-05,  1.7193e-05,  1.3512e-05, -3.2850e-05,\n",
       "                                    6.2169e-06, -2.9099e-05], device='cuda:0')),\n",
       "                          ('model.7.conv.1.weight',\n",
       "                           tensor([0.0965, 0.0882, 0.0719, 0.0889, 0.0850, 0.0878, 0.0796, 0.0747, 0.0866,\n",
       "                                   0.1157, 0.1053, 0.0745, 0.0720, 0.0944, 0.0986, 0.0944, 0.0901, 0.0912,\n",
       "                                   0.0892, 0.0880, 0.0936, 0.0758, 0.0918, 0.0726, 0.0810, 0.0964, 0.0891,\n",
       "                                   0.0736, 0.1007, 0.0694, 0.0766, 0.0939], device='cuda:0')),\n",
       "                          ('model.7.conv.1.bias',\n",
       "                           tensor([-0.0355, -0.0596, -0.0559, -0.0541, -0.0357, -0.0503, -0.0179, -0.0142,\n",
       "                                   -0.0295, -0.0894, -0.1069, -0.0452,  0.0052, -0.0513, -0.0252, -0.0814,\n",
       "                                   -0.0391, -0.0412, -0.0159, -0.0287, -0.0807, -0.0308, -0.0443, -0.0105,\n",
       "                                   -0.0224, -0.0725, -0.0268, -0.0178, -0.0661,  0.0022, -0.0507, -0.0589],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.7.conv.1.running_mean',\n",
       "                           tensor([ 0.0128, -0.0561,  0.0185,  0.0229, -0.0682, -0.0313, -0.0878, -0.0003,\n",
       "                                   -0.0753, -0.0716, -0.0207, -0.0037, -0.0449, -0.0110, -0.1400, -0.0444,\n",
       "                                    0.0100, -0.0299, -0.0725, -0.0235,  0.0017, -0.0587, -0.0540, -0.0527,\n",
       "                                   -0.0372,  0.0221, -0.0232, -0.0668,  0.0284, -0.1299,  0.0270,  0.0094],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.7.conv.1.running_var',\n",
       "                           tensor([0.0091, 0.0097, 0.0150, 0.0061, 0.0110, 0.0060, 0.0081, 0.0079, 0.0119,\n",
       "                                   0.0096, 0.0120, 0.0050, 0.0098, 0.0081, 0.0098, 0.0082, 0.0073, 0.0077,\n",
       "                                   0.0103, 0.0060, 0.0145, 0.0061, 0.0103, 0.0064, 0.0063, 0.0088, 0.0051,\n",
       "                                   0.0050, 0.0070, 0.0081, 0.0069, 0.0075], device='cuda:0')),\n",
       "                          ('model.7.conv.1.num_batches_tracked',\n",
       "                           tensor(70314, device='cuda:0')),\n",
       "                          ('model.7.conv.3.weight',\n",
       "                           tensor([[[[-1.1129e-02,  1.4186e-02,  6.9966e-03],\n",
       "                                     [ 1.9313e-02,  3.2317e-03,  8.7174e-04],\n",
       "                                     [-8.3139e-03, -9.9721e-03,  2.9618e-04]],\n",
       "                           \n",
       "                                    [[-1.2744e-03, -7.5578e-03,  8.8319e-04],\n",
       "                                     [-2.0961e-04,  7.0686e-03, -2.0094e-03],\n",
       "                                     [ 7.0237e-03, -2.8096e-03,  7.1769e-03]],\n",
       "                           \n",
       "                                    [[ 2.8359e-03, -8.4151e-03,  6.9210e-03],\n",
       "                                     [-2.8439e-04, -2.1843e-03,  2.7659e-03],\n",
       "                                     [ 7.1695e-03, -6.2677e-03, -4.3514e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[ 2.3670e-06, -2.1660e-02, -8.8158e-03],\n",
       "                                     [-1.1037e-02, -1.0021e-02,  4.9800e-03],\n",
       "                                     [-4.3457e-03, -2.9596e-03,  5.9767e-03]],\n",
       "                           \n",
       "                                    [[-3.7575e-03, -2.3943e-02,  1.6503e-02],\n",
       "                                     [ 4.2545e-04,  4.3469e-02, -1.3383e-02],\n",
       "                                     [ 1.6570e-03,  1.4092e-02, -2.1765e-03]],\n",
       "                           \n",
       "                                    [[-2.7282e-03, -9.1825e-03, -1.0963e-02],\n",
       "                                     [-5.3990e-03,  1.8084e-02, -2.5727e-02],\n",
       "                                     [ 1.8006e-02, -6.1732e-03,  1.0189e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-1.3231e-02, -1.5159e-02,  1.0462e-03],\n",
       "                                     [-2.5049e-03,  9.0608e-03, -9.3966e-04],\n",
       "                                     [-4.4902e-03, -2.5458e-03, -1.6763e-03]],\n",
       "                           \n",
       "                                    [[-1.3343e-02, -2.0885e-02, -2.7124e-03],\n",
       "                                     [-1.9055e-03,  2.2046e-03,  3.6078e-03],\n",
       "                                     [-7.5044e-03, -1.2906e-02,  4.0978e-03]],\n",
       "                           \n",
       "                                    [[-1.7814e-02,  1.9010e-03,  6.3466e-03],\n",
       "                                     [-1.4398e-02, -6.9686e-03, -4.3126e-03],\n",
       "                                     [-1.3957e-02,  2.5547e-03,  3.0871e-04]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-1.7933e-02,  4.3225e-03,  1.1735e-02],\n",
       "                                     [ 7.1310e-03,  8.6327e-03,  2.3062e-03],\n",
       "                                     [ 6.7228e-04, -8.9720e-03,  6.8230e-03]],\n",
       "                           \n",
       "                                    [[-2.2411e-03,  8.7389e-03, -3.2916e-02],\n",
       "                                     [ 6.6145e-03,  6.3421e-02, -1.8370e-02],\n",
       "                                     [ 7.2133e-03, -6.3382e-03, -1.0048e-02]],\n",
       "                           \n",
       "                                    [[ 3.7836e-03,  7.6635e-03, -8.4251e-03],\n",
       "                                     [-2.3916e-02, -1.0781e-02,  2.5724e-02],\n",
       "                                     [-3.0437e-02,  5.2329e-02,  1.7786e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 7.4509e-04, -6.3676e-03,  9.2149e-03],\n",
       "                                     [ 6.4911e-03, -1.9754e-03,  3.7052e-03],\n",
       "                                     [ 2.8522e-03, -1.0824e-02, -5.2499e-04]],\n",
       "                           \n",
       "                                    [[ 1.7206e-03, -7.2571e-03,  6.9918e-03],\n",
       "                                     [-9.9157e-03, -1.2088e-02,  8.7216e-03],\n",
       "                                     [ 3.3384e-03, -6.0030e-03, -1.4330e-03]],\n",
       "                           \n",
       "                                    [[ 1.2528e-03, -4.0156e-03, -2.0458e-03],\n",
       "                                     [-4.6123e-03,  3.4708e-03, -1.1734e-04],\n",
       "                                     [ 2.9455e-03,  7.7515e-03,  4.1590e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-6.7995e-03, -3.3723e-03, -1.8987e-03],\n",
       "                                     [ 1.8578e-03, -5.3539e-03, -1.1084e-02],\n",
       "                                     [-7.6989e-04, -7.2152e-03, -1.3556e-03]],\n",
       "                           \n",
       "                                    [[-6.0663e-03, -5.8239e-03, -2.0991e-02],\n",
       "                                     [-3.3095e-03,  3.5559e-02, -1.2068e-02],\n",
       "                                     [-1.7511e-02, -2.9378e-05, -1.0675e-02]],\n",
       "                           \n",
       "                                    [[-1.0129e-02,  7.6133e-03, -1.1536e-02],\n",
       "                                     [ 7.0559e-03,  4.6595e-03, -9.3293e-03],\n",
       "                                     [-1.0042e-03, -1.5496e-02,  7.0761e-03]]],\n",
       "                           \n",
       "                           \n",
       "                                   ...,\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 1.0962e-02, -1.4797e-02, -8.2299e-03],\n",
       "                                     [-4.3252e-03, -5.3477e-03,  1.3996e-02],\n",
       "                                     [ 1.6037e-02,  3.7169e-02,  1.0215e-02]],\n",
       "                           \n",
       "                                    [[-4.9865e-03, -3.5334e-03,  8.5219e-04],\n",
       "                                     [-5.9402e-03, -8.2039e-03,  1.0956e-02],\n",
       "                                     [ 5.9776e-03, -1.8998e-03, -8.4138e-03]],\n",
       "                           \n",
       "                                    [[ 1.9780e-03,  2.3004e-03,  7.0138e-03],\n",
       "                                     [-1.0560e-02,  6.3367e-03, -4.1251e-04],\n",
       "                                     [ 9.3321e-03, -3.8119e-03,  8.2892e-04]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-2.3827e-02,  1.1293e-02, -2.6053e-02],\n",
       "                                     [-1.0748e-03, -2.1637e-02, -1.2888e-03],\n",
       "                                     [-6.8628e-03, -6.3366e-04, -7.0808e-03]],\n",
       "                           \n",
       "                                    [[ 1.4130e-02,  5.2421e-02,  2.2584e-02],\n",
       "                                     [-1.2608e-02,  5.3055e-03, -1.3818e-02],\n",
       "                                     [-3.9374e-03, -4.1238e-03, -8.1606e-03]],\n",
       "                           \n",
       "                                    [[ 2.4827e-02,  7.6227e-03,  1.3699e-02],\n",
       "                                     [-3.2935e-03,  1.2224e-03, -2.5129e-02],\n",
       "                                     [-3.0967e-03,  1.0360e-03, -2.8501e-04]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[ 6.2502e-03,  3.0882e-03,  7.9222e-03],\n",
       "                                     [ 8.8853e-03, -3.4526e-03, -3.5456e-03],\n",
       "                                     [-4.1942e-03, -3.8781e-03, -3.9069e-03]],\n",
       "                           \n",
       "                                    [[-1.4664e-02, -1.0861e-02,  8.4386e-03],\n",
       "                                     [-5.0791e-04,  7.5230e-03, -9.4020e-03],\n",
       "                                     [-1.4954e-02,  2.3836e-02,  7.0152e-03]],\n",
       "                           \n",
       "                                    [[ 4.6083e-03,  1.6516e-02,  1.1396e-02],\n",
       "                                     [ 1.7842e-03, -1.1703e-03, -1.1148e-02],\n",
       "                                     [-6.4353e-03,  3.2268e-03, -2.7964e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-1.2924e-02,  1.5491e-02,  2.0646e-02],\n",
       "                                     [-4.4617e-03, -2.1216e-04,  6.2494e-03],\n",
       "                                     [-6.5111e-03,  8.2693e-03, -1.1753e-02]],\n",
       "                           \n",
       "                                    [[-2.1282e-02, -9.4765e-04,  2.8503e-03],\n",
       "                                     [ 2.7025e-02, -8.9074e-03, -9.6876e-04],\n",
       "                                     [-4.5788e-03, -5.2528e-03, -1.0312e-03]],\n",
       "                           \n",
       "                                    [[-1.0032e-02,  2.0695e-02, -8.5763e-03],\n",
       "                                     [-1.8461e-02,  4.2605e-02, -4.6533e-03],\n",
       "                                     [-2.0494e-02,  8.8245e-03, -1.3572e-02]]],\n",
       "                           \n",
       "                           \n",
       "                                   [[[-6.5727e-03,  4.9377e-03, -1.5082e-02],\n",
       "                                     [-3.5153e-03,  8.8779e-03,  2.0674e-03],\n",
       "                                     [ 6.8131e-03, -3.1832e-03, -5.9498e-03]],\n",
       "                           \n",
       "                                    [[-2.8791e-03, -2.9970e-03,  1.0139e-02],\n",
       "                                     [-1.0495e-02, -1.4066e-02,  1.8958e-02],\n",
       "                                     [-1.1087e-02, -1.3972e-02, -7.3326e-03]],\n",
       "                           \n",
       "                                    [[ 3.7522e-03, -7.6796e-03, -1.3134e-02],\n",
       "                                     [ 5.8375e-03,  2.6807e-03,  2.9239e-03],\n",
       "                                     [ 1.1200e-03, -1.0221e-02,  4.5463e-03]],\n",
       "                           \n",
       "                                    ...,\n",
       "                           \n",
       "                                    [[-2.2021e-02, -2.6300e-02,  2.5311e-02],\n",
       "                                     [-1.0747e-02, -1.8823e-02,  6.4231e-03],\n",
       "                                     [ 4.6838e-03, -2.4413e-03,  3.6212e-02]],\n",
       "                           \n",
       "                                    [[-5.9907e-03, -3.2467e-03, -3.6480e-03],\n",
       "                                     [-3.7984e-03, -3.6943e-03, -1.4656e-02],\n",
       "                                     [-2.1983e-02, -1.4938e-02, -1.2476e-02]],\n",
       "                           \n",
       "                                    [[ 2.5621e-02,  2.7254e-03,  8.1159e-03],\n",
       "                                     [ 2.1792e-02, -2.4023e-02, -1.2465e-02],\n",
       "                                     [ 9.8263e-03,  1.0516e-02, -3.0977e-03]]]], device='cuda:0')),\n",
       "                          ('model.7.conv.3.bias',\n",
       "                           tensor([-5.1644e-07, -1.6173e-05, -1.2076e-06, -9.1354e-06, -9.9382e-06,\n",
       "                                   -4.4100e-05,  4.3354e-05, -4.2161e-06,  2.0942e-05,  1.7177e-05,\n",
       "                                    1.5601e-05, -4.4463e-05,  3.9081e-05, -2.8221e-05,  3.3393e-06,\n",
       "                                   -2.6286e-05,  4.0276e-05, -1.8465e-05,  1.0499e-05, -8.1687e-06,\n",
       "                                    3.1867e-05, -8.8313e-06, -3.7126e-05,  1.6232e-05,  7.1905e-06,\n",
       "                                   -1.9050e-06,  2.3538e-05,  4.8606e-05, -9.9748e-06, -4.1250e-06,\n",
       "                                    1.4397e-05,  1.4741e-05], device='cuda:0')),\n",
       "                          ('model.7.conv.4.weight',\n",
       "                           tensor([0.1563, 0.2008, 0.1167, 0.1540, 0.2174, 0.1475, 0.1175, 0.1377, 0.1473,\n",
       "                                   0.1751, 0.1341, 0.1609, 0.2146, 0.1591, 0.1757, 0.1760, 0.1438, 0.1529,\n",
       "                                   0.0984, 0.1824, 0.2046, 0.2121, 0.2079, 0.0556, 0.1718, 0.1329, 0.1905,\n",
       "                                   0.1770, 0.1766, 0.1809, 0.1572, 0.1595], device='cuda:0')),\n",
       "                          ('model.7.conv.4.bias',\n",
       "                           tensor([-0.1162, -0.1440, -0.1048, -0.1317, -0.1312, -0.1187, -0.1090, -0.1175,\n",
       "                                   -0.1045, -0.1325, -0.0794, -0.1171, -0.1259, -0.1247, -0.1356, -0.1448,\n",
       "                                   -0.1270, -0.0888, -0.0850, -0.1284, -0.1261, -0.1375, -0.1443, -0.0637,\n",
       "                                   -0.1405, -0.0926, -0.1342, -0.1470, -0.1415, -0.1249, -0.1296, -0.1084],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.7.conv.4.running_mean',\n",
       "                           tensor([-1.5915e-03, -1.1495e-03, -6.0396e-03, -4.9255e-04,  1.0723e-03,\n",
       "                                    1.0221e-03, -6.8008e-03, -3.6029e-03, -6.0188e-03, -2.8402e-03,\n",
       "                                   -8.2460e-04, -4.1124e-03, -8.2679e-04, -1.5713e-05, -2.3122e-04,\n",
       "                                    6.4600e-04, -8.8404e-04, -4.8281e-03, -1.2098e-02, -3.4997e-03,\n",
       "                                    4.3633e-04, -9.1351e-04, -3.6311e-03, -4.5829e-03, -4.1972e-03,\n",
       "                                    4.0407e-04,  1.6300e-03,  3.2579e-03, -3.9835e-03, -1.5550e-03,\n",
       "                                   -5.5911e-03, -8.3820e-05], device='cuda:0')),\n",
       "                          ('model.7.conv.4.running_var',\n",
       "                           tensor([8.5422e-05, 1.4643e-04, 6.9746e-05, 1.5956e-04, 1.8574e-04, 9.6435e-05,\n",
       "                                   7.4909e-05, 9.2589e-05, 9.4506e-05, 1.1767e-04, 1.7844e-04, 8.9289e-05,\n",
       "                                   1.4697e-04, 2.2193e-04, 1.0466e-04, 1.1231e-04, 8.9215e-05, 1.3497e-04,\n",
       "                                   9.8673e-05, 1.1965e-04, 1.4785e-04, 1.3769e-04, 1.2946e-04, 5.4956e-05,\n",
       "                                   9.7773e-05, 1.3167e-04, 1.2160e-04, 1.6480e-04, 2.3310e-04, 1.1665e-04,\n",
       "                                   8.6134e-05, 1.4254e-04], device='cuda:0')),\n",
       "                          ('model.7.conv.4.num_batches_tracked',\n",
       "                           tensor(70314, device='cuda:0')),\n",
       "                          ('model.9.model.0.weight',\n",
       "                           tensor([[-1.9267e-03, -5.9513e-03, -2.5996e-03,  ...,  5.2714e-03,\n",
       "                                     4.5534e-03,  8.4736e-05],\n",
       "                                   [ 1.4030e-02,  3.5738e-03,  3.0823e-03,  ..., -1.0672e-03,\n",
       "                                    -3.0879e-03, -6.2582e-03],\n",
       "                                   [ 9.6176e-04, -7.4544e-04, -4.7011e-04,  ..., -4.5611e-04,\n",
       "                                    -2.1486e-03,  4.3569e-05],\n",
       "                                   ...,\n",
       "                                   [ 7.7633e-04,  5.8664e-04, -1.7178e-04,  ..., -5.7227e-04,\n",
       "                                    -1.0728e-04, -1.1064e-03],\n",
       "                                   [ 3.0227e-03, -8.1701e-04,  2.9266e-03,  ...,  1.0618e-03,\n",
       "                                    -3.3985e-03, -8.3193e-04],\n",
       "                                   [-7.7740e-05, -1.9142e-03, -2.7677e-03,  ...,  9.1528e-04,\n",
       "                                     1.6874e-03, -4.1725e-04]], device='cuda:0')),\n",
       "                          ('model.9.model.0.bias',\n",
       "                           tensor([-0.0144, -0.0101, -0.0008,  ..., -0.0016, -0.0107,  0.0034],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.9.model.2.weight',\n",
       "                           tensor([[ 1.9239e-03,  4.7890e-04,  4.8436e-04,  ...,  6.5673e-04,\n",
       "                                    -1.5258e-03,  6.3536e-04],\n",
       "                                   [-7.9441e-03, -1.4103e-02, -1.2753e-03,  ..., -3.3478e-03,\n",
       "                                     6.7714e-03,  6.7209e-03],\n",
       "                                   [-3.2390e-03,  6.7051e-03,  3.0460e-03,  ..., -8.3542e-04,\n",
       "                                     8.3678e-03, -3.2876e-03],\n",
       "                                   ...,\n",
       "                                   [ 1.8865e-04,  2.6413e-05,  5.3431e-05,  ...,  3.9979e-05,\n",
       "                                    -2.6792e-04,  5.6569e-05],\n",
       "                                   [ 1.7767e-02,  2.5200e-03,  4.0976e-03,  ...,  5.9401e-03,\n",
       "                                    -1.9532e-02,  6.7057e-03],\n",
       "                                   [-5.3507e-03, -1.4972e-02, -2.0235e-03,  ..., -3.7772e-03,\n",
       "                                     7.4117e-03,  6.9600e-03]], device='cuda:0')),\n",
       "                          ('model.9.model.2.bias',\n",
       "                           tensor([ 4.1223e-03,  2.3879e-02, -1.1814e-02,  2.2085e-02,  3.3952e-02,\n",
       "                                   -1.0313e-05, -2.0702e-05,  3.0159e-02,  4.2921e-02,  1.6025e-02,\n",
       "                                   -2.8517e-06, -1.2048e-02, -9.3376e-03,  1.1585e-02,  5.6592e-02,\n",
       "                                    1.0573e-02,  1.8789e-02,  1.1956e-02,  1.0952e-02,  1.4599e-02,\n",
       "                                    1.6400e-02, -1.9452e-03,  1.4256e-02, -4.2675e-02,  9.1861e-03,\n",
       "                                   -8.9818e-03, -1.8918e-06, -8.7993e-03, -8.5722e-03,  5.5539e-02,\n",
       "                                    4.5701e-02, -3.4273e-03, -9.2146e-03,  4.9139e-03,  2.2904e-02,\n",
       "                                    2.2466e-02, -1.6588e-05,  8.9609e-03, -3.5288e-04, -7.4524e-03,\n",
       "                                    2.8958e-02,  6.0982e-03,  3.2070e-02, -3.2165e-03,  8.8808e-02,\n",
       "                                   -1.9983e-05,  1.2337e-03,  4.7382e-03,  8.2145e-03,  9.9397e-03,\n",
       "                                    3.4952e-02,  1.3993e-02,  2.6880e-03, -5.4909e-03,  6.0881e-03,\n",
       "                                    3.6642e-02,  2.2526e-02,  4.0304e-02,  1.8410e-02,  2.7733e-02,\n",
       "                                    2.8518e-02,  3.0188e-02,  1.7740e-02,  3.4388e-02,  5.6821e-04,\n",
       "                                    1.1553e-02,  3.1559e-04,  3.7116e-03,  1.0997e-04,  1.1389e-02,\n",
       "                                   -6.9097e-04,  1.7453e-02,  2.0546e-02,  3.3892e-03,  6.1974e-03,\n",
       "                                    4.5936e-03, -1.3879e-02,  2.5367e-03,  1.3685e-02,  2.7657e-02,\n",
       "                                   -2.7767e-03,  2.6406e-02, -2.6185e-02,  3.5701e-02,  1.2533e-02,\n",
       "                                    5.0392e-03,  2.5581e-03, -1.6385e-03,  8.8627e-03, -1.6264e-03,\n",
       "                                    1.8204e-02,  5.6977e-03, -6.6524e-03,  2.9208e-02,  8.8941e-03,\n",
       "                                    2.7724e-04,  7.8639e-03, -2.1257e-03, -1.1067e-03,  5.4292e-02,\n",
       "                                    2.9275e-03,  1.0039e-03, -3.2039e-04,  7.6069e-03, -4.7266e-03,\n",
       "                                    4.1219e-02,  5.9934e-02,  1.9615e-03,  1.4511e-02, -1.0731e-02,\n",
       "                                    9.8571e-03,  1.5329e-05,  2.1471e-02,  3.2709e-03,  1.5143e-02,\n",
       "                                   -2.2513e-03,  5.0010e-02,  2.5757e-03,  1.1756e-02, -4.3801e-03,\n",
       "                                    9.3586e-06,  6.9377e-02,  6.1559e-02, -1.9241e-03,  4.1689e-02,\n",
       "                                    2.6971e-03,  5.3871e-02,  8.1920e-03,  3.5039e-02,  2.5811e-02,\n",
       "                                    3.3933e-03, -6.1353e-04,  1.2693e-02,  5.8543e-05,  4.3883e-03,\n",
       "                                    2.1363e-02,  5.4949e-02,  1.4892e-04,  4.4827e-05, -1.9838e-05,\n",
       "                                    1.6838e-03, -3.9997e-03,  4.3956e-02,  1.5222e-03,  1.6090e-02,\n",
       "                                    4.1425e-03,  2.0043e-02,  1.3634e-02,  2.9936e-02,  2.8318e-03,\n",
       "                                   -7.8467e-04,  4.7726e-03,  4.3706e-04,  1.9156e-02, -1.9110e-02,\n",
       "                                    2.7337e-04,  4.6159e-02,  9.4789e-03,  1.0545e-02,  6.9724e-02,\n",
       "                                    3.0014e-02,  3.2312e-03,  1.3304e-02,  5.8073e-02, -7.6169e-06,\n",
       "                                   -1.2602e-03,  1.6329e-02,  5.4669e-03,  2.1316e-03,  4.3008e-02,\n",
       "                                    2.8729e-02,  6.1989e-03,  1.4235e-02,  9.4375e-03,  1.4363e-02,\n",
       "                                    3.9581e-02,  4.6077e-02,  5.1157e-02,  1.2533e-02,  3.4786e-03,\n",
       "                                    3.5515e-02,  9.8679e-03,  3.8007e-04, -2.1832e-05,  1.9037e-02,\n",
       "                                    4.6214e-03,  6.4939e-03,  3.3235e-02,  1.4539e-02,  1.0431e-02,\n",
       "                                   -5.9643e-04,  1.8219e-02,  1.8119e-02,  2.0163e-02,  5.5186e-03,\n",
       "                                   -4.0373e-03,  5.5476e-03,  8.9732e-03,  3.5594e-02, -1.1956e-05,\n",
       "                                    7.7213e-02,  4.6218e-04,  7.8455e-03,  8.0226e-02,  7.2935e-03,\n",
       "                                    4.2950e-03,  3.4649e-02,  1.8525e-02, -5.9321e-04,  4.1512e-02,\n",
       "                                    1.4249e-02, -8.1005e-07,  1.9897e-03,  2.2200e-02,  7.6836e-04,\n",
       "                                    5.9385e-02,  1.4364e-02,  1.4036e-02,  3.2661e-03,  3.5521e-03,\n",
       "                                    3.3296e-02, -1.0804e-03,  2.3802e-02, -1.9431e-03,  1.1323e-02,\n",
       "                                    7.0455e-03,  2.1703e-02,  5.6658e-03,  2.0381e-02,  4.1243e-02,\n",
       "                                   -1.9745e-03,  1.6227e-02,  1.7794e-02, -5.4047e-03,  2.1694e-02,\n",
       "                                    4.5819e-03,  2.3121e-02,  4.4335e-03,  6.2529e-03, -3.3014e-03,\n",
       "                                    2.1120e-02,  1.2311e-02,  1.9604e-05, -3.0432e-04,  1.0921e-03,\n",
       "                                    3.2679e-03,  1.2797e-02,  3.1756e-02, -1.3891e-05,  1.3254e-02,\n",
       "                                    2.5908e-02,  2.6037e-02, -9.2405e-03,  3.6391e-04,  3.4075e-02,\n",
       "                                    3.1365e-02], device='cuda:0')),\n",
       "                          ('model.9.model.4.weight',\n",
       "                           tensor([[ 5.0716e-03, -2.2981e-02, -1.2298e-02,  ...,  4.9963e-04,\n",
       "                                     4.6129e-02, -2.3122e-02],\n",
       "                                   [-3.4637e-05, -5.2550e-05,  5.0276e-05,  ...,  3.0397e-05,\n",
       "                                     5.1014e-05, -1.9356e-05],\n",
       "                                   [-4.9290e-05,  7.7172e-05,  4.7469e-05,  ..., -4.0537e-05,\n",
       "                                    -6.9424e-05,  8.6736e-05],\n",
       "                                   ...,\n",
       "                                   [ 1.9181e-05, -4.4851e-05,  3.3680e-05,  ...,  5.3249e-05,\n",
       "                                    -4.2311e-05, -3.7087e-05],\n",
       "                                   [-1.2919e-05, -4.6960e-05, -3.9408e-05,  ...,  3.1293e-05,\n",
       "                                     1.6227e-04, -1.1628e-04],\n",
       "                                   [ 4.6965e-04, -2.0696e-03, -1.0903e-03,  ...,  5.3176e-05,\n",
       "                                     4.1248e-03, -2.1417e-03]], device='cuda:0')),\n",
       "                          ('model.9.model.4.bias',\n",
       "                           tensor([ 9.7990e-02, -6.8657e-06,  1.1518e-04,  1.4873e-02,  2.7834e-02,\n",
       "                                   -3.8218e-05,  8.9002e-03,  6.1019e-02,  4.4350e-02,  4.5814e-02,\n",
       "                                   -4.8056e-05, -3.4301e-06,  1.3323e-02,  3.2540e-05,  7.7087e-02,\n",
       "                                    4.1740e-02,  5.3682e-02,  4.3037e-02,  9.9110e-03,  2.0085e-02,\n",
       "                                    2.2964e-02,  1.4288e-01,  7.6499e-03,  4.1216e-02,  1.0382e-01,\n",
       "                                    8.6291e-02,  9.1293e-03,  3.4990e-05, -6.8895e-06,  8.4563e-02,\n",
       "                                    8.5555e-03,  1.6635e-02,  1.0308e-02,  7.9823e-02,  4.7288e-02,\n",
       "                                    9.2873e-02,  6.6510e-02,  1.4326e-02,  2.4930e-02,  9.6799e-04,\n",
       "                                    8.0429e-02,  6.1851e-02,  3.3539e-02,  4.1654e-02,  1.0757e-02,\n",
       "                                   -4.0820e-05,  7.7019e-03,  3.8676e-02,  4.2663e-02,  7.5014e-02,\n",
       "                                    1.7769e-02,  1.3636e-02, -4.1254e-05,  6.7586e-02,  1.5514e-02,\n",
       "                                   -1.3882e-05,  1.0945e-02,  5.1814e-02,  2.2822e-02,  4.0299e-02,\n",
       "                                    1.9432e-03,  4.1132e-02,  7.7502e-04,  7.6162e-02,  8.5964e-03,\n",
       "                                    7.3925e-02,  3.3116e-02, -5.6559e-06,  3.0089e-02,  2.8753e-02,\n",
       "                                    2.9175e-03,  2.1067e-02,  6.5656e-02, -2.3405e-05,  3.1440e-02,\n",
       "                                   -3.7226e-05,  3.5810e-03,  6.6812e-02,  5.5755e-06, -1.9389e-05,\n",
       "                                    5.3380e-02, -5.0484e-05,  9.3321e-02,  6.5743e-02, -3.2088e-05,\n",
       "                                    6.1323e-02,  6.6718e-02,  8.2361e-02,  8.2452e-03,  7.1413e-02,\n",
       "                                    7.0045e-02,  6.3504e-02, -4.4711e-06, -1.8115e-05,  4.5540e-02,\n",
       "                                   -3.8788e-05,  2.3927e-02,  7.0470e-02,  6.7545e-03,  2.5123e-02,\n",
       "                                    1.5234e-01,  1.9139e-02,  2.2375e-02,  4.3408e-02,  7.1237e-03,\n",
       "                                    7.9963e-02,  8.5827e-02,  6.5061e-04,  4.7799e-02,  4.5490e-02,\n",
       "                                    6.1679e-04,  7.1948e-02,  8.7999e-02,  1.5348e-05,  6.0796e-02,\n",
       "                                    1.4250e-02,  4.7799e-02,  5.4556e-03,  1.0063e-01,  3.3033e-02,\n",
       "                                   -1.1400e-05,  2.3443e-02,  6.5100e-02,  1.9780e-03,  8.5994e-02,\n",
       "                                   -3.4669e-05,  2.4716e-04,  8.8881e-03], device='cuda:0')),\n",
       "                          ('model.9.model.6.weight',\n",
       "                           tensor([[-3.3805e-01, -2.2373e-05,  5.2373e-04, -7.6026e-02,  1.3059e-01,\n",
       "                                    -5.7232e-05,  4.1725e-02, -2.7490e-01, -1.7451e-01, -1.5836e-01,\n",
       "                                     3.9837e-05, -5.2433e-05,  6.2597e-02, -3.5343e-05, -2.6656e-01,\n",
       "                                    -1.4409e-01,  2.5192e-01,  2.0192e-01, -5.0058e-02,  9.4191e-02,\n",
       "                                    -7.8938e-02, -4.9203e-01, -3.9031e-02,  1.9352e-01, -3.5797e-01,\n",
       "                                    -2.9761e-01,  4.2698e-02, -8.5236e-06,  2.2506e-05,  3.9645e-01,\n",
       "                                    -4.3971e-02,  7.8077e-02, -5.2786e-02,  3.7477e-01, -1.6233e-01,\n",
       "                                     4.3585e-01, -3.2253e-01, -4.9073e-02,  1.1706e-01,  4.4616e-03,\n",
       "                                    -3.3982e-01, -2.6130e-01, -1.1593e-01, -2.1308e-01, -3.6797e-02,\n",
       "                                    -2.7809e-05,  3.6088e-02,  1.8146e-01,  2.0029e-01, -2.5876e-01,\n",
       "                                    -6.1038e-02,  6.4111e-02,  2.8420e-05, -2.3196e-01,  7.2914e-02,\n",
       "                                     1.3930e-05, -3.7534e-02,  2.4318e-01,  1.0699e-01,  1.8916e-01,\n",
       "                                     9.0518e-03, -1.8570e-01,  3.4009e-03,  3.5780e-01,  4.0553e-02,\n",
       "                                     3.4666e-01,  1.5508e-01, -4.5432e-05, -1.0339e-01,  1.3505e-01,\n",
       "                                     1.3563e-02, -1.0779e-01,  3.0861e-01, -4.1547e-06,  1.4759e-01,\n",
       "                                     3.5009e-05,  1.6767e-02,  3.1400e-01, -4.2723e-05, -6.3141e-05,\n",
       "                                     2.5034e-01, -4.6505e-05,  4.3766e-01,  3.0836e-01,  1.1154e-06,\n",
       "                                     2.8749e-01, -2.3065e-01,  3.8648e-01,  3.8688e-02, -2.5106e-01,\n",
       "                                    -2.4029e-01, -2.2320e-01, -6.0473e-05,  1.4265e-05,  2.1359e-01,\n",
       "                                     2.7289e-05,  1.1219e-01,  3.3084e-01, -2.3115e-02, -1.2717e-01,\n",
       "                                    -5.9937e-01,  8.9780e-02, -7.6729e-02, -2.2096e-01,  3.3320e-02,\n",
       "                                    -3.6048e-01, -3.3770e-01,  3.0823e-03, -2.4595e-01,  2.1356e-01,\n",
       "                                     2.9126e-03,  3.3741e-01,  4.1346e-01,  2.6215e-05,  2.8516e-01,\n",
       "                                    -7.3498e-02,  2.2430e-01,  2.5692e-02, -3.9596e-01,  1.5508e-01,\n",
       "                                     4.4761e-05, -8.0989e-02,  3.0597e-01,  9.2860e-03,  4.0417e-01,\n",
       "                                     1.2921e-05, -1.1079e-03, -3.0536e-02]], device='cuda:0')),\n",
       "                          ('model.9.model.6.bias',\n",
       "                           tensor([-0.0878], device='cuda:0'))]),\n",
       "             'epoch': 6,\n",
       "             'acc': tensor(70.2406, device='cuda:0')})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_training(eval_dataset_info, validate_dataset_info, model, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb599919-75b2-4ee8-9cd1-46b7cd414755",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense_768_768_256_128_1\n",
      "EPOCH 1\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5788663625717163  0.6263464093208313\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   68.66%              64.30%\n",
      "EPOCH 2\n",
      "           Train               Test\n",
      "---------  ------------------  -----------------\n",
      "Loss       0.5405199527740479  0.617194414138794\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   71.81%              65.51%\n",
      "EPOCH 3\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5102490186691284  0.6108804941177368\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.01%              66.55%\n",
      "EPOCH 4\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.49315857887268066  0.6155732870101929\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   75.39%               66.18%\n",
      "EPOCH 5\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4735076129436493  0.6127709746360779\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   76.72%              67.12%\n",
      "EPOCH 6\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.461326003074646  0.6236198544502258\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   77.48%             67.34%\n",
      "EPOCH 7\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.45040518045425415  0.6282151341438293\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.21%               66.98%\n",
      "EPOCH 8\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4416705369949341  0.6433152556419373\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   78.77%              66.41%\n",
      "EPOCH 9\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4276305139064789  0.6500852108001709\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.68%              66.80%\n",
      "EPOCH 10\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.41644152998924255  0.6543137431144714\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.40%               67.14%\n",
      "EPOCH 11\n",
      "           Train                Test\n",
      "---------  -------------------  -----------------\n",
      "Loss       0.41356906294822693  0.663985013961792\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.51%               67.09%\n",
      "EPOCH 12\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4073076844215393  0.6695910692214966\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.91%              67.06%\n",
      "EPOCH 13\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.39545944333076477  0.6760374307632446\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.67%               66.77%\n",
      "EPOCH 14\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.390633225440979  0.6761347651481628\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   81.91%             67.30%\n",
      "EPOCH 15\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.38121575117111206  0.6969116926193237\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   82.49%               66.45%\n",
      "EPOCH 16\n",
      "           Train               Test\n",
      "---------  ------------------  ----------------\n",
      "Loss       0.3768199682235718  0.70350581407547\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   82.71%              66.69%\n",
      "EPOCH 17\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.37042874097824097  0.7044873833656311\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   83.08%               66.88%\n",
      "EPOCH 18\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.36669233441352844  0.7095281481742859\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   83.28%               66.85%\n",
      "EPOCH 19\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3634420931339264  0.7341362237930298\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   83.44%              66.10%\n",
      "EPOCH 20\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.357252299785614  0.7344824075698853\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   83.84%             66.14%\n",
      "EPOCH 21\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.35313108563423157  0.7432737350463867\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   84.01%               66.41%\n",
      "EPOCH 22\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.35188111662864685  0.7521575093269348\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   84.05%               66.51%\n",
      "EPOCH 23\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.34477534890174866  0.7562469244003296\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   84.50%               66.04%\n",
      "EPOCH 24\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.342040091753006  0.7683669924736023\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   84.61%             66.05%\n",
      "EPOCH 25\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.33731019496917725  0.7671332955360413\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   84.86%               66.55%\n",
      "EPOCH 26\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3361314535140991  0.7848289608955383\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   84.83%              66.28%\n",
      "EPOCH 27\n",
      "           Train                Test\n",
      "---------  -------------------  -----------------\n",
      "Loss       0.33403849601745605  0.780156672000885\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   84.98%               66.64%\n",
      "EPOCH 28\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.32759034633636475  0.7929931282997131\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   85.35%               66.26%\n",
      "EPOCH 29\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3246247172355652  0.8038759231567383\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   85.50%              66.23%\n",
      "EPOCH 30\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.32274287939071655  0.8227987885475159\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   85.58%               65.81%\n",
      "EPOCH 31\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.31859657168388367  0.8224717974662781\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   85.82%               65.93%\n",
      "EPOCH 32\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.32520338892936707  0.8302059173583984\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   85.29%               66.39%\n",
      "EPOCH 33\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.323373943567276  0.8362675309181213\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   85.42%             66.39%\n",
      "EPOCH 34\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.31305187940597534  0.8346499800682068\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   86.10%               65.67%\n",
      "EPOCH 35\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.31329628825187683  0.8334548473358154\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   86.04%               66.26%\n",
      "EPOCH 36\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.30802521109580994  0.8507573008537292\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   86.34%               65.78%\n",
      "EPOCH 37\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3069287836551666  0.8536186218261719\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   86.42%              65.63%\n",
      "EPOCH 38\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.30431586503982544  0.8652238845825195\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   86.52%               65.81%\n",
      "EPOCH 39\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3027707040309906  0.8772634267807007\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   86.57%              66.16%\n",
      "EPOCH 40\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.30033808946609497  0.8840187191963196\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   86.69%               65.78%\n",
      "EPOCH 41\n",
      "           Train               Test\n",
      "---------  ------------------  -----------------\n",
      "Loss       0.3001497983932495  0.885642409324646\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   86.67%              66.14%\n",
      "EPOCH 42\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.29667404294013977  0.9110878109931946\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   86.85%               65.93%\n",
      "EPOCH 43\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.29595011472702026  0.8954495787620544\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   86.91%               65.67%\n",
      "EPOCH 44\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.2932584285736084  0.9160041809082031\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   87.02%              65.76%\n",
      "EPOCH 45\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.29089030623435974  0.9082660675048828\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   87.18%               65.62%\n",
      "EPOCH 46\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.2900296747684479  0.9108957648277283\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   87.23%              65.50%\n",
      "EPOCH 47\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3041488230228424  0.9355602860450745\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   86.28%              66.34%\n",
      "EPOCH 48\n",
      "           Train                Test\n",
      "---------  -------------------  -----------------\n",
      "Loss       0.28897663950920105  0.921416163444519\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   87.24%               65.58%\n",
      "EPOCH 49\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.2876136600971222  0.9540236592292786\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   87.30%              65.12%\n",
      "EPOCH 50\n",
      "           Train              Test\n",
      "---------  -----------------  -----------------\n",
      "Loss       0.287295401096344  0.972966730594635\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   87.30%             64.99%\n",
      "EPOCH 51\n",
      "Took: 3414.01s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'vloss': tensor(0.6109, device='cuda:0'),\n",
       "             'model': OrderedDict([('model.0.weight',\n",
       "                           tensor([[-0.0237, -0.0210, -0.0029,  ...,  0.0209, -0.0342,  0.0159],\n",
       "                                   [-0.0137, -0.0030,  0.0268,  ..., -0.0234, -0.0301, -0.0103],\n",
       "                                   [-0.0097, -0.0236,  0.0174,  ..., -0.0175, -0.0432, -0.0219],\n",
       "                                   ...,\n",
       "                                   [-0.0011,  0.0006, -0.0051,  ..., -0.0499, -0.0113,  0.0017],\n",
       "                                   [ 0.0109,  0.0178,  0.0126,  ...,  0.0217,  0.0074,  0.0221],\n",
       "                                   [ 0.0277,  0.0275,  0.0270,  ...,  0.0150, -0.0142,  0.0136]],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.0.bias',\n",
       "                           tensor([-4.3279e-02, -8.5282e-02,  3.5466e-02, -4.2082e-03, -7.4021e-02,\n",
       "                                   -4.3472e-02, -3.5590e-02, -1.9669e-01,  8.6554e-03, -6.4964e-02,\n",
       "                                   -3.7396e-02,  8.8816e-03, -1.9548e-02,  1.0186e-02, -2.9374e-02,\n",
       "                                    2.7909e-02,  7.0543e-05, -5.9580e-02,  4.1246e-02, -6.2493e-02,\n",
       "                                   -3.4399e-02, -5.2326e-02, -2.9703e-02, -5.9374e-02, -1.5600e-02,\n",
       "                                   -1.6722e-02, -1.5220e-02, -2.5459e-03, -1.2595e-02, -5.6138e-02,\n",
       "                                   -5.4418e-02, -2.4973e-02,  1.4292e-02, -3.3939e-02, -5.0662e-02,\n",
       "                                   -3.7797e-02, -4.1535e-02,  7.8368e-02, -1.0989e-01, -6.0301e-02,\n",
       "                                   -3.2397e-02, -5.6014e-02,  5.9376e-03, -2.7842e-02, -2.2033e-02,\n",
       "                                   -3.7695e-02, -3.9998e-02, -5.9910e-02, -3.6189e-02, -4.8045e-02,\n",
       "                                   -4.5060e-02, -5.7728e-02, -3.3428e-02,  3.4073e-02, -1.5060e-02,\n",
       "                                   -2.0580e-02, -5.9663e-02, -3.8168e-03,  4.2203e-03, -6.0708e-02,\n",
       "                                   -2.1557e-02,  7.7994e-03,  4.7843e-03, -5.5699e-02,  2.8243e-02,\n",
       "                                   -1.9196e-02, -9.6188e-02, -2.9004e-02, -1.9751e-03, -2.6088e-02,\n",
       "                                   -5.7438e-02, -7.5998e-02, -9.9487e-04,  1.1685e-03, -7.4897e-02,\n",
       "                                   -1.9266e-03, -8.7134e-02,  2.5656e-02, -1.0732e-01, -4.1497e-02,\n",
       "                                   -3.0454e-02, -6.3570e-02,  4.9357e-02, -1.0261e-01, -4.4211e-02,\n",
       "                                    6.9464e-02, -8.4528e-02,  1.1144e-02, -2.3410e-02, -3.2066e-02,\n",
       "                                   -4.4538e-02, -2.4083e-02, -5.2260e-02, -3.5903e-03, -5.4505e-02,\n",
       "                                   -3.9175e-02,  6.9094e-03, -3.7145e-02, -5.0168e-02, -1.1715e-02,\n",
       "                                    5.4225e-03, -2.9718e-02, -1.2016e-02,  1.4167e-02, -2.1429e-02,\n",
       "                                   -5.3835e-02, -3.4523e-03, -6.4650e-02, -3.0444e-02, -3.7376e-02,\n",
       "                                    5.7882e-03, -5.6677e-02, -5.8831e-02, -1.3376e-03,  6.9824e-03,\n",
       "                                    6.0573e-03, -1.2997e-02, -6.7755e-02,  1.7389e-02, -3.1585e-02,\n",
       "                                   -1.5035e-02, -4.4267e-04,  3.5183e-04, -7.9796e-02, -6.8546e-02,\n",
       "                                   -5.2414e-03, -6.8470e-02,  3.0473e-03, -3.0058e-02,  2.5502e-02,\n",
       "                                   -1.9488e-02, -7.0178e-02, -3.1523e-02, -7.3424e-02, -6.5494e-02,\n",
       "                                   -2.3381e-03, -3.6259e-02, -5.2229e-02,  3.2824e-02, -6.9958e-03,\n",
       "                                   -7.8712e-02, -3.7389e-02, -5.3130e-02, -4.4985e-02, -7.4855e-02,\n",
       "                                   -4.3867e-02,  2.4434e-02, -6.7145e-04, -2.4964e-02,  5.9364e-03,\n",
       "                                   -2.3012e-02,  5.2389e-02, -3.8384e-02, -4.3662e-02, -4.0527e-02,\n",
       "                                   -3.9038e-02, -8.7636e-02, -3.2350e-02, -4.2855e-02, -1.8023e-02,\n",
       "                                   -3.6757e-02,  1.5068e-01, -5.5419e-03, -6.9670e-02, -3.5120e-02,\n",
       "                                   -1.0945e-02, -1.7429e-02, -4.6114e-02, -4.4786e-02,  1.5578e-02,\n",
       "                                   -2.5133e-02, -5.8633e-02,  4.8911e-03, -2.0416e-02, -4.0004e-02,\n",
       "                                   -4.5246e-02, -2.7046e-02, -1.4504e-01, -9.4560e-02, -4.4056e-02,\n",
       "                                   -8.1619e-02, -1.8509e-02, -2.3792e-02, -2.2885e-02,  7.5933e-03,\n",
       "                                   -2.3915e-02, -1.6150e-02,  7.4561e-02, -3.2218e-02, -3.6525e-02,\n",
       "                                   -4.0327e-02, -4.7873e-02,  2.9751e-02, -4.4605e-02, -1.1316e-02,\n",
       "                                   -4.4780e-02, -1.4613e-02, -7.1311e-02,  3.8155e-03, -7.6457e-02,\n",
       "                                   -6.4624e-02, -3.7806e-02, -2.1974e-03, -9.5210e-02,  6.7698e-03,\n",
       "                                   -6.1474e-02, -4.0924e-02, -6.1482e-02,  3.6687e-03, -2.9271e-02,\n",
       "                                    6.2800e-04, -7.3727e-02, -2.9210e-02,  2.8998e-03, -2.5549e-02,\n",
       "                                   -1.3387e-02, -1.0190e-01, -2.6483e-02, -4.0655e-03, -4.4213e-02,\n",
       "                                   -5.7037e-02,  2.1097e-03, -7.9455e-02, -4.6012e-02, -3.7124e-02,\n",
       "                                   -7.1816e-03, -4.3984e-02,  1.7480e-02, -1.5765e-02, -8.0734e-02,\n",
       "                                   -6.5206e-02, -2.1702e-02, -3.4561e-02,  2.0010e-03, -1.1624e-02,\n",
       "                                   -6.9643e-03, -7.1262e-02, -1.7566e-01, -7.9442e-02, -6.0295e-02,\n",
       "                                    6.2317e-03, -1.9331e-02, -5.7338e-02, -5.3569e-02, -2.6812e-02,\n",
       "                                   -1.3795e-02, -3.9776e-02, -7.6708e-02, -1.4854e-02,  1.9837e-02,\n",
       "                                   -9.9565e-03,  1.7747e-02, -4.1759e-02, -3.0437e-02, -4.8792e-02,\n",
       "                                    2.3798e-03, -7.0102e-03, -4.7885e-02, -1.0297e-02, -4.4440e-02,\n",
       "                                    5.4066e-03, -6.6148e-02, -1.7190e-02, -1.7739e-01,  1.5073e-02,\n",
       "                                   -3.8475e-03, -7.8428e-02, -2.3751e-02, -4.9377e-02, -3.3844e-02,\n",
       "                                    2.2907e-03, -6.1034e-02, -6.2351e-02, -2.0430e-02, -3.2120e-02,\n",
       "                                   -1.6956e-02, -3.9130e-02, -1.2073e-02, -3.7039e-02, -6.2190e-02,\n",
       "                                   -2.3199e-04, -5.7165e-02, -2.6862e-02, -2.8134e-02,  1.3473e-02,\n",
       "                                   -5.2900e-02,  3.0448e-02, -3.8534e-02, -3.2093e-02,  1.7206e-03,\n",
       "                                   -3.5256e-02, -5.3893e-02, -5.2613e-02, -4.9627e-02,  1.4565e-01,\n",
       "                                   -5.7241e-02, -5.7980e-02, -1.3523e-02, -5.3527e-02, -2.5244e-02,\n",
       "                                    8.3660e-03, -1.0325e-01, -8.5860e-02, -9.5408e-02, -7.4683e-05,\n",
       "                                   -8.6978e-02, -5.6076e-02, -7.1167e-02, -4.6301e-02, -6.1296e-02,\n",
       "                                   -3.6861e-02, -1.0980e-01, -9.9731e-02, -6.1006e-02, -8.1230e-03,\n",
       "                                   -1.3898e-02, -8.9367e-02,  9.2174e-03,  1.5677e-02,  4.6750e-03,\n",
       "                                   -5.3594e-02,  5.0059e-02, -2.7616e-02, -2.7307e-02, -3.4615e-02,\n",
       "                                   -6.3912e-02, -8.2892e-02, -3.6657e-02,  2.8789e-02,  2.2661e-02,\n",
       "                                   -3.0505e-02, -7.5142e-02, -2.7527e-02, -1.4520e-02,  2.0988e-02,\n",
       "                                   -1.4902e-02, -8.5171e-03,  3.1390e-02, -3.0100e-02, -5.0864e-02,\n",
       "                                   -4.9755e-02, -3.4493e-02, -5.1796e-02, -4.4337e-02, -3.6287e-02,\n",
       "                                   -6.7712e-02, -2.2380e-02, -7.9691e-02, -4.1827e-02, -1.3103e-01,\n",
       "                                   -6.9061e-02, -3.7263e-02, -4.6374e-02,  3.1168e-02, -7.0532e-02,\n",
       "                                   -6.0153e-02, -3.9353e-02,  7.6548e-02, -3.8056e-02, -5.3394e-03,\n",
       "                                   -3.5491e-02,  5.0996e-02, -5.5085e-02, -2.8829e-02,  2.1022e-02,\n",
       "                                   -9.4295e-02, -1.7663e-01, -1.6668e-01, -1.3159e-01, -6.0503e-03,\n",
       "                                    1.3729e-04, -1.3169e-02, -2.3683e-02, -5.2024e-02, -7.9345e-02,\n",
       "                                   -1.9442e-02, -3.5741e-02, -5.1592e-02, -1.7247e-02, -5.1368e-02,\n",
       "                                   -7.8400e-02, -2.0558e-02, -5.3146e-02, -2.2674e-02, -1.5449e-02,\n",
       "                                   -3.3662e-03, -2.4916e-02, -1.0286e-02, -6.7587e-02, -1.9420e-02,\n",
       "                                   -4.9122e-02, -4.3056e-02, -5.4163e-02,  6.2874e-03, -2.8789e-02,\n",
       "                                   -2.9890e-02, -4.0474e-02, -7.8464e-02, -7.7338e-02,  1.8309e-02,\n",
       "                                   -7.3942e-03,  6.8585e-02, -3.4015e-02,  1.0016e-02,  1.5994e-02,\n",
       "                                   -7.5635e-02,  3.0835e-03, -2.6350e-02, -3.7173e-02, -3.6769e-02,\n",
       "                                   -2.1497e-02, -3.8764e-02, -8.8277e-02, -4.7764e-02, -4.6296e-02,\n",
       "                                    2.1857e-02, -2.5683e-02,  8.5277e-02, -3.7953e-03, -4.5705e-02,\n",
       "                                   -1.1420e-01, -9.7221e-03, -5.3461e-02, -5.2574e-02, -1.2586e-02,\n",
       "                                   -4.2591e-02, -8.5195e-02,  4.0697e-02, -4.6567e-02, -3.0996e-03,\n",
       "                                   -4.4774e-02, -9.7034e-02,  4.8904e-02, -1.0876e-01, -4.2343e-02,\n",
       "                                   -7.9250e-02, -1.3737e-02, -6.6036e-02, -3.6169e-02, -1.3333e-02,\n",
       "                                   -2.2106e-02, -3.0763e-02, -5.8244e-03, -7.9487e-04, -2.0041e-03,\n",
       "                                   -4.6519e-02,  2.4597e-02,  2.4662e-01, -3.4133e-02, -3.1286e-02,\n",
       "                                   -8.9060e-02, -1.5770e-02, -1.1375e-02, -5.9103e-02, -2.9294e-02,\n",
       "                                   -3.2873e-02, -5.1427e-02, -4.3299e-02, -2.6674e-02, -8.1582e-02,\n",
       "                                    3.3706e-03, -6.1524e-02, -5.6595e-02, -1.9989e-02, -5.5162e-02,\n",
       "                                   -2.7659e-02,  1.2773e-02, -3.7645e-02,  5.3163e-02,  1.2027e-02,\n",
       "                                   -3.0209e-02, -3.7956e-02, -2.5209e-02, -2.6487e-02,  2.1781e-03,\n",
       "                                   -2.8304e-02, -5.3037e-02,  9.1902e-03,  6.0500e-03,  1.5207e-01,\n",
       "                                   -1.1457e-01, -5.0490e-03, -7.9427e-02, -2.9071e-02, -1.5184e-02,\n",
       "                                   -2.3042e-02, -3.7282e-02, -4.5680e-02, -3.4500e-02, -3.3234e-02,\n",
       "                                   -3.1092e-02, -7.9184e-02,  2.7645e-04, -8.1972e-02,  1.1333e-03,\n",
       "                                    4.4126e-03, -8.5279e-02, -2.2475e-02,  4.4970e-03, -1.9636e-02,\n",
       "                                   -4.0157e-02,  1.6460e-02, -6.9054e-02, -3.3733e-02,  3.4276e-02,\n",
       "                                   -2.8729e-02, -7.4854e-02, -3.1655e-02, -3.0127e-02, -5.2556e-02,\n",
       "                                    1.0486e-01,  3.9761e-03, -3.4525e-02, -7.0411e-03, -1.1577e-02,\n",
       "                                   -6.4622e-02, -2.5095e-02, -2.5180e-02, -4.3422e-02, -3.2586e-02,\n",
       "                                   -2.3519e-02, -1.8135e-02, -3.3397e-02, -1.2264e-01, -4.4750e-02,\n",
       "                                   -9.3197e-02,  1.8579e-02, -4.0036e-02, -1.7827e-02,  1.4818e-01,\n",
       "                                   -1.9954e-02, -2.6559e-02, -4.7527e-02, -8.2726e-02, -2.8529e-02,\n",
       "                                    1.1516e-02, -1.8277e-02, -5.8558e-02, -3.6755e-02, -4.6490e-02,\n",
       "                                   -7.2089e-03, -2.8922e-02, -2.5850e-02, -9.9073e-03, -3.9008e-02,\n",
       "                                   -8.5461e-03, -2.7944e-02, -4.7954e-02, -5.1860e-03, -2.7353e-06,\n",
       "                                   -1.4009e-02, -6.0066e-02, -5.5351e-02, -1.4554e-01,  2.2811e-03,\n",
       "                                   -2.5964e-02, -6.8970e-03,  9.7778e-03, -6.9440e-02, -1.9207e-02,\n",
       "                                   -4.2430e-02,  3.1745e-02, -4.9799e-03, -3.5239e-02, -3.7016e-03,\n",
       "                                   -1.6970e-03, -5.9687e-02, -5.3516e-02,  7.3551e-02, -6.3819e-02,\n",
       "                                   -4.0406e-02, -1.2738e-02, -1.3075e-01, -2.9349e-02, -6.0243e-02,\n",
       "                                    1.5380e-01, -5.6152e-02, -6.9895e-02, -7.7495e-03, -4.3514e-02,\n",
       "                                   -2.2307e-02, -4.6141e-02, -7.3301e-02, -8.0806e-02, -4.5293e-02,\n",
       "                                   -3.1452e-02, -2.9302e-02, -3.6107e-02, -4.9877e-02, -4.5328e-02,\n",
       "                                   -4.2858e-02, -2.4614e-02, -6.6743e-02, -2.2093e-02, -4.0118e-02,\n",
       "                                    3.9879e-03, -2.2306e-02, -4.6370e-02, -2.9982e-02, -2.0645e-02,\n",
       "                                   -5.5320e-02, -7.9404e-03, -5.3978e-02, -1.5786e-02, -5.8349e-03,\n",
       "                                   -7.3456e-03,  1.5030e-02, -2.6067e-02, -2.7169e-02, -4.6721e-02,\n",
       "                                   -1.9914e-02, -1.4383e-02,  1.8921e-02, -4.8804e-02, -4.8200e-02,\n",
       "                                   -1.4751e-02, -3.0924e-02, -3.9156e-02, -6.2685e-02, -1.1924e-03,\n",
       "                                   -4.2209e-02,  1.7106e-03, -2.2142e-02, -5.4734e-02, -4.3947e-02,\n",
       "                                   -5.5861e-02, -5.6750e-02, -4.6408e-02, -7.6290e-02,  2.8924e-02,\n",
       "                                    7.3235e-02, -4.1879e-02, -4.7864e-02, -2.5252e-03, -8.0851e-02,\n",
       "                                   -7.9174e-02, -3.4516e-02, -1.9980e-03, -7.1277e-03, -5.6724e-02,\n",
       "                                   -4.7037e-02, -5.5035e-02, -3.1789e-02, -3.8684e-02,  1.8014e-02,\n",
       "                                    2.8063e-02, -9.1545e-03,  1.2898e-03,  6.1349e-03, -1.8172e-01,\n",
       "                                   -2.3198e-02, -2.3355e-02, -4.0383e-03, -2.0940e-01,  1.3384e-02,\n",
       "                                    3.8795e-02,  2.4893e-02, -2.4318e-02,  4.5779e-03, -5.1785e-02,\n",
       "                                   -1.5992e-01, -1.1192e-02, -2.1518e-02, -5.5214e-02, -2.6796e-02,\n",
       "                                   -1.5323e-03, -2.5360e-02, -4.4706e-02,  1.7938e-02, -4.7716e-02,\n",
       "                                   -4.8099e-02,  1.8772e-02, -1.4202e-02, -3.4391e-02, -3.4452e-02,\n",
       "                                   -1.9078e-01, -4.9299e-02, -4.4637e-02, -1.7607e-02, -3.1931e-02,\n",
       "                                   -3.3533e-02, -1.0438e-01, -2.6106e-02, -7.3458e-02, -3.3452e-02,\n",
       "                                    1.1073e-02, -7.1780e-02, -5.2100e-02, -3.1407e-02,  3.1531e-04,\n",
       "                                   -5.8176e-02,  2.9695e-02, -1.9811e-02, -5.6663e-02, -4.8974e-02,\n",
       "                                    1.2003e-01, -1.2458e-03, -8.9644e-02, -2.6124e-02, -3.3992e-03,\n",
       "                                   -2.4568e-02,  3.1668e-02, -4.2630e-03, -5.2580e-02, -6.2756e-02,\n",
       "                                    6.7351e-03,  4.9515e-02, -1.1278e-01, -1.0574e-02,  4.8816e-02,\n",
       "                                    1.0805e-02, -2.6963e-02, -7.9464e-02, -2.0541e-02, -6.9304e-02,\n",
       "                                   -4.9219e-02, -2.1901e-03,  3.7166e-03, -6.3721e-02, -5.0404e-02,\n",
       "                                   -5.0228e-02, -6.1256e-02, -1.5670e-02, -7.1689e-02,  1.5231e-02,\n",
       "                                   -1.0019e-02,  3.8759e-03, -1.7795e-02, -6.3080e-03, -2.0461e-01,\n",
       "                                   -1.4573e-02,  7.4704e-03, -8.3038e-02, -1.1448e-02, -5.4315e-02,\n",
       "                                   -1.2483e-02, -6.8667e-03, -4.5658e-02, -4.2205e-02, -1.6251e-02,\n",
       "                                   -1.6634e-02, -1.7750e-02, -9.2412e-04, -2.0236e-02, -2.5325e-02,\n",
       "                                   -1.8649e-02, -8.7318e-02, -2.1340e-02,  4.2100e-02, -3.8324e-02,\n",
       "                                   -2.3804e-02, -3.0146e-02, -8.9342e-02, -2.7985e-02, -2.3545e-02,\n",
       "                                   -2.9817e-02, -9.7086e-03,  1.9649e-01, -2.8050e-02,  6.9679e-03,\n",
       "                                   -5.3035e-02,  3.5179e-03, -6.5411e-02, -2.5240e-02, -1.4561e-02,\n",
       "                                   -2.0016e-02,  3.9756e-02, -4.1327e-02], device='cuda:0')),\n",
       "                          ('model.2.weight',\n",
       "                           tensor([[-0.0177,  0.0084,  0.0174,  ...,  0.0025, -0.0208,  0.0172],\n",
       "                                   [-0.0344,  0.0265, -0.0145,  ..., -0.0158, -0.0120, -0.0229],\n",
       "                                   [-0.0304, -0.0465,  0.0090,  ...,  0.0063, -0.0393, -0.0113],\n",
       "                                   ...,\n",
       "                                   [ 0.0303,  0.0129, -0.0226,  ...,  0.0484,  0.0102,  0.0233],\n",
       "                                   [ 0.0254,  0.0516,  0.0186,  ..., -0.0283,  0.0947, -0.0510],\n",
       "                                   [ 0.0454,  0.0279,  0.0015,  ..., -0.0549,  0.0203, -0.0312]],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.2.bias',\n",
       "                           tensor([ 3.2507e-03,  3.5252e-02, -1.1548e-02, -2.2395e-02, -3.0366e-02,\n",
       "                                    5.5298e-03, -1.8093e-02, -1.0592e-02, -1.0285e-02,  3.5574e-02,\n",
       "                                   -2.9163e-02,  1.1266e-03, -1.0586e-04, -1.7096e-02, -2.3158e-03,\n",
       "                                    2.5306e-02, -5.5018e-03, -3.0695e-02,  6.6322e-02,  2.5923e-02,\n",
       "                                   -6.0499e-02,  2.5315e-03,  2.2977e-02, -1.2500e-03,  1.6300e-03,\n",
       "                                    1.0879e-02, -1.8294e-02,  1.2917e-02, -1.8909e-02,  2.3460e-02,\n",
       "                                   -3.7189e-02, -1.3460e-03,  1.7115e-02, -6.4116e-02, -2.0452e-02,\n",
       "                                    1.3712e-02, -4.2677e-02, -2.0507e-03,  8.8071e-03,  9.7051e-02,\n",
       "                                   -1.1022e-02,  6.6189e-02, -1.8578e-02,  8.0429e-02,  5.8410e-02,\n",
       "                                    6.4763e-02,  9.5436e-02,  5.2840e-03,  2.8412e-02,  1.5863e-02,\n",
       "                                   -3.1357e-02, -1.4302e-03,  6.4464e-02, -4.0858e-03,  5.8656e-03,\n",
       "                                   -4.8194e-02,  1.2199e-03,  2.3532e-02, -1.6498e-02,  2.1818e-03,\n",
       "                                   -3.9395e-02,  5.3776e-02, -2.8324e-02, -2.6780e-03, -2.2476e-02,\n",
       "                                    1.8270e-02,  4.8189e-02,  4.2118e-02,  1.9328e-02,  5.2997e-02,\n",
       "                                   -2.1691e-02, -2.8106e-02,  6.3664e-03,  1.8593e-02, -1.5490e-02,\n",
       "                                   -6.9736e-03, -2.8025e-02,  9.5122e-03, -1.3646e-02, -2.0546e-02,\n",
       "                                    4.7680e-04, -4.3745e-02, -3.8774e-02,  9.2876e-03, -1.9015e-02,\n",
       "                                   -1.5612e-02,  2.1955e-03,  4.5634e-04,  5.7167e-02,  6.0177e-02,\n",
       "                                   -3.1713e-02,  5.9562e-02,  3.1073e-02,  7.4668e-02, -1.5619e-02,\n",
       "                                   -1.3903e-02,  8.8979e-02, -1.8412e-03, -6.8853e-02, -1.3840e-02,\n",
       "                                    1.5816e-03, -3.5299e-02, -1.0172e-02, -1.6392e-03,  1.1777e-02,\n",
       "                                    2.2981e-02, -4.0098e-02, -7.1417e-02,  4.0159e-02, -3.9417e-02,\n",
       "                                   -1.8687e-02,  4.1270e-02, -1.3058e-02, -1.1004e-02, -3.1396e-02,\n",
       "                                   -4.8220e-03, -2.6476e-02, -1.2734e-02,  2.4696e-03, -1.0484e-02,\n",
       "                                   -2.2136e-02, -1.3522e-02,  3.8082e-02,  1.9968e-02,  3.6599e-02,\n",
       "                                   -2.2395e-02,  3.6615e-02,  2.3139e-02,  6.9423e-02, -1.5663e-03,\n",
       "                                   -2.9995e-02,  1.6979e-02,  8.0891e-02, -1.3230e-02,  6.7971e-03,\n",
       "                                   -3.6192e-02,  9.4715e-03,  1.3812e-02,  6.8156e-02,  6.4835e-02,\n",
       "                                    3.0834e-02,  7.6383e-03,  2.2750e-02, -2.5795e-03, -2.3816e-02,\n",
       "                                   -1.9062e-02,  2.3476e-02, -1.2243e-02,  1.7287e-02, -2.1401e-02,\n",
       "                                   -2.2751e-02,  4.1126e-02,  4.8016e-03,  5.7320e-02, -7.8939e-02,\n",
       "                                   -3.9828e-02,  5.0569e-02, -1.8758e-02,  1.0561e-02,  1.8892e-02,\n",
       "                                   -2.0777e-02, -2.0358e-02,  1.1994e-01, -1.6705e-02,  3.3360e-02,\n",
       "                                    1.1366e-02,  4.4055e-03,  3.6916e-03,  1.3765e-02, -4.5319e-03,\n",
       "                                    7.1203e-02, -2.2334e-02, -3.0488e-04,  2.5194e-02, -4.1931e-03,\n",
       "                                    1.9597e-02, -8.3815e-03,  2.0278e-02, -8.2634e-04, -2.9212e-02,\n",
       "                                   -1.5648e-02, -1.2630e-02, -3.4141e-02,  1.2095e-02, -7.3536e-03,\n",
       "                                    6.8742e-03,  3.1960e-03, -9.6057e-03,  1.2797e-02, -5.0794e-03,\n",
       "                                   -1.8010e-02,  2.6833e-03, -2.7376e-02,  1.7662e-02,  8.7907e-03,\n",
       "                                   -4.9762e-03, -1.7449e-02,  5.8273e-03, -1.4733e-03,  1.6066e-02,\n",
       "                                    3.9757e-03, -1.1104e-02,  6.2857e-02, -1.6320e-02,  1.8244e-02,\n",
       "                                    5.8484e-02, -1.6516e-02, -3.1901e-02, -3.5703e-02,  5.6955e-02,\n",
       "                                    5.7185e-02,  1.0257e-01,  6.6746e-02, -1.4432e-02,  5.8841e-03,\n",
       "                                   -1.0850e-02,  1.3637e-01, -2.1114e-02, -5.5002e-03, -2.2513e-02,\n",
       "                                    2.6544e-02,  5.7444e-02, -1.2921e-02,  2.4293e-02, -2.4884e-02,\n",
       "                                   -6.5204e-02, -2.2835e-02,  1.5262e-02, -4.6913e-02, -6.2045e-03,\n",
       "                                    2.7129e-02,  3.3235e-02,  3.3478e-02, -8.4889e-03, -2.8429e-02,\n",
       "                                    3.4577e-02,  1.9865e-02, -4.2314e-02, -2.2266e-02, -9.8073e-03,\n",
       "                                    2.5827e-02, -3.1281e-02,  7.4087e-02,  4.6511e-02,  8.1717e-02,\n",
       "                                    2.5233e-02,  6.9917e-03, -6.4859e-05,  5.1057e-03,  2.5267e-02,\n",
       "                                    3.1813e-02,  2.6098e-02,  4.5743e-03,  3.5027e-02,  1.6978e-01,\n",
       "                                    1.3961e-01], device='cuda:0')),\n",
       "                          ('model.4.weight',\n",
       "                           tensor([[ 0.0048,  0.0410, -0.0106,  ...,  0.0441, -0.1039, -0.0864],\n",
       "                                   [ 0.0479, -0.0238, -0.0335,  ...,  0.0125, -0.0179, -0.0283],\n",
       "                                   [ 0.0304, -0.0219, -0.0332,  ..., -0.0008,  0.1231,  0.2182],\n",
       "                                   ...,\n",
       "                                   [-0.0381, -0.0132,  0.0440,  ...,  0.0261,  0.0712,  0.0371],\n",
       "                                   [-0.0458,  0.0427, -0.0604,  ..., -0.0323, -0.0423,  0.0442],\n",
       "                                   [-0.0294,  0.0640, -0.0557,  ..., -0.0238,  0.1141,  0.1185]],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.4.bias',\n",
       "                           tensor([ 0.0333,  0.0139,  0.1871,  0.1494, -0.0120,  0.1361,  0.0157,  0.0178,\n",
       "                                    0.0133,  0.0462,  0.0091,  0.0741,  0.0760,  0.1304,  0.1604,  0.1097,\n",
       "                                    0.0302,  0.0416,  0.2567, -0.0598, -0.0515,  0.0875,  0.0570, -0.0280,\n",
       "                                   -0.0305,  0.0882,  0.0075, -0.0541,  0.1023, -0.0434, -0.0522,  0.1184,\n",
       "                                   -0.0270,  0.1577, -0.0023, -0.0132, -0.0366, -0.0350,  0.1157, -0.0374,\n",
       "                                   -0.0313, -0.0490, -0.0285, -0.0288,  0.1021,  0.1138,  0.0706,  0.0645,\n",
       "                                    0.0133, -0.0436, -0.0218,  0.0965,  0.1071, -0.0616,  0.2573, -0.0186,\n",
       "                                    0.0027, -0.0160,  0.0062, -0.0013,  0.1233,  0.2526, -0.0680,  0.0565,\n",
       "                                   -0.0468, -0.0245, -0.0252, -0.0572, -0.0326,  0.0214, -0.0456, -0.0066,\n",
       "                                   -0.0600,  0.0113, -0.0014, -0.0269,  0.0541,  0.0271,  0.0362,  0.1303,\n",
       "                                   -0.0406,  0.0728, -0.0498,  0.0263,  0.0074,  0.0636,  0.0634, -0.0395,\n",
       "                                   -0.0166, -0.0372,  0.0345,  0.0372,  0.0661, -0.0523, -0.0108, -0.0400,\n",
       "                                    0.0849,  0.1236, -0.0179, -0.0276,  0.1717,  0.0542,  0.2777, -0.0251,\n",
       "                                   -0.0608,  0.0478,  0.0362,  0.0119, -0.0582, -0.0156, -0.0044,  0.0905,\n",
       "                                    0.1509, -0.0224,  0.0627, -0.0472, -0.0130, -0.0490,  0.0335,  0.0271,\n",
       "                                    0.0396,  0.0540,  0.0315, -0.0098,  0.2126,  0.0652, -0.0561,  0.0611],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.6.weight',\n",
       "                           tensor([[-0.2583, -0.1496,  0.6493, -0.8455,  0.1372,  0.6719, -0.0323, -0.2257,\n",
       "                                    -0.1070,  0.0429,  0.0462, -0.2481, -0.5717,  0.5517,  0.5112, -0.5216,\n",
       "                                     0.0844, -0.5060,  0.9560,  0.0240, -0.0256, -0.5742, -0.5834,  0.0843,\n",
       "                                     0.0468,  0.2904,  0.0147,  0.0393, -0.4656, -0.0384, -0.0213, -0.5997,\n",
       "                                    -0.0612,  0.5923, -0.0316,  0.0177,  0.0130,  0.0034,  0.3947,  0.1303,\n",
       "                                     0.0998,  0.0270,  0.0786,  0.1116,  0.7815, -0.6373,  0.2327, -0.4775,\n",
       "                                     0.0105, -0.0444,  0.0254,  0.1654,  0.3232,  0.0706,  0.9359,  0.0679,\n",
       "                                     0.0822,  0.0667, -0.0060,  0.0880,  0.2806,  0.9276,  0.1121, -0.2495,\n",
       "                                    -0.0652,  0.0459, -0.0114,  0.0598,  0.0147, -0.0506, -0.0143, -0.0063,\n",
       "                                    -0.0152, -0.0170,  0.1103, -0.0200, -0.1978, -0.2479, -0.2468,  0.6984,\n",
       "                                     0.0665, -0.4173,  0.0310, -0.0857,  0.1367, -0.4896,  0.2441, -0.0218,\n",
       "                                     0.0535, -0.0625,  0.0192,  0.0640, -0.5252,  0.0481,  0.0595, -0.0459,\n",
       "                                    -0.4197,  0.5086, -0.0259,  0.0869,  0.5605, -0.4534,  0.9927,  0.0704,\n",
       "                                     0.0156, -0.2914, -0.2492, -0.2347, -0.0318, -0.0074, -0.0265,  0.6169,\n",
       "                                    -0.6580,  0.1001, -0.3669,  0.0052, -0.1114,  0.0035, -0.2526, -0.1113,\n",
       "                                     0.0241, -0.3848, -0.0679,  0.0203,  1.0856,  0.2479, -0.0199,  0.3340]],\n",
       "                                  device='cuda:0')),\n",
       "                          ('model.6.bias',\n",
       "                           tensor([0.8977], device='cuda:0'))]),\n",
       "             'epoch': 3,\n",
       "             'acc': tensor(66.5497, device='cuda:0')})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_training(eval_dataset_info, validate_dataset_info, model, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffb4fac2-492b-4772-9db9-2bc21f95780f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense_768_768_256_128_1\n",
      "EPOCH 1\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5408589243888855  0.5241131782531738\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   72.37%              73.87%\n",
      "EPOCH 2\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5238837003707886  0.5175482630729675\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   73.47%              74.14%\n",
      "EPOCH 3\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5144701600074768  0.5142543911933899\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.13%              74.55%\n",
      "EPOCH 4\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5009475946426392  0.5277002453804016\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   75.07%              73.78%\n",
      "EPOCH 5\n",
      "           Train                Test\n",
      "---------  -------------------  -----------------\n",
      "Loss       0.49207305908203125  0.519706666469574\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   75.64%               74.37%\n",
      "EPOCH 6\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 67\u001b[0m, in \u001b[0;36mTrain.train\u001b[0;34m(self, model, optimizer, epochs, p)\u001b[0m\n\u001b[1;32m     66\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 67\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[22], line 31\u001b[0m, in \u001b[0;36mTrain.train_one_epoch\u001b[0;34m(self, model, optimizer, p)\u001b[0m\n\u001b[1;32m     29\u001b[0m last_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n",
      "File \u001b[0;32m~/venv/nn-snake/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/venv/nn-snake/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    674\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m~/venv/nn-snake/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/nn-snake/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03mTake in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/nn-snake/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/venv/nn-snake/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n",
      "File \u001b[0;32m~/venv/nn-snake/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:222\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collate([\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 130\u001b[0m, in \u001b[0;36mTrain.find_best\u001b[0;34m(self, model, epochs, lr, momentum)\u001b[0m\n\u001b[1;32m    128\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# optimizer = optim.Adam(model.parameters())\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m best_model_state_dict, epoch, acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(model, best_model_state_dict, epoch, lr, momentum, acc)\n\u001b[1;32m    132\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m, in \u001b[0;36mtimeit.<locals>.timed\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtimed\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m      5\u001b[0m     ts \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     te \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mte\u001b[38;5;241m-\u001b[39mts\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 90\u001b[0m, in \u001b[0;36mTrain.train\u001b[0;34m(self, model, optimizer, epochs, p)\u001b[0m\n\u001b[1;32m     87\u001b[0m             best_acc \u001b[38;5;241m=\u001b[39m validate_acc\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(model, best_model, best_epoch, \u001b[43mlr\u001b[49m, momentum, acc)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_model, best_epoch, best_acc\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr' is not defined"
     ]
    }
   ],
   "source": [
    "pipe.find_best(model, 50, lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c7732-d71f-4bda-af54-277c04f9ed43",
   "metadata": {},
   "source": [
    "## Results of traingin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "442a84bb-be00-4be9-bd68-7e689d485e80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense_768_1\n",
      "EPOCH 1\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6204628944396973  0.6315557956695557\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   65.92%              64.05%\n",
      "EPOCH 2\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6179744005203247  0.6281192898750305\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.26%              64.48%\n",
      "EPOCH 3\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6164440512657166  0.6277697086334229\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.42%              64.46%\n",
      "EPOCH 4\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6157306432723999  0.6264545321464539\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.46%              64.62%\n",
      "EPOCH 5\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.614887535572052  0.6263909339904785\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   66.67%             64.59%\n",
      "EPOCH 6\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6145502328872681  0.6265802979469299\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.65%              64.61%\n",
      "EPOCH 7\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6140517592430115  0.6257997751235962\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.78%              64.70%\n",
      "EPOCH 8\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6137537956237793  0.6258960366249084\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.86%              64.82%\n",
      "EPOCH 9\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6134706139564514  0.6248118281364441\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.90%              64.89%\n",
      "EPOCH 10\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6133242249488831  0.6248045563697815\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.86%              64.88%\n",
      "EPOCH 11\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6132699847221375  0.6243957281112671\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.82%              64.91%\n",
      "EPOCH 12\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6129680275917053  0.6249285340309143\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.04%              64.91%\n",
      "EPOCH 13\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6128088235855103  0.6244370341300964\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.94%              64.95%\n",
      "EPOCH 14\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6126130223274231  0.6245331168174744\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.03%              64.99%\n",
      "EPOCH 15\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6125046014785767  0.6240091919898987\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.08%              65.01%\n",
      "EPOCH 16\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6124497652053833  0.6247316598892212\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.02%              64.90%\n",
      "EPOCH 17\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6123896837234497  0.6240048408508301\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.02%              64.96%\n",
      "EPOCH 18\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6123141646385193  0.6237436532974243\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.02%              65.02%\n",
      "EPOCH 19\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6121392250061035  0.6238915324211121\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.12%              65.06%\n",
      "EPOCH 20\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6120864748954773  0.6244131922721863\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.12%              64.99%\n",
      "EPOCH 21\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6122604012489319  0.6245662569999695\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.21%              65.08%\n",
      "EPOCH 22\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6118826866149902  0.6232569217681885\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.14%              65.16%\n",
      "EPOCH 23\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6119386553764343  0.6240968108177185\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.10%              65.05%\n",
      "EPOCH 24\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6118521094322205  0.6234468221664429\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.14%              65.09%\n",
      "EPOCH 25\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6118314862251282  0.6238157749176025\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.20%              65.11%\n",
      "EPOCH 26\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6118741631507874  0.6240118145942688\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.21%              65.10%\n",
      "EPOCH 27\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6118302941322327  0.6232374906539917\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.06%              65.07%\n",
      "EPOCH 28\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6118630766868591  0.6234681606292725\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.04%              65.10%\n",
      "EPOCH 29\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6115846037864685  0.6232432126998901\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.16%              65.09%\n",
      "EPOCH 30\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6116559505462646  0.6237759590148926\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.20%              65.13%\n",
      "EPOCH 31\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6115373969078064  0.6235159039497375\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.18%              65.18%\n",
      "EPOCH 32\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6114633679389954  0.6229617595672607\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.20%              65.21%\n",
      "EPOCH 33\n",
      "           Train               Test\n",
      "---------  ------------------  -----------------\n",
      "Loss       0.6115182042121887  0.623185932636261\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.14%              65.21%\n",
      "EPOCH 34\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6116162538528442  0.6233314871788025\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.09%              65.13%\n",
      "EPOCH 35\n",
      "           Train               Test\n",
      "---------  ------------------  -----------------\n",
      "Loss       0.6115061044692993  0.622838020324707\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.12%              65.16%\n",
      "EPOCH 36\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6113900542259216  0.6233962774276733\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.17%              65.11%\n",
      "EPOCH 37\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6117107272148132  0.6227271556854248\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.09%              65.18%\n",
      "EPOCH 38\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6115384101867676  0.6234745979309082\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.11%              65.12%\n",
      "EPOCH 39\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6123538017272949  0.6247656345367432\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.31%              65.10%\n",
      "EPOCH 40\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6113278865814209  0.6228039264678955\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.21%              65.16%\n",
      "EPOCH 41\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6112481951713562  0.6231058239936829\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.22%              65.24%\n",
      "EPOCH 42\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6112664341926575  0.6227133274078369\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.18%              65.22%\n",
      "EPOCH 43\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6112903952598572  0.6231885552406311\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.26%              65.23%\n",
      "EPOCH 44\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.611275315284729  0.6231415867805481\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   67.26%             65.17%\n",
      "EPOCH 45\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6112347841262817  0.6228695511817932\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.18%              65.24%\n",
      "EPOCH 46\n",
      "           Train               Test\n",
      "---------  ------------------  -----------------\n",
      "Loss       0.6113144755363464  0.622650682926178\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.12%              65.17%\n",
      "EPOCH 47\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.611391007900238  0.6237472295761108\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   67.29%             65.12%\n",
      "EPOCH 48\n",
      "           Train               Test\n",
      "---------  ------------------  -----------------\n",
      "Loss       0.6113461852073669  0.622808575630188\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.16%              65.22%\n",
      "EPOCH 49\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.611151933670044  0.6228394508361816\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   67.22%             65.22%\n",
      "EPOCH 50\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6111154556274414  0.6233230829238892\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.25%              65.19%\n",
      "Took: 466.60s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe.find_best(model, 50, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1a68392-35ae-4b7a-97c3-c7cb1dd939df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense_768_768_1\n",
      "EPOCH 1\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6102012395858765  0.6209103465080261\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   66.76%              65.18%\n",
      "EPOCH 2\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5948296785354614  0.6062929034233093\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   68.64%              67.05%\n",
      "EPOCH 3\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5827310085296631  0.5942636132240295\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   69.76%              68.22%\n",
      "EPOCH 4\n",
      "           Train               Test\n",
      "---------  ------------------  ----------------\n",
      "Loss       0.5741562843322754  0.58530193567276\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   70.34%              69.02%\n",
      "EPOCH 5\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5669348835945129  0.5776150822639465\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   70.89%              69.57%\n",
      "EPOCH 6\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5620591044425964  0.5709043741226196\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   71.18%              70.07%\n",
      "EPOCH 7\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.555168867111206  0.5638788342475891\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   71.67%             70.60%\n",
      "EPOCH 8\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5479937791824341  0.5552473068237305\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   72.29%              71.41%\n",
      "EPOCH 9\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5428370833396912  0.5504432320594788\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   72.61%              71.76%\n",
      "EPOCH 10\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.537254273891449  0.5451469421386719\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   72.98%             72.06%\n",
      "EPOCH 11\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5347552299499512  0.5403775572776794\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   73.14%              72.47%\n",
      "EPOCH 12\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5284383296966553  0.5352951884269714\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   73.58%              72.78%\n",
      "EPOCH 13\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5304242372512817  0.5359892845153809\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   73.39%              72.76%\n",
      "EPOCH 14\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5172369480133057  0.5225674510002136\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.34%              73.75%\n",
      "EPOCH 15\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5113872289657593  0.5177096724510193\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.70%              73.97%\n",
      "EPOCH 16\n",
      "           Train               Test\n",
      "---------  ------------------  -----------------\n",
      "Loss       0.5065327882766724  0.511284589767456\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   75.14%              74.64%\n",
      "EPOCH 17\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5047701001167297  0.5096986889839172\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   75.25%              74.62%\n",
      "EPOCH 18\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4968680739402771  0.49966728687286377\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   75.81%              75.46%\n",
      "EPOCH 19\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4976676404476166  0.5013783574104309\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   75.85%              75.46%\n",
      "EPOCH 20\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4853665232658386  0.4886176586151123\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   76.72%              76.34%\n",
      "EPOCH 21\n",
      "           Train             Test\n",
      "---------  ----------------  ------------------\n",
      "Loss       0.48182213306427  0.4852541983127594\n",
      "Precision  nan               nan\n",
      "Recall     nan               nan\n",
      "Accuracy   76.90%            76.49%\n",
      "EPOCH 22\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.47798046469688416  0.4811975061893463\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.02%               76.61%\n",
      "EPOCH 23\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.47609326243400574  0.47920024394989014\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.29%               76.99%\n",
      "EPOCH 24\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.46769779920578003  0.46945348381996155\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.79%               77.51%\n",
      "EPOCH 25\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4649471640586853  0.4662509262561798\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   78.07%              77.85%\n",
      "EPOCH 26\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.471418559551239  0.4736786484718323\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   77.01%             76.53%\n",
      "EPOCH 27\n",
      "           Train                Test\n",
      "---------  -------------------  -----------------\n",
      "Loss       0.45483070611953735  0.457169771194458\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.59%               78.28%\n",
      "EPOCH 28\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4508642256259918  0.45246627926826477\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   78.61%              78.25%\n",
      "EPOCH 29\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.44560372829437256  0.4474204480648041\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.13%               78.91%\n",
      "EPOCH 30\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.46688205003738403  0.46766141057014465\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.93%               77.69%\n",
      "EPOCH 31\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4651169180870056  0.4682908356189728\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   77.07%              76.61%\n",
      "EPOCH 32\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.44382184743881226  0.4449794590473175\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.42%               79.17%\n",
      "EPOCH 33\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4324720799922943  0.43377551436424255\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.16%              79.76%\n",
      "EPOCH 34\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4260045289993286  0.42721065878868103\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.17%              79.89%\n",
      "EPOCH 35\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4211284816265106  0.4222549498081207\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.63%              80.32%\n",
      "EPOCH 36\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4259362816810608  0.4233842194080353\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.19%              80.30%\n",
      "EPOCH 37\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4177809953689575  0.4181599020957947\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.85%              80.54%\n",
      "EPOCH 38\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.41412481665611267  0.4126662015914917\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.93%               80.87%\n",
      "EPOCH 39\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4210655987262726  0.41979026794433594\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.26%              80.14%\n",
      "EPOCH 40\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.42384597659111023  0.42359670996665955\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.58%               80.39%\n",
      "EPOCH 41\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4016023278236389  0.40209606289863586\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.85%              81.66%\n",
      "EPOCH 42\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4041329026222229  0.40461310744285583\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.81%              81.78%\n",
      "EPOCH 43\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.41585618257522583  0.4134344160556793\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.38%               80.40%\n",
      "EPOCH 44\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.40484440326690674  0.40572890639305115\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.15%               80.88%\n",
      "EPOCH 45\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.3900502622127533  0.39017975330352783\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   82.51%              82.22%\n",
      "EPOCH 46\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.49396049976348877  0.4909243881702423\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   74.69%               74.75%\n",
      "EPOCH 47\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.42666956782341003  0.4216647148132324\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.48%               79.69%\n",
      "EPOCH 48\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4002518951892853  0.3985320031642914\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.10%              80.99%\n",
      "EPOCH 49\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3750990033149719  0.3756539523601532\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   83.28%              83.15%\n",
      "EPOCH 50\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.3826825022697449  0.38578328490257263\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   82.66%              82.26%\n",
      "Took: 514.58s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe.find_best(model, 50, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b28464852583617f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T20:32:27.823212Z",
     "start_time": "2024-05-04T19:15:51.913062Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense_768_256_128_1\n",
      "EPOCH 1\n",
      "           Train               Test\n",
      "---------  ------------------  -----------------\n",
      "Loss       0.6070567965507507  0.618075966835022\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.12%              65.64%\n",
      "EPOCH 2\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5833998918533325  0.5958794951438904\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   69.69%              68.03%\n",
      "EPOCH 3\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5701068043708801  0.5803307890892029\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   70.67%              69.39%\n",
      "EPOCH 4\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5587959885597229  0.5666363835334778\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   71.49%              70.46%\n",
      "EPOCH 5\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5496858358383179  0.5562384128570557\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   72.25%              71.48%\n",
      "EPOCH 6\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5446968078613281  0.5500649809837341\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   72.44%              71.81%\n",
      "EPOCH 7\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5309149026870728  0.5358408093452454\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   73.57%              72.97%\n",
      "EPOCH 8\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5250592827796936  0.5283703207969666\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   73.77%              73.25%\n",
      "EPOCH 9\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5155128836631775  0.5186350345611572\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.58%              74.02%\n",
      "EPOCH 10\n",
      "           Train              Test\n",
      "---------  -----------------  -----------------\n",
      "Loss       0.507570207118988  0.510254442691803\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   75.04%             74.78%\n",
      "EPOCH 11\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.49955084919929504  0.5023821592330933\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   75.72%               75.27%\n",
      "EPOCH 12\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.49429208040237427  0.49714991450309753\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   75.95%               75.61%\n",
      "EPOCH 13\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.488166481256485  0.4898819029331207\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   76.25%             75.98%\n",
      "EPOCH 14\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.47947749495506287  0.48093217611312866\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   76.85%               76.68%\n",
      "EPOCH 15\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4822973906993866  0.4902496933937073\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   76.48%              75.72%\n",
      "EPOCH 16\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.47989019751548767  0.4846378266811371\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   76.87%               76.46%\n",
      "EPOCH 17\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.47537723183631897  0.47614040970802307\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   76.76%               76.51%\n",
      "EPOCH 18\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4604560136795044  0.4624243378639221\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   78.11%              77.76%\n",
      "EPOCH 19\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.45914557576179504  0.4593011438846588\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.07%               77.87%\n",
      "EPOCH 20\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4596189260482788  0.4636123776435852\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   77.75%              77.25%\n",
      "EPOCH 21\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4472948908805847  0.4476048946380615\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   78.67%              78.53%\n",
      "EPOCH 22\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.44975805282592773  0.44587674736976624\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.36%               78.48%\n",
      "EPOCH 23\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.44055891036987305  0.4403548240661621\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.23%               79.06%\n",
      "EPOCH 24\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4555393159389496  0.46096551418304443\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   78.32%              77.83%\n",
      "EPOCH 25\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.43580448627471924  0.4359867572784424\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.35%               79.21%\n",
      "EPOCH 26\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4276912808418274  0.43202832341194153\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   79.94%              79.45%\n",
      "EPOCH 27\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4222290515899658  0.4239262342453003\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.23%              79.90%\n",
      "EPOCH 28\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.43471649289131165  0.4408460259437561\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.46%               78.92%\n",
      "EPOCH 29\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4162839353084564  0.4189373254776001\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.52%              80.14%\n",
      "EPOCH 30\n",
      "           Train                Test\n",
      "---------  -------------------  -----------------\n",
      "Loss       0.42495864629745483  0.428753137588501\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.71%               79.21%\n",
      "EPOCH 31\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.40954089164733887  0.4087679386138916\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   80.85%               80.68%\n",
      "EPOCH 32\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4343276619911194  0.43389272689819336\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   78.91%              78.80%\n",
      "EPOCH 33\n",
      "           Train             Test\n",
      "---------  ----------------  -------------------\n",
      "Loss       0.40593221783638  0.40657228231430054\n",
      "Precision  nan               nan\n",
      "Recall     nan               nan\n",
      "Accuracy   81.05%            80.74%\n",
      "EPOCH 34\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.4010608494281769  0.4000587463378906\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.28%              81.17%\n",
      "EPOCH 35\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3993961811065674  0.4022269546985626\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.40%              81.07%\n",
      "EPOCH 36\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.40302780270576477  0.40336328744888306\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.20%               80.98%\n",
      "EPOCH 37\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3957943618297577  0.3981249928474426\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.50%              81.22%\n",
      "EPOCH 38\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.39559492468833923  0.3925889730453491\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.50%               81.53%\n",
      "EPOCH 39\n",
      "           Train             Test\n",
      "---------  ----------------  -------------------\n",
      "Loss       0.42503622174263  0.42821094393730164\n",
      "Precision  nan               nan\n",
      "Recall     nan               nan\n",
      "Accuracy   79.76%            79.52%\n",
      "EPOCH 40\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.38729870319366455  0.38919731974601746\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.97%               81.63%\n",
      "EPOCH 41\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3884521424770355  0.3912750482559204\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.84%              81.50%\n",
      "EPOCH 42\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.3818473815917969  0.38233426213264465\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   82.26%              82.03%\n",
      "EPOCH 43\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.37988659739494324  0.3805818259716034\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   82.50%               82.27%\n",
      "EPOCH 44\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.3804941475391388  0.37959596514701843\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   82.29%              82.14%\n",
      "EPOCH 45\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.3810991942882538  0.38349035382270813\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   82.14%              81.79%\n",
      "EPOCH 46\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.3755324184894562  0.37874317169189453\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   82.63%              82.24%\n",
      "EPOCH 47\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.3733989894390106  0.37626323103904724\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   82.66%              82.21%\n",
      "EPOCH 48\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3663339912891388  0.3670692443847656\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   83.15%              83.10%\n",
      "EPOCH 49\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3675404191017151  0.3671211004257202\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   83.01%              82.90%\n",
      "EPOCH 50\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.37493133544921875  0.37565743923187256\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   82.70%               82.46%\n",
      "Took: 505.56s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe.find_best(model, 50, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d9126a4-6bfb-4bb3-b850-3235a42e6b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense_768_768_256_128_1\n",
      "EPOCH 1\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.6034354567527771  0.6142628788948059\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   67.42%              66.08%\n",
      "EPOCH 2\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5760378241539001  0.5863416790962219\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   70.27%              69.00%\n",
      "EPOCH 3\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5583397746086121  0.5652486681938171\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   71.71%              70.89%\n",
      "EPOCH 4\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.541077733039856  0.5462588667869568\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   72.80%             72.07%\n",
      "EPOCH 5\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.526005744934082  0.5288388133049011\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   73.93%             73.46%\n",
      "EPOCH 6\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.5180150866508484  0.5150623917579651\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   74.17%              74.17%\n",
      "EPOCH 7\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.48856860399246216  0.4889536499977112\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   76.38%               76.17%\n",
      "EPOCH 8\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.47357070446014404  0.47312647104263306\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   77.35%               77.16%\n",
      "EPOCH 9\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.48380109667778015  0.4845643937587738\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   76.03%               75.55%\n",
      "EPOCH 10\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.45825818181037903  0.4605250656604767\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   78.43%               78.14%\n",
      "EPOCH 11\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.43143805861473083  0.43006402254104614\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   79.84%               79.85%\n",
      "EPOCH 12\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4197225272655487  0.41706860065460205\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.40%              80.31%\n",
      "EPOCH 13\n",
      "           Train              Test\n",
      "---------  -----------------  -----------------\n",
      "Loss       0.410136878490448  0.409771203994751\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   80.92%             80.64%\n",
      "EPOCH 14\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.4185236692428589  0.41583433747291565\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   80.34%              80.34%\n",
      "EPOCH 15\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3895960748195648  0.3880406618118286\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   81.89%              81.77%\n",
      "EPOCH 16\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3757263123989105  0.3782899081707001\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   82.87%              82.62%\n",
      "EPOCH 17\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.3704771399497986  0.36755144596099854\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   83.07%              83.05%\n",
      "EPOCH 18\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.3543568551540375  0.35443761944770813\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   83.89%              83.69%\n",
      "EPOCH 19\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.38765862584114075  0.38603997230529785\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   81.74%               81.69%\n",
      "EPOCH 20\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.35967889428138733  0.36413338780403137\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   83.49%               83.09%\n",
      "EPOCH 21\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3263791799545288  0.3289375901222229\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   85.36%              84.91%\n",
      "EPOCH 22\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.3255113363265991  0.32513776421546936\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   85.22%              85.03%\n",
      "EPOCH 23\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.31478628516197205  0.3184007704257965\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   85.80%               85.32%\n",
      "EPOCH 24\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.3052902817726135  0.3031676411628723\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   86.31%              86.13%\n",
      "EPOCH 25\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.3233391344547272  0.32750675082206726\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   85.38%              84.95%\n",
      "EPOCH 26\n",
      "           Train              Test\n",
      "---------  -----------------  ------------------\n",
      "Loss       0.290065735578537  0.2932111620903015\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   87.18%             86.78%\n",
      "EPOCH 27\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.28523099422454834  0.2882334887981415\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   87.29%               86.90%\n",
      "EPOCH 28\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.2731294631958008  0.2749404311180115\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   88.06%              87.69%\n",
      "EPOCH 29\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.2665637135505676  0.26800471544265747\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   88.27%              87.95%\n",
      "EPOCH 30\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.2648170590400696  0.26251375675201416\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   88.31%              88.20%\n",
      "EPOCH 31\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.2723034918308258  0.27562853693962097\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   87.78%              87.41%\n",
      "EPOCH 32\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.25265929102897644  0.2560844421386719\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   88.89%               88.40%\n",
      "EPOCH 33\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.2499002367258072  0.2523864507675171\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   89.00%              88.60%\n",
      "EPOCH 34\n",
      "           Train              Test\n",
      "---------  -----------------  -----------------\n",
      "Loss       0.252909779548645  0.255149781703949\n",
      "Precision  nan                nan\n",
      "Recall     nan                nan\n",
      "Accuracy   88.79%             88.40%\n",
      "EPOCH 35\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.24779140949249268  0.24324755370616913\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   89.17%               89.13%\n",
      "EPOCH 36\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.23772487044334412  0.2349887192249298\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   89.55%               89.53%\n",
      "EPOCH 37\n",
      "           Train               Test\n",
      "---------  ------------------  ------------------\n",
      "Loss       0.2419988214969635  0.2379680871963501\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   89.22%              89.23%\n",
      "EPOCH 38\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.22711007297039032  0.22587400674819946\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   90.08%               89.88%\n",
      "EPOCH 39\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.2317776083946228  0.22770844399929047\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   89.68%              89.66%\n",
      "EPOCH 40\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.21580423414707184  0.21526575088500977\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   90.63%               90.45%\n",
      "EPOCH 41\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.2110527902841568  0.20605435967445374\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   90.87%              90.92%\n",
      "EPOCH 42\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.2079048454761505  0.20688284933567047\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   91.03%              90.88%\n",
      "EPOCH 43\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.20814815163612366  0.20606441795825958\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   90.92%               90.77%\n",
      "EPOCH 44\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.19580423831939697  0.19656094908714294\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   91.57%               91.28%\n",
      "EPOCH 45\n",
      "           Train                Test\n",
      "---------  -------------------  -----------------\n",
      "Loss       0.24368888139724731  0.240617573261261\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   88.84%               88.91%\n",
      "EPOCH 46\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.19481508433818817  0.19130171835422516\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   91.54%               91.45%\n",
      "EPOCH 47\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.1926952451467514  0.18698370456695557\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   91.66%              91.70%\n",
      "EPOCH 48\n",
      "           Train                Test\n",
      "---------  -------------------  ------------------\n",
      "Loss       0.19866319000720978  0.1970497965812683\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   91.25%               91.10%\n",
      "EPOCH 49\n",
      "           Train               Test\n",
      "---------  ------------------  -------------------\n",
      "Loss       0.1826283484697342  0.17871156334877014\n",
      "Precision  nan                 nan\n",
      "Recall     nan                 nan\n",
      "Accuracy   92.12%              92.04%\n",
      "EPOCH 50\n",
      "           Train                Test\n",
      "---------  -------------------  -------------------\n",
      "Loss       0.18097224831581116  0.17490306496620178\n",
      "Precision  nan                  nan\n",
      "Recall     nan                  nan\n",
      "Accuracy   92.22%               92.27%\n",
      "Took: 596.48s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe.find_best(model, 50, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcb51abc-7c85-47e2-9397-2a6982466a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All weights and biases have been saved to the 'models/weights/3l/92/' directory.\n"
     ]
    }
   ],
   "source": [
    "def save_weights_to_csv(model: nn.Module, directory: str):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_name, param_type = name.rsplit('.', 1)\n",
    "        param_data = param.detach().cpu().numpy()\n",
    "        df = pd.DataFrame(param_data)\n",
    "        filename = f\"{layer_name}_{param_type}.csv\"\n",
    "        df.to_csv(os.path.join(directory, filename), header=None, index=False)\n",
    "\n",
    "    print(f\"All weights and biases have been saved to the '{directory}' directory.\")\n",
    "\n",
    "save_weights_to_csv(model, 'models/weights/3l/92/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "515b8b65-6b91-420d-963a-b8c7ea57a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_to_bits(dataset):\n",
    "    dataset.map(lambda x: \";\".join(np.char.mod('%d', np.unpackbits(np.array([x]).view(np.uint8))))).to_csv(\"dataset_bits.csv\", sep=\";\", index=False)\n",
    "    bity = pd.read_csv(\"dataset_bits.csv\", dtype=\"uint64\", sep=\";\", header=None)\n",
    "    dataset.rename(columns={\"draw\": 768})\n",
    "    pd.concat([bity, dataset.rename(columns={\"draw\": 768})], axis=1).to_csv(\"dataset_bits.csv\", sep=\";\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
